{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef load_data(data, window_size, overlap_size):\\n    \\n    #faster implementation but no overlapping function is possible\\n\\n    splits = np.shape(data)[0]//window_size # number of splits\\n    data = data[:splits*window_size] #cut off end of array such that array can be split equaly\\n    data = data.reshape((splits,-1,np.shape(data)[1])) #split array in windows\\n    return data\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def load_data(data, window_size, overlap_size):\n",
    "    \n",
    "    #faster implementation but no overlapping function is possible\n",
    "\n",
    "    splits = np.shape(data)[0]//window_size # number of splits\n",
    "    data = data[:splits*window_size] #cut off end of array such that array can be split equaly\n",
    "    data = data.reshape((splits,-1,np.shape(data)[1])) #split array in windows\n",
    "    return data\n",
    "\"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from skimage.util.shape import view_as_windows\n",
    "import warnings\n",
    "\n",
    "def load_data(data, window_size, overlap_size):\n",
    "    \"\"\"\n",
    "    Split data in windows of equal size with overlap\n",
    "    \n",
    "    INPUT:\n",
    "    @data: data numpy array of shape [elements per file, features]\n",
    "    @window: number of elements per window\n",
    "    @overlap_size: defines the overlapping elements between consecutive windows\n",
    "    \n",
    "    OUTPUT\n",
    "    @data: data numpy array of shape [number_of_windows, elements per window, features]\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if window_size==overlap_size:\n",
    "        raise Exception(\"Overlap arg must be smaller than length of windows\")\n",
    "    S = window_size - overlap_size\n",
    "    nd0 = ((len(data)-window_size)//S)+1\n",
    "    if nd0*S-S!=len(data)-window_size:\n",
    "        warnings.warn(\"Not all elements were covered\")\n",
    "    return view_as_windows(data, (window_size,data.shape[1]), step=S)[:,0,:,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_nan_element(data_with_nan):\n",
    "    \"\"\"\n",
    "    Delete all elements in the data which have any nan valued feature\n",
    "    \n",
    "    INPUT:\n",
    "    @data_with_nan: data numpy array containing nan_values\n",
    "    \n",
    "    OUTPUT\n",
    "    @data_with_nan: data numpy array inlcuding just elements per window which do have no nan_vaues in any feature\n",
    "    \"\"\"\n",
    "    nan_val = np.isnan(data_with_nan) #mask for all nan_elements as 2d array [elements_per_window, features]\n",
    "    nan_val = np.any(nan_val,axis = 1) #mask for all nan_rows as 1d array [elements_per_window]\n",
    "    return data_with_nan[nan_val==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef create_folder_dictionary(list_of_train_BSD_states, list_of_test_BSD_states, data_path):\\n    \\n    data_path = data_path\\n    training_folders = {}\\n    testing_folders = {}\\n    \\n    #Sorting the individual folders by findinding the BSD_states in the folder names\\n    for data_path_element in os.listdir(data_path):\\n        if any(element in data_path_element for element in list_of_train_BSD_states): \\n            training_folders[data_path_element]  = os.listdir(os.path.join(data_path,data_path_element))\\n        elif any(element in data_path_element for element in list_of_test_BSD_states): \\n            testing_folders[data_path_element] = os.listdir(os.path.join(data_path,data_path_element))\\n    return training_folders, testing_folders\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def create_folder_dictionary(list_of_train_BSD_states, list_of_test_BSD_states, data_path):\n",
    "    \n",
    "    data_path = data_path\n",
    "    training_folders = {}\n",
    "    testing_folders = {}\n",
    "    \n",
    "    #Sorting the individual folders by findinding the BSD_states in the folder names\n",
    "    for data_path_element in os.listdir(data_path):\n",
    "        if any(element in data_path_element for element in list_of_train_BSD_states): \n",
    "            training_folders[data_path_element]  = os.listdir(os.path.join(data_path,data_path_element))\n",
    "        elif any(element in data_path_element for element in list_of_test_BSD_states): \n",
    "            testing_folders[data_path_element] = os.listdir(os.path.join(data_path,data_path_element))\n",
    "    return training_folders, testing_folders\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder_dictionary(list_of_train_BSD_states, list_of_test_BSD_states, data_path):\n",
    "    \"\"\"\n",
    "    Create a dictionaty for testing and training containing folder names as keys and files as values\n",
    "    \n",
    "    INPUT:\n",
    "    @list_of_train_BSD_states: list containing the training BSD states as string\n",
    "    @list_of_test_BSD_states: list containing the testing BSD states as string\n",
    "    @data_path: data directory containing folders for each BSD state\n",
    "    \n",
    "    OUTPUT\n",
    "    @training_folders: dictionary folders and keys for training\n",
    "    @testing_folders: dictionary folders and keys for testing\n",
    "    \"\"\"\n",
    "    \n",
    "    data_path = data_path\n",
    "    state_dictionary = {\n",
    "        \"1\":\"NR01_20200317_PGS_31_BSD_31\",\n",
    "        \"2\":\"NR02_20200423_PGS_31_BSD_21\",\n",
    "        \"3\":\"NR03_20200424_PGS_31_BSD_11\",\n",
    "        \"4\":\"NR04_20200424_PGS_31_BSD_P1\",\n",
    "        \"5\":\"NR05_20200930_PGS_31_BSD_22\",\n",
    "        \"6\":\"NR06_20201001_PGS_31_BSD_12\",\n",
    "        \"7\":\"NR07_20201001_PGS_31_BSD_32\",\n",
    "        \"8\":\"NR08_20200918_PGS_31_BSD_33\",\n",
    "        \"9\":\"NR09_20200917_PGS_31_BSD_P2\",\n",
    "        \"10\":\"NR10_20200502_PGS_21_BSD_31\",\n",
    "        \"11\":\"NR11_20200429_PGS_21_BSD_21\",\n",
    "        \"12\":\"NR12_20200429_PGS_21_BSD_11\",\n",
    "        \"13\":\"NR13_20200428_PGS_21_BSD_P1\",\n",
    "        \"14\":\"NR14_20200731_PGS_21_BSD_22\",\n",
    "        \"15\":\"NR15_20200901_PGS_21_BSD_12\",\n",
    "        \"16\":\"NR16_20200908_PGS_21_BSD_32\",\n",
    "        \"17\":\"NR17_20200717_PGS_21_BSD_33\",\n",
    "        \"18\":\"NR18_20200714_PGS_21_BSD_P2\",\n",
    "        \"19\":\"NR19_20200505_PGS_11_BSD_31\",\n",
    "        \"20\":\"NR20_20200507_PGS_11_BSD_21\",\n",
    "        \"21\":\"NR21_20200508_PGS_11_BSD_11\",\n",
    "        \"22\":\"NR22_20200508_PGS_11_BSD_P1\",\n",
    "        \"23\":\"NR23_20200511_PGS_11_BSD_22\",\n",
    "        \"24\":\"NR24_20200512_PGS_11_BSD_12\",\n",
    "        \"25\":\"NR25_20200512_PGS_11_BSD_32\",\n",
    "        \"26\":\"NR26_20200513_PGS_11_BSD_33\",\n",
    "        \"27\":\"NR27_20200513_PGS_11_BSD_P2\",\n",
    "    }\n",
    "    \n",
    "    \n",
    "    training_folders = {}\n",
    "    testing_folders = {}\n",
    "    \n",
    "    for train_element in list_of_train_BSD_states:\n",
    "        training_folders[state_dictionary[train_element]]=os.listdir(os.path.join(data_path,state_dictionary[train_element]))\n",
    "    for test_element in list_of_test_BSD_states:\n",
    "        testing_folders[state_dictionary[test_element]]=os.listdir(os.path.join(data_path,state_dictionary[test_element]))\n",
    "    \n",
    "    return training_folders, testing_folders\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(path):\n",
    "    \"\"\"\n",
    "    Creates a list of all feature names\n",
    "    INPUT:\n",
    "    @path: path to any BSD file since the features are the same for all files\n",
    "    \n",
    "    OUTPUT\n",
    "    @features: list of features:\n",
    "    ['C:s_ist/X', 'C:s_soll/X', 'C:s_diff/X', 'C:v_(n_ist)/X', 'C:v_(n_soll)/X', 'C:P_mech./X', 'C:Pos._Diff./X',\n",
    "    'C:I_ist/X', 'C:I_soll/X', 'C:x_bottom', 'C:y_bottom', 'C:z_bottom', 'C:x_nut', 'C:y_nut', 'C:z_nut',\n",
    "    'C:x_top', 'C:y_top', 'C:z_top', 'D:s_ist/X', 'D:s_soll/X', 'D:s_diff/X', 'D:v_(n_ist)/X', 'D:v_(n_soll)/X',\n",
    "    'D:P_mech./X', 'D:Pos._Diff./X', 'D:I_ist/X', 'D:I_soll/X', 'D:x_bottom', 'D:y_bottom', 'D:z_bottom',\n",
    "    'D:x_nut', 'D:y_nut', 'D:z_nut', 'D:x_top', 'D:y_top', 'D:z_top', 'S:x_bottom', 'S:y_bottom', 'S:z_bottom',\n",
    "    'S:x_nut', 'S:y_nut', 'S:z_nut', 'S:x_top', 'S:y_top', 'S:z_top', 'S:Nominal_rotational_speed[rad/s]',\n",
    "    'S:Actual_rotational_speed[µm/s]', 'S:Actual_position_of_the_position_encoder(dy/dt)[µm/s]',\n",
    "    'S:Actual_position_of_the_motor_encoder(dy/dt)[µm/s]']\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "        csvreader = csv.reader(file)\n",
    "        features = next(csvreader)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_data_from_BSD_state(folders, data_path, features_of_interest, window_size, overlap_size):\n",
    "    \"\"\"\n",
    "    Concatenates all the windowed data from each file to one big torch array\n",
    "    INPUT:\n",
    "    @folders: dictionary containing folders (as keys) and files (as values) to downloaded\n",
    "    @data_path: data directory containing folders for each BSD state\n",
    "    @features_of_interest: list of features which should be included for training\n",
    "    @window_size: number of elements per widow\n",
    "    \n",
    "    OUTPUT:\n",
    "    @n_samples: number of total elements from all included files\n",
    "    @x_data: torch array containing all the data elements \n",
    "    @y_data: torch array containing the labels for all elements\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # arrays to collect data and label\n",
    "    x_data_concatenated = None\n",
    "    y_data_concatenated = None\n",
    "    \n",
    "    \n",
    "    iterator = 0\n",
    "    first = True\n",
    "    \n",
    "    \n",
    "    for BSD_path in folders.keys(): #folder path\n",
    "        for file_path in folders[BSD_path]: #file path \n",
    "            path_BSD_file = os.path.join(data_path, BSD_path, file_path) # concatenate the data_path, folder and file path\n",
    "            #in first iteration get a list if all features\n",
    "            if first == True:\n",
    "                features = get_features(path_BSD_file)\n",
    "            \n",
    "            data_BSD_file = np.genfromtxt(path_BSD_file, dtype = np.dtype('d'), delimiter=',')[1:,:] #write csv in numpy\n",
    "            feature_index_list = np.where(np.isin(features, features_of_interest)) #get index for all features of interest\n",
    "            data_BSD_file = data_BSD_file[:,feature_index_list] #slice numpy array such that just features of interest are included\n",
    "            data_BSD_file = np.squeeze(data_BSD_file, axis = 1) # one unnecessary extra dimension was created while slicing\n",
    "            data_BSD_file = del_nan_element(data_BSD_file) #delete all elements with any nan feature\n",
    "            data_BSD_file = load_data(data_BSD_file, window_size, overlap_size) #window the data\n",
    "            data_BSD_file = np.swapaxes(data_BSD_file,1,2) #swap axes for CNN\n",
    "            \n",
    "            \n",
    "            #rewrite labels as BSD_condition_1 = 0, BSD_condition_2 = 1, BSD_condition_3 = 2, BSD_condition_P1 = 3\n",
    "            label = BSD_path[-2] #take the first number of the BSD state for class label\n",
    "            if label == \"P\":\n",
    "                label = int(3)\n",
    "            else:\n",
    "                label =int(int(label)-1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #concatenate the data from each file in one numpy array\n",
    "            if  first == True: #overwrite variable\n",
    "                x_data_concatenated = np.copy(data_BSD_file)\n",
    "                y_data_concatenated = np.copy(np.asarray([label]*np.shape(data_BSD_file)[0]))\n",
    "                first = False\n",
    "            else: #concatenate data numpy arrays\n",
    "                x_data_concatenated = np.concatenate((x_data_concatenated, data_BSD_file), axis=0)\n",
    "                y_data_concatenated = np.concatenate((y_data_concatenated,np.asarray([label]*np.shape(data_BSD_file)[0])), axis=0)\n",
    "            \n",
    "            iterator +=1\n",
    "            print(f\"{iterator}/{len(folders.keys())*len(folders[list(folders.keys())[0]])} folders downloaded\")\n",
    "            print(f\"downloaded folder: {BSD_path}/{file_path}\")\n",
    "            print(f\"Shape of collected datafram: X_shape: {np.shape(x_data_concatenated)}, Y_shape: {np.shape(y_data_concatenated)}\")\n",
    "    \n",
    "    #generate torch array\n",
    "    n_samples = np.shape(x_data_concatenated)[0]\n",
    "    x_data = torch.from_numpy(x_data_concatenated)\n",
    "    y_data = torch.from_numpy(y_data_concatenated)\n",
    "    \n",
    "    return n_samples, x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesData(Dataset):\n",
    "    \"\"\"\n",
    "    Class for creating dataset using PyTorch data primitive Dataset. An instance of this class can be used in the \n",
    "    PyTorch data primitive Dataloader\n",
    "    \n",
    "    The following patameters can be adjusted:\n",
    "    @windwo_size: Size of window which is used as Input in CNN\n",
    "    @feature_of_interest: List of all features which should be used in the CNN\n",
    "    @list_of_train_BSD_states: List of BSD states which should be used for training. Be careful at least 4 BSD\n",
    "    states representing the 4 different classes should be included for the training\n",
    "    @list_of_test_BSD_states: List of BSD states which should be used for testing\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, domain):\n",
    "        window_size = 1024\n",
    "        overlap_size = 300\n",
    "        features_of_interest =     ['S:x_bottom', 'S:y_bottom', 'S:z_bottom',\n",
    "    'S:x_nut', 'S:y_nut', 'S:z_nut', 'S:x_top', 'S:y_top', 'S:z_top', 'S:Nominal_rotational_speed[rad/s]',\n",
    "    'S:Actual_rotational_speed[µm/s]', 'S:Actual_position_of_the_position_encoder(dy/dt)[µm/s]',\n",
    "    'S:Actual_position_of_the_motor_encoder(dy/dt)[µm/s]']\n",
    "        \n",
    "        \n",
    "        \n",
    "    #['C:s_ist/X', 'C:s_soll/X', 'C:s_diff/X', 'C:v_(n_ist)/X', 'C:v_(n_soll)/X', 'C:P_mech./X', 'C:Pos._Diff./X',\n",
    "    #'C:I_ist/X', 'C:I_soll/X', 'C:x_bottom', 'C:y_bottom', 'C:z_bottom', 'C:x_nut', 'C:y_nut', 'C:z_nut',\n",
    "    #'C:x_top', 'C:y_top', 'C:z_top', 'D:s_ist/X', 'D:s_soll/X', 'D:s_diff/X', 'D:v_(n_ist)/X', 'D:v_(n_soll)/X',\n",
    "    #'D:P_mech./X', 'D:Pos._Diff./X', 'D:I_ist/X', 'D:I_soll/X', 'D:x_bottom', 'D:y_bottom', 'D:z_bottom',\n",
    "    #'D:x_nut', 'D:y_nut', 'D:z_nut', 'D:x_top', 'D:y_top', 'D:z_top','S:x_bottom', 'S:y_bottom', 'S:z_bottom',\n",
    "    #'S:x_nut', 'S:y_nut', 'S:z_nut', 'S:x_top', 'S:y_top', 'S:z_top', 'S:Nominal_rotational_speed[rad/s]',\n",
    "    #'S:Actual_rotational_speed[µm/s]', 'S:Actual_position_of_the_position_encoder(dy/dt)[µm/s]',\n",
    "    #'S:Actual_position_of_the_motor_encoder(dy/dt)[µm/s]']\n",
    "        number_of_files_per_BDS_state = 10\n",
    "        \n",
    "        #list_of_train_BSD_states = [\"BSD_31\", \"BSD_21\", \"BSD_11\", \"BSD_P1\"]\n",
    "        #list_of_test_BSD_states = [\"BSD_32\", \"BSD_22\", \"BSD_12\", \"BSD_P2\"]\n",
    "        \n",
    "        list_of_train_BSD_states = [\"1\", \"2\", \"3\", \"4\", \"10\", \"11\", \"12\", \"13\", \"19\", \"20\", \"21\", \"22\"]\n",
    "        list_of_test_BSD_states = [\"5\", \"6\", \"7\", \"9\", \"14\", \"15\", \"16\", \"18\", \"23\", \"24\", \"25\", \"27\"]\n",
    "        \n",
    "        #data_path = os.path.join(os.path.abspath(os.path.join(os.getcwd(), os.pardir)), \"data\")\n",
    "        \n",
    "        data_path = Path(os.getcwd()).parents[1]\n",
    "        data_path = os.path.join(data_path, \"data\")\n",
    "        \n",
    "        training_folders, testing_folders = create_folder_dictionary(list_of_train_BSD_states, list_of_test_BSD_states, data_path)\n",
    "        folders_domain = {}\n",
    "        folders_domain[\"test\"] = testing_folders\n",
    "        folders_domain[\"train\"] = training_folders\n",
    "        \n",
    "        \n",
    "        self.n_samples, self.x_data, self.y_data = concatenate_data_from_BSD_state(folders_domain[domain], data_path, features_of_interest, window_size, overlap_size)\n",
    "        \n",
    "                  \n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: Not all elements were covered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/120 folders downloaded\n",
      "downloaded folder: NR05_20200930_PGS_31_BSD_22/478_2020_09_30.csv\n",
      "Shape of collected datafram: X_shape: (22, 13, 1024), Y_shape: (22,)\n",
      "2/120 folders downloaded\n",
      "downloaded folder: NR05_20200930_PGS_31_BSD_22/480_2020_09_30.csv\n",
      "Shape of collected datafram: X_shape: (44, 13, 1024), Y_shape: (44,)\n",
      "3/120 folders downloaded\n",
      "downloaded folder: NR05_20200930_PGS_31_BSD_22/477_2020_09_30.csv\n",
      "Shape of collected datafram: X_shape: (66, 13, 1024), Y_shape: (66,)\n",
      "4/120 folders downloaded\n",
      "downloaded folder: NR05_20200930_PGS_31_BSD_22/476_2020_09_30.csv\n",
      "Shape of collected datafram: X_shape: (88, 13, 1024), Y_shape: (88,)\n",
      "5/120 folders downloaded\n",
      "downloaded folder: NR05_20200930_PGS_31_BSD_22/481_2020_09_30.csv\n",
      "Shape of collected datafram: X_shape: (110, 13, 1024), Y_shape: (110,)\n",
      "6/120 folders downloaded\n",
      "downloaded folder: NR05_20200930_PGS_31_BSD_22/479_2020_09_30.csv\n",
      "Shape of collected datafram: X_shape: (132, 13, 1024), Y_shape: (132,)\n",
      "7/120 folders downloaded\n",
      "downloaded folder: NR05_20200930_PGS_31_BSD_22/484_2020_09_30.csv\n",
      "Shape of collected datafram: X_shape: (154, 13, 1024), Y_shape: (154,)\n",
      "8/120 folders downloaded\n",
      "downloaded folder: NR05_20200930_PGS_31_BSD_22/483_2020_09_30.csv\n",
      "Shape of collected datafram: X_shape: (176, 13, 1024), Y_shape: (176,)\n",
      "9/120 folders downloaded\n",
      "downloaded folder: NR05_20200930_PGS_31_BSD_22/482_2020_09_30.csv\n",
      "Shape of collected datafram: X_shape: (198, 13, 1024), Y_shape: (198,)\n",
      "10/120 folders downloaded\n",
      "downloaded folder: NR05_20200930_PGS_31_BSD_22/475_2020_09_30.csv\n",
      "Shape of collected datafram: X_shape: (220, 13, 1024), Y_shape: (220,)\n",
      "11/120 folders downloaded\n",
      "downloaded folder: NR06_20201001_PGS_31_BSD_12/503_2020_10_01.csv\n",
      "Shape of collected datafram: X_shape: (242, 13, 1024), Y_shape: (242,)\n",
      "12/120 folders downloaded\n",
      "downloaded folder: NR06_20201001_PGS_31_BSD_12/504_2020_10_01.csv\n",
      "Shape of collected datafram: X_shape: (264, 13, 1024), Y_shape: (264,)\n",
      "13/120 folders downloaded\n",
      "downloaded folder: NR06_20201001_PGS_31_BSD_12/505_2020_10_01.csv\n",
      "Shape of collected datafram: X_shape: (286, 13, 1024), Y_shape: (286,)\n",
      "14/120 folders downloaded\n",
      "downloaded folder: NR06_20201001_PGS_31_BSD_12/502_2020_10_01.csv\n",
      "Shape of collected datafram: X_shape: (308, 13, 1024), Y_shape: (308,)\n",
      "15/120 folders downloaded\n",
      "downloaded folder: NR06_20201001_PGS_31_BSD_12/500_2020_10_01.csv\n",
      "Shape of collected datafram: X_shape: (330, 13, 1024), Y_shape: (330,)\n",
      "16/120 folders downloaded\n",
      "downloaded folder: NR06_20201001_PGS_31_BSD_12/508_2020_10_01.csv\n",
      "Shape of collected datafram: X_shape: (352, 13, 1024), Y_shape: (352,)\n",
      "17/120 folders downloaded\n",
      "downloaded folder: NR06_20201001_PGS_31_BSD_12/507_2020_10_01.csv\n",
      "Shape of collected datafram: X_shape: (374, 13, 1024), Y_shape: (374,)\n",
      "18/120 folders downloaded\n",
      "downloaded folder: NR06_20201001_PGS_31_BSD_12/506_2020_10_01.csv\n",
      "Shape of collected datafram: X_shape: (396, 13, 1024), Y_shape: (396,)\n",
      "19/120 folders downloaded\n",
      "downloaded folder: NR06_20201001_PGS_31_BSD_12/501_2020_10_01.csv\n",
      "Shape of collected datafram: X_shape: (418, 13, 1024), Y_shape: (418,)\n",
      "20/120 folders downloaded\n",
      "downloaded folder: NR06_20201001_PGS_31_BSD_12/499_2020_10_01.csv\n",
      "Shape of collected datafram: X_shape: (440, 13, 1024), Y_shape: (440,)\n",
      "21/120 folders downloaded\n",
      "downloaded folder: NR07_20201001_PGS_31_BSD_32/493_2020_10_01.csv\n",
      "Shape of collected datafram: X_shape: (462, 13, 1024), Y_shape: (462,)\n",
      "22/120 folders downloaded\n",
      "downloaded folder: NR07_20201001_PGS_31_BSD_32/494_2020_10_01.csv\n",
      "Shape of collected datafram: X_shape: (484, 13, 1024), Y_shape: (484,)\n",
      "23/120 folders downloaded\n",
      "downloaded folder: NR07_20201001_PGS_31_BSD_32/489_2020_10_01.csv\n",
      "Shape of collected datafram: X_shape: (506, 13, 1024), Y_shape: (506,)\n",
      "24/120 folders downloaded\n",
      "downloaded folder: NR07_20201001_PGS_31_BSD_32/488_2020_10_01.csv\n",
      "Shape of collected datafram: X_shape: (528, 13, 1024), Y_shape: (528,)\n",
      "25/120 folders downloaded\n",
      "downloaded folder: NR07_20201001_PGS_31_BSD_32/495_2020_10_01.csv\n",
      "Shape of collected datafram: X_shape: (550, 13, 1024), Y_shape: (550,)\n",
      "26/120 folders downloaded\n",
      "downloaded folder: NR07_20201001_PGS_31_BSD_32/487_2020_10_01.csv\n",
      "Shape of collected datafram: X_shape: (572, 13, 1024), Y_shape: (572,)\n",
      "27/120 folders downloaded\n",
      "downloaded folder: NR07_20201001_PGS_31_BSD_32/492_2020_10_01.csv\n",
      "Shape of collected datafram: X_shape: (594, 13, 1024), Y_shape: (594,)\n",
      "28/120 folders downloaded\n",
      "downloaded folder: NR07_20201001_PGS_31_BSD_32/490_2020_10_01.csv\n",
      "Shape of collected datafram: X_shape: (616, 13, 1024), Y_shape: (616,)\n",
      "29/120 folders downloaded\n",
      "downloaded folder: NR07_20201001_PGS_31_BSD_32/496_2020_10_01.csv\n",
      "Shape of collected datafram: X_shape: (638, 13, 1024), Y_shape: (638,)\n",
      "30/120 folders downloaded\n",
      "downloaded folder: NR07_20201001_PGS_31_BSD_32/491_2020_10_01.csv\n",
      "Shape of collected datafram: X_shape: (660, 13, 1024), Y_shape: (660,)\n",
      "31/120 folders downloaded\n",
      "downloaded folder: NR09_20200917_PGS_31_BSD_P2/460_2020_09_17.csv\n",
      "Shape of collected datafram: X_shape: (682, 13, 1024), Y_shape: (682,)\n",
      "32/120 folders downloaded\n",
      "downloaded folder: NR09_20200917_PGS_31_BSD_P2/456_2020_09_17.csv\n",
      "Shape of collected datafram: X_shape: (704, 13, 1024), Y_shape: (704,)\n",
      "33/120 folders downloaded\n",
      "downloaded folder: NR09_20200917_PGS_31_BSD_P2/459_2020_09_17.csv\n",
      "Shape of collected datafram: X_shape: (726, 13, 1024), Y_shape: (726,)\n",
      "34/120 folders downloaded\n",
      "downloaded folder: NR09_20200917_PGS_31_BSD_P2/451_2020_09_17.csv\n",
      "Shape of collected datafram: X_shape: (748, 13, 1024), Y_shape: (748,)\n",
      "35/120 folders downloaded\n",
      "downloaded folder: NR09_20200917_PGS_31_BSD_P2/458_2020_09_17.csv\n",
      "Shape of collected datafram: X_shape: (770, 13, 1024), Y_shape: (770,)\n",
      "36/120 folders downloaded\n",
      "downloaded folder: NR09_20200917_PGS_31_BSD_P2/457_2020_09_17.csv\n",
      "Shape of collected datafram: X_shape: (792, 13, 1024), Y_shape: (792,)\n",
      "37/120 folders downloaded\n",
      "downloaded folder: NR09_20200917_PGS_31_BSD_P2/455_2020_09_17.csv\n",
      "Shape of collected datafram: X_shape: (814, 13, 1024), Y_shape: (814,)\n",
      "38/120 folders downloaded\n",
      "downloaded folder: NR09_20200917_PGS_31_BSD_P2/452_2020_09_17.csv\n",
      "Shape of collected datafram: X_shape: (836, 13, 1024), Y_shape: (836,)\n",
      "39/120 folders downloaded\n",
      "downloaded folder: NR09_20200917_PGS_31_BSD_P2/453_2020_09_17.csv\n",
      "Shape of collected datafram: X_shape: (858, 13, 1024), Y_shape: (858,)\n",
      "40/120 folders downloaded\n",
      "downloaded folder: NR09_20200917_PGS_31_BSD_P2/454_2020_09_17.csv\n",
      "Shape of collected datafram: X_shape: (880, 13, 1024), Y_shape: (880,)\n",
      "41/120 folders downloaded\n",
      "downloaded folder: NR14_20200731_PGS_21_BSD_22/423_2020_07_31.csv\n",
      "Shape of collected datafram: X_shape: (902, 13, 1024), Y_shape: (902,)\n",
      "42/120 folders downloaded\n",
      "downloaded folder: NR14_20200731_PGS_21_BSD_22/424_2020_07_31.csv\n",
      "Shape of collected datafram: X_shape: (924, 13, 1024), Y_shape: (924,)\n",
      "43/120 folders downloaded\n",
      "downloaded folder: NR14_20200731_PGS_21_BSD_22/422_2020_07_31.csv\n",
      "Shape of collected datafram: X_shape: (946, 13, 1024), Y_shape: (946,)\n",
      "44/120 folders downloaded\n",
      "downloaded folder: NR14_20200731_PGS_21_BSD_22/420_2020_07_31.csv\n",
      "Shape of collected datafram: X_shape: (968, 13, 1024), Y_shape: (968,)\n",
      "45/120 folders downloaded\n",
      "downloaded folder: NR14_20200731_PGS_21_BSD_22/421_2020_07_31.csv\n",
      "Shape of collected datafram: X_shape: (990, 13, 1024), Y_shape: (990,)\n",
      "46/120 folders downloaded\n",
      "downloaded folder: NR14_20200731_PGS_21_BSD_22/415_2020_07_31.csv\n",
      "Shape of collected datafram: X_shape: (1012, 13, 1024), Y_shape: (1012,)\n",
      "47/120 folders downloaded\n",
      "downloaded folder: NR14_20200731_PGS_21_BSD_22/416_2020_07_31.csv\n",
      "Shape of collected datafram: X_shape: (1034, 13, 1024), Y_shape: (1034,)\n",
      "48/120 folders downloaded\n",
      "downloaded folder: NR14_20200731_PGS_21_BSD_22/419_2020_07_31.csv\n",
      "Shape of collected datafram: X_shape: (1056, 13, 1024), Y_shape: (1056,)\n",
      "49/120 folders downloaded\n",
      "downloaded folder: NR14_20200731_PGS_21_BSD_22/418_2020_07_31.csv\n",
      "Shape of collected datafram: X_shape: (1078, 13, 1024), Y_shape: (1078,)\n",
      "50/120 folders downloaded\n",
      "downloaded folder: NR14_20200731_PGS_21_BSD_22/417_2020_07_31.csv\n",
      "Shape of collected datafram: X_shape: (1100, 13, 1024), Y_shape: (1100,)\n",
      "51/120 folders downloaded\n",
      "downloaded folder: NR15_20200901_PGS_21_BSD_12/435_2020_09_01.csv\n",
      "Shape of collected datafram: X_shape: (1122, 13, 1024), Y_shape: (1122,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/120 folders downloaded\n",
      "downloaded folder: NR15_20200901_PGS_21_BSD_12/428_2020_09_01.csv\n",
      "Shape of collected datafram: X_shape: (1144, 13, 1024), Y_shape: (1144,)\n",
      "53/120 folders downloaded\n",
      "downloaded folder: NR15_20200901_PGS_21_BSD_12/427_2020_09_01.csv\n",
      "Shape of collected datafram: X_shape: (1166, 13, 1024), Y_shape: (1166,)\n",
      "54/120 folders downloaded\n",
      "downloaded folder: NR15_20200901_PGS_21_BSD_12/432_2020_09_01.csv\n",
      "Shape of collected datafram: X_shape: (1188, 13, 1024), Y_shape: (1188,)\n",
      "55/120 folders downloaded\n",
      "downloaded folder: NR15_20200901_PGS_21_BSD_12/433_2020_09_01.csv\n",
      "Shape of collected datafram: X_shape: (1210, 13, 1024), Y_shape: (1210,)\n",
      "56/120 folders downloaded\n",
      "downloaded folder: NR15_20200901_PGS_21_BSD_12/429_2020_09_01.csv\n",
      "Shape of collected datafram: X_shape: (1232, 13, 1024), Y_shape: (1232,)\n",
      "57/120 folders downloaded\n",
      "downloaded folder: NR15_20200901_PGS_21_BSD_12/434_2020_09_01.csv\n",
      "Shape of collected datafram: X_shape: (1254, 13, 1024), Y_shape: (1254,)\n",
      "58/120 folders downloaded\n",
      "downloaded folder: NR15_20200901_PGS_21_BSD_12/436_2020_09_01.csv\n",
      "Shape of collected datafram: X_shape: (1276, 13, 1024), Y_shape: (1276,)\n",
      "59/120 folders downloaded\n",
      "downloaded folder: NR15_20200901_PGS_21_BSD_12/431_2020_09_01.csv\n",
      "Shape of collected datafram: X_shape: (1298, 13, 1024), Y_shape: (1298,)\n",
      "60/120 folders downloaded\n",
      "downloaded folder: NR15_20200901_PGS_21_BSD_12/430_2020_09_01.csv\n",
      "Shape of collected datafram: X_shape: (1320, 13, 1024), Y_shape: (1320,)\n",
      "61/120 folders downloaded\n",
      "downloaded folder: NR16_20200908_PGS_21_BSD_32/447_2020_09_08.csv\n",
      "Shape of collected datafram: X_shape: (1342, 13, 1024), Y_shape: (1342,)\n",
      "62/120 folders downloaded\n",
      "downloaded folder: NR16_20200908_PGS_21_BSD_32/448_2020_09_08.csv\n",
      "Shape of collected datafram: X_shape: (1364, 13, 1024), Y_shape: (1364,)\n",
      "63/120 folders downloaded\n",
      "downloaded folder: NR16_20200908_PGS_21_BSD_32/439_2020_09_08.csv\n",
      "Shape of collected datafram: X_shape: (1386, 13, 1024), Y_shape: (1386,)\n",
      "64/120 folders downloaded\n",
      "downloaded folder: NR16_20200908_PGS_21_BSD_32/440_2020_09_08.csv\n",
      "Shape of collected datafram: X_shape: (1408, 13, 1024), Y_shape: (1408,)\n",
      "65/120 folders downloaded\n",
      "downloaded folder: NR16_20200908_PGS_21_BSD_32/441_2020_09_08.csv\n",
      "Shape of collected datafram: X_shape: (1430, 13, 1024), Y_shape: (1430,)\n",
      "66/120 folders downloaded\n",
      "downloaded folder: NR16_20200908_PGS_21_BSD_32/446_2020_09_08.csv\n",
      "Shape of collected datafram: X_shape: (1452, 13, 1024), Y_shape: (1452,)\n",
      "67/120 folders downloaded\n",
      "downloaded folder: NR16_20200908_PGS_21_BSD_32/444_2020_09_08.csv\n",
      "Shape of collected datafram: X_shape: (1474, 13, 1024), Y_shape: (1474,)\n",
      "68/120 folders downloaded\n",
      "downloaded folder: NR16_20200908_PGS_21_BSD_32/443_2020_09_08.csv\n",
      "Shape of collected datafram: X_shape: (1496, 13, 1024), Y_shape: (1496,)\n",
      "69/120 folders downloaded\n",
      "downloaded folder: NR16_20200908_PGS_21_BSD_32/442_2020_09_08.csv\n",
      "Shape of collected datafram: X_shape: (1518, 13, 1024), Y_shape: (1518,)\n",
      "70/120 folders downloaded\n",
      "downloaded folder: NR16_20200908_PGS_21_BSD_32/445_2020_09_08.csv\n",
      "Shape of collected datafram: X_shape: (1540, 13, 1024), Y_shape: (1540,)\n",
      "71/120 folders downloaded\n",
      "downloaded folder: NR18_20200714_PGS_21_BSD_P2/400_2020_07_14.csv\n",
      "Shape of collected datafram: X_shape: (1562, 13, 1024), Y_shape: (1562,)\n",
      "72/120 folders downloaded\n",
      "downloaded folder: NR18_20200714_PGS_21_BSD_P2/394_2020_07_14.csv\n",
      "Shape of collected datafram: X_shape: (1584, 13, 1024), Y_shape: (1584,)\n",
      "73/120 folders downloaded\n",
      "downloaded folder: NR18_20200714_PGS_21_BSD_P2/393_2020_07_14.csv\n",
      "Shape of collected datafram: X_shape: (1606, 13, 1024), Y_shape: (1606,)\n",
      "74/120 folders downloaded\n",
      "downloaded folder: NR18_20200714_PGS_21_BSD_P2/392_2020_07_14.csv\n",
      "Shape of collected datafram: X_shape: (1628, 13, 1024), Y_shape: (1628,)\n",
      "75/120 folders downloaded\n",
      "downloaded folder: NR18_20200714_PGS_21_BSD_P2/395_2020_07_14.csv\n",
      "Shape of collected datafram: X_shape: (1650, 13, 1024), Y_shape: (1650,)\n",
      "76/120 folders downloaded\n",
      "downloaded folder: NR18_20200714_PGS_21_BSD_P2/397_2020_07_14.csv\n",
      "Shape of collected datafram: X_shape: (1672, 13, 1024), Y_shape: (1672,)\n",
      "77/120 folders downloaded\n",
      "downloaded folder: NR18_20200714_PGS_21_BSD_P2/398_2020_07_14.csv\n",
      "Shape of collected datafram: X_shape: (1694, 13, 1024), Y_shape: (1694,)\n",
      "78/120 folders downloaded\n",
      "downloaded folder: NR18_20200714_PGS_21_BSD_P2/391_2020_07_14.csv\n",
      "Shape of collected datafram: X_shape: (1716, 13, 1024), Y_shape: (1716,)\n",
      "79/120 folders downloaded\n",
      "downloaded folder: NR18_20200714_PGS_21_BSD_P2/399_2020_07_14.csv\n",
      "Shape of collected datafram: X_shape: (1738, 13, 1024), Y_shape: (1738,)\n",
      "80/120 folders downloaded\n",
      "downloaded folder: NR18_20200714_PGS_21_BSD_P2/396_2020_07_14.csv\n",
      "Shape of collected datafram: X_shape: (1760, 13, 1024), Y_shape: (1760,)\n",
      "81/120 folders downloaded\n",
      "downloaded folder: NR23_20200511_PGS_11_BSD_22/290_2020_05_11.csv\n",
      "Shape of collected datafram: X_shape: (1782, 13, 1024), Y_shape: (1782,)\n",
      "82/120 folders downloaded\n",
      "downloaded folder: NR23_20200511_PGS_11_BSD_22/291_2020_05_11.csv\n",
      "Shape of collected datafram: X_shape: (1804, 13, 1024), Y_shape: (1804,)\n",
      "83/120 folders downloaded\n",
      "downloaded folder: NR23_20200511_PGS_11_BSD_22/289_2020_05_11.csv\n",
      "Shape of collected datafram: X_shape: (1826, 13, 1024), Y_shape: (1826,)\n",
      "84/120 folders downloaded\n",
      "downloaded folder: NR23_20200511_PGS_11_BSD_22/294_2020_05_11.csv\n",
      "Shape of collected datafram: X_shape: (1848, 13, 1024), Y_shape: (1848,)\n",
      "85/120 folders downloaded\n",
      "downloaded folder: NR23_20200511_PGS_11_BSD_22/286_2020_05_11.csv\n",
      "Shape of collected datafram: X_shape: (1870, 13, 1024), Y_shape: (1870,)\n",
      "86/120 folders downloaded\n",
      "downloaded folder: NR23_20200511_PGS_11_BSD_22/293_2020_05_11.csv\n",
      "Shape of collected datafram: X_shape: (1892, 13, 1024), Y_shape: (1892,)\n",
      "87/120 folders downloaded\n",
      "downloaded folder: NR23_20200511_PGS_11_BSD_22/292_2020_05_11.csv\n",
      "Shape of collected datafram: X_shape: (1914, 13, 1024), Y_shape: (1914,)\n",
      "88/120 folders downloaded\n",
      "downloaded folder: NR23_20200511_PGS_11_BSD_22/287_2020_05_11.csv\n",
      "Shape of collected datafram: X_shape: (1936, 13, 1024), Y_shape: (1936,)\n",
      "89/120 folders downloaded\n",
      "downloaded folder: NR23_20200511_PGS_11_BSD_22/295_2020_05_11.csv\n",
      "Shape of collected datafram: X_shape: (1958, 13, 1024), Y_shape: (1958,)\n",
      "90/120 folders downloaded\n",
      "downloaded folder: NR23_20200511_PGS_11_BSD_22/288_2020_05_11.csv\n",
      "Shape of collected datafram: X_shape: (1980, 13, 1024), Y_shape: (1980,)\n",
      "91/120 folders downloaded\n",
      "downloaded folder: NR24_20200512_PGS_11_BSD_12/315_2020_05_12.csv\n",
      "Shape of collected datafram: X_shape: (2002, 13, 1024), Y_shape: (2002,)\n",
      "92/120 folders downloaded\n",
      "downloaded folder: NR24_20200512_PGS_11_BSD_12/312_2020_05_12.csv\n",
      "Shape of collected datafram: X_shape: (2024, 13, 1024), Y_shape: (2024,)\n",
      "93/120 folders downloaded\n",
      "downloaded folder: NR24_20200512_PGS_11_BSD_12/313_2020_05_12.csv\n",
      "Shape of collected datafram: X_shape: (2046, 13, 1024), Y_shape: (2046,)\n",
      "94/120 folders downloaded\n",
      "downloaded folder: NR24_20200512_PGS_11_BSD_12/314_2020_05_12.csv\n",
      "Shape of collected datafram: X_shape: (2068, 13, 1024), Y_shape: (2068,)\n",
      "95/120 folders downloaded\n",
      "downloaded folder: NR24_20200512_PGS_11_BSD_12/309_2020_05_12.csv\n",
      "Shape of collected datafram: X_shape: (2090, 13, 1024), Y_shape: (2090,)\n",
      "96/120 folders downloaded\n",
      "downloaded folder: NR24_20200512_PGS_11_BSD_12/316_2020_05_12.csv\n",
      "Shape of collected datafram: X_shape: (2112, 13, 1024), Y_shape: (2112,)\n",
      "97/120 folders downloaded\n",
      "downloaded folder: NR24_20200512_PGS_11_BSD_12/311_2020_05_12.csv\n",
      "Shape of collected datafram: X_shape: (2134, 13, 1024), Y_shape: (2134,)\n",
      "98/120 folders downloaded\n",
      "downloaded folder: NR24_20200512_PGS_11_BSD_12/318_2020_05_12.csv\n",
      "Shape of collected datafram: X_shape: (2156, 13, 1024), Y_shape: (2156,)\n",
      "99/120 folders downloaded\n",
      "downloaded folder: NR24_20200512_PGS_11_BSD_12/310_2020_05_12.csv\n",
      "Shape of collected datafram: X_shape: (2178, 13, 1024), Y_shape: (2178,)\n",
      "100/120 folders downloaded\n",
      "downloaded folder: NR24_20200512_PGS_11_BSD_12/317_2020_05_12.csv\n",
      "Shape of collected datafram: X_shape: (2200, 13, 1024), Y_shape: (2200,)\n",
      "101/120 folders downloaded\n",
      "downloaded folder: NR25_20200512_PGS_11_BSD_32/336_2020_05_12.csv\n",
      "Shape of collected datafram: X_shape: (2222, 13, 1024), Y_shape: (2222,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/120 folders downloaded\n",
      "downloaded folder: NR25_20200512_PGS_11_BSD_32/339_2020_05_12.csv\n",
      "Shape of collected datafram: X_shape: (2244, 13, 1024), Y_shape: (2244,)\n",
      "103/120 folders downloaded\n",
      "downloaded folder: NR25_20200512_PGS_11_BSD_32/340_2020_05_12.csv\n",
      "Shape of collected datafram: X_shape: (2266, 13, 1024), Y_shape: (2266,)\n",
      "104/120 folders downloaded\n",
      "downloaded folder: NR25_20200512_PGS_11_BSD_32/341_2020_05_12.csv\n",
      "Shape of collected datafram: X_shape: (2288, 13, 1024), Y_shape: (2288,)\n",
      "105/120 folders downloaded\n",
      "downloaded folder: NR25_20200512_PGS_11_BSD_32/338_2020_05_12.csv\n",
      "Shape of collected datafram: X_shape: (2310, 13, 1024), Y_shape: (2310,)\n",
      "106/120 folders downloaded\n",
      "downloaded folder: NR25_20200512_PGS_11_BSD_32/337_2020_05_12.csv\n",
      "Shape of collected datafram: X_shape: (2332, 13, 1024), Y_shape: (2332,)\n",
      "107/120 folders downloaded\n",
      "downloaded folder: NR25_20200512_PGS_11_BSD_32/335_2020_05_12.csv\n",
      "Shape of collected datafram: X_shape: (2354, 13, 1024), Y_shape: (2354,)\n",
      "108/120 folders downloaded\n",
      "downloaded folder: NR25_20200512_PGS_11_BSD_32/332_2020_05_12.csv\n",
      "Shape of collected datafram: X_shape: (2376, 13, 1024), Y_shape: (2376,)\n",
      "109/120 folders downloaded\n",
      "downloaded folder: NR25_20200512_PGS_11_BSD_32/333_2020_05_12.csv\n",
      "Shape of collected datafram: X_shape: (2398, 13, 1024), Y_shape: (2398,)\n",
      "110/120 folders downloaded\n",
      "downloaded folder: NR25_20200512_PGS_11_BSD_32/334_2020_05_12.csv\n",
      "Shape of collected datafram: X_shape: (2420, 13, 1024), Y_shape: (2420,)\n",
      "111/120 folders downloaded\n",
      "downloaded folder: NR27_20200513_PGS_11_BSD_P2/379_2020_05_13.csv\n",
      "Shape of collected datafram: X_shape: (2442, 13, 1024), Y_shape: (2442,)\n",
      "112/120 folders downloaded\n",
      "downloaded folder: NR27_20200513_PGS_11_BSD_P2/386_2020_05_13.csv\n",
      "Shape of collected datafram: X_shape: (2464, 13, 1024), Y_shape: (2464,)\n",
      "113/120 folders downloaded\n",
      "downloaded folder: NR27_20200513_PGS_11_BSD_P2/381_2020_05_13.csv\n",
      "Shape of collected datafram: X_shape: (2486, 13, 1024), Y_shape: (2486,)\n",
      "114/120 folders downloaded\n",
      "downloaded folder: NR27_20200513_PGS_11_BSD_P2/380_2020_05_13.csv\n",
      "Shape of collected datafram: X_shape: (2508, 13, 1024), Y_shape: (2508,)\n",
      "115/120 folders downloaded\n",
      "downloaded folder: NR27_20200513_PGS_11_BSD_P2/378_2020_05_13.csv\n",
      "Shape of collected datafram: X_shape: (2530, 13, 1024), Y_shape: (2530,)\n",
      "116/120 folders downloaded\n",
      "downloaded folder: NR27_20200513_PGS_11_BSD_P2/387_2020_05_13.csv\n",
      "Shape of collected datafram: X_shape: (2552, 13, 1024), Y_shape: (2552,)\n",
      "117/120 folders downloaded\n",
      "downloaded folder: NR27_20200513_PGS_11_BSD_P2/385_2020_05_13.csv\n",
      "Shape of collected datafram: X_shape: (2574, 13, 1024), Y_shape: (2574,)\n",
      "118/120 folders downloaded\n",
      "downloaded folder: NR27_20200513_PGS_11_BSD_P2/382_2020_05_13.csv\n",
      "Shape of collected datafram: X_shape: (2596, 13, 1024), Y_shape: (2596,)\n",
      "119/120 folders downloaded\n",
      "downloaded folder: NR27_20200513_PGS_11_BSD_P2/383_2020_05_13.csv\n",
      "Shape of collected datafram: X_shape: (2618, 13, 1024), Y_shape: (2618,)\n",
      "120/120 folders downloaded\n",
      "downloaded folder: NR27_20200513_PGS_11_BSD_P2/384_2020_05_13.csv\n",
      "Shape of collected datafram: X_shape: (2640, 13, 1024), Y_shape: (2640,)\n",
      "1/120 folders downloaded\n",
      "downloaded folder: NR01_20200317_PGS_31_BSD_31/020_2020_03_18.csv\n",
      "Shape of collected datafram: X_shape: (22, 13, 1024), Y_shape: (22,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rg/g_9b4q1j0h94scy_c8tdgd980000gn/T/ipykernel_1205/2406018480.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#DATASET TRAIN (domain = \"train\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeSeriesData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/rg/g_9b4q1j0h94scy_c8tdgd980000gn/T/ipykernel_1205/1297641108.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, domain)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate_data_from_BSD_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolders_domain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_of_interest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/rg/g_9b4q1j0h94scy_c8tdgd980000gn/T/ipykernel_1205/2720319543.py\u001b[0m in \u001b[0;36mconcatenate_data_from_BSD_state\u001b[0;34m(folders, data_path, features_of_interest, window_size, overlap_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_BSD_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mdata_BSD_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_BSD_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#write csv in numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mfeature_index_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_of_interest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#get index for all features of interest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mdata_BSD_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_BSD_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_index_list\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#slice numpy array such that just features of interest are included\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, like)\u001b[0m\n\u001b[1;32m   2252\u001b[0m                         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mttype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2253\u001b[0m             \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2254\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2255\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0musemask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2256\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#DATASET TEST (domain = \"test\")\n",
    "dataset_test = TimeSeriesData(\"test\")\n",
    "\n",
    "#DATASET TRAIN (domain = \"train\")\n",
    "dataset_train = TimeSeriesData(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        formula [(W−K+2P)/S]+1.\n",
    "        \"\"\"\n",
    "        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=100, stride=1)#input: 1024\n",
    "        self.conv2 = nn.Conv1d(64,32,kernel_size=10, stride = 1, padding=1)#input: [(1024-100+2*0)/1]+1 = 925\n",
    "        self.batch1 =nn.BatchNorm1d(32)#input: [(925-10+2*1)/1]+1 = 918\n",
    "        self.conv3 = nn.Conv1d(32,32,kernel_size=5, stride = 1, padding=1) #input:918\n",
    "        self.batch2 =nn.BatchNorm1d(32)#input: [(918-5+2*1)/1]+1 = 916\n",
    "        self.fc1 = nn.Linear(32*916, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.selu(self.conv1(x)) #conv1\n",
    "        x = self.conv2(x) #conv2\n",
    "        x = F.selu(self.batch1(x)) #batch1\n",
    "        x = self.conv3(x) #conv3\n",
    "        x = F.selu(self.batch2(x)) #batch2\n",
    "        x = torch.reshape(x,(x.shape[0],x.shape[1]*x.shape[2])) #flatten\n",
    "        x = self.fc1(x) #linear1\n",
    "        output = x\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv1d(13, 64, kernel_size=(100,), stride=(1,))\n",
      "  (conv2): Conv1d(64, 32, kernel_size=(10,), stride=(1,), padding=(1,))\n",
      "  (batch1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(1,))\n",
      "  (batch2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=29312, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 13\n",
    "output_size = 4\n",
    "hidden_size = 1000\n",
    "num_layers = 2\n",
    "\n",
    "model = CNN(input_size, output_size)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rg/g_9b4q1j0h94scy_c8tdgd980000gn/T/ipykernel_1205/76036566.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# define train/val dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvalidation_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#split dataset randomly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_train' is not defined"
     ]
    }
   ],
   "source": [
    "# define train/val dimensions\n",
    "train_size = int(0.8 * len(dataset_train))\n",
    "validation_size = len(dataset_train) - train_size\n",
    "\n",
    "#split dataset randomly\n",
    "training_dataset, validation_dataset = torch.utils.data.random_split(dataset_train, [train_size, validation_size])\n",
    "\n",
    "#define batch size for dataloader\n",
    "batch_size = 4\n",
    "\n",
    "#dataloader\n",
    "train_loader = DataLoader(dataset=training_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "\n",
    "val_loader = DataLoader(dataset=validation_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "test_loader = DataLoader(dataset=dataset_test,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "\n",
    "data_loader = {}\n",
    "data_loader['train'] = train_loader\n",
    "data_loader['val'] = val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "writer = SummaryWriter('runs/Dataloader2')\n",
    "\n",
    "#define training params\n",
    "num_epochs = 100\n",
    "learning_rate = 0.008\n",
    "\n",
    "#define loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#collect loss for each batch\n",
    "loss_list = []\n",
    "\n",
    "#collect accuracy\n",
    "running_correct = 0\n",
    "\n",
    "#information for plotting\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "\n",
    "############## TENSORBOARD ########################\n",
    "examples = iter(test_loader)\n",
    "example_data, example_targets = examples.next()\n",
    "\n",
    "writer.add_graph(model, example_data.float())\n",
    "###################################################\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (window, labels) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(window.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #get current accuracy\n",
    "        output = outputs.argmax(dim=1)\n",
    "        running_correct += (output == labels).sum().item()\n",
    "        \n",
    "        #plot information during training\n",
    "        if (i+1) % 20 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "            ############## TENSORBOARD ########################\n",
    "            writer.add_scalar('training loss', loss / 20, epoch * n_total_steps + i)\n",
    "            running_accuracy = running_correct / 20 / output.size(0)\n",
    "            writer.add_scalar('accuracy', running_accuracy, epoch * n_total_steps + i)\n",
    "            running_correct = 0\n",
    "            running_loss = 0.0\n",
    "            ###################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np_loss_list = []\n",
    "loss_total_epoch = 0\n",
    "\n",
    "#iterate through collected loss during training\n",
    "for iterator, loss in enumerate(loss_list):\n",
    "    \n",
    "    #remove the computational graph of the torch tensor and transform torch to numpy tensor\n",
    "    loss_total_epoch += loss.detach().numpy()\n",
    "    \n",
    "    #sum up the batch losses from each epoch\n",
    "    if iterator%len(train_loader) == 0:\n",
    "        np_loss_list.append(loss_total_epoch)\n",
    "        loss_total_epoch =0 \n",
    "\n",
    "#plot loss\n",
    "plt.plot(np_loss_list)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400 successfull\n",
      "Epoch 2/400 successfull\n",
      "Epoch 3/400 successfull\n",
      "Epoch 4/400 successfull\n",
      "Epoch 5/400 successfull\n",
      "Epoch 6/400 successfull\n",
      "Epoch 7/400 successfull\n",
      "Epoch 8/400 successfull\n",
      "Epoch 9/400 successfull\n",
      "Epoch 10/400 successfull\n",
      "Epoch 11/400 successfull\n",
      "Epoch 12/400 successfull\n",
      "Epoch 13/400 successfull\n",
      "Epoch 14/400 successfull\n",
      "Epoch 15/400 successfull\n",
      "Epoch 16/400 successfull\n",
      "Epoch 17/400 successfull\n",
      "Epoch 18/400 successfull\n",
      "Epoch 19/400 successfull\n",
      "Epoch 20/400 successfull\n",
      "Epoch 21/400 successfull\n",
      "Epoch 22/400 successfull\n",
      "Epoch 23/400 successfull\n",
      "Epoch 24/400 successfull\n",
      "Epoch 25/400 successfull\n",
      "Epoch 26/400 successfull\n",
      "Epoch 27/400 successfull\n",
      "Epoch 28/400 successfull\n",
      "Epoch 29/400 successfull\n",
      "Epoch 30/400 successfull\n",
      "Epoch 31/400 successfull\n",
      "Epoch 32/400 successfull\n",
      "Epoch 33/400 successfull\n",
      "Epoch 34/400 successfull\n",
      "Epoch 35/400 successfull\n",
      "Epoch 36/400 successfull\n",
      "Epoch 37/400 successfull\n",
      "Epoch 38/400 successfull\n",
      "Epoch 39/400 successfull\n",
      "Epoch 40/400 successfull\n",
      "Epoch 41/400 successfull\n",
      "Epoch 42/400 successfull\n",
      "Epoch 43/400 successfull\n",
      "Epoch 44/400 successfull\n",
      "Epoch 45/400 successfull\n",
      "Epoch 46/400 successfull\n",
      "Epoch 47/400 successfull\n",
      "Epoch 48/400 successfull\n",
      "Epoch 49/400 successfull\n",
      "Epoch 50/400 successfull\n",
      "Epoch 51/400 successfull\n",
      "Epoch 52/400 successfull\n",
      "Epoch 53/400 successfull\n",
      "Epoch 54/400 successfull\n",
      "Epoch 55/400 successfull\n",
      "Epoch 56/400 successfull\n",
      "Epoch 57/400 successfull\n",
      "Epoch 58/400 successfull\n",
      "Epoch 59/400 successfull\n",
      "Epoch 60/400 successfull\n",
      "Epoch 61/400 successfull\n",
      "Epoch 62/400 successfull\n",
      "Epoch 63/400 successfull\n",
      "Epoch 64/400 successfull\n",
      "Epoch 65/400 successfull\n",
      "Epoch 66/400 successfull\n",
      "Epoch 67/400 successfull\n",
      "Epoch 68/400 successfull\n",
      "Epoch 69/400 successfull\n",
      "Epoch 70/400 successfull\n",
      "Epoch 71/400 successfull\n",
      "Epoch 72/400 successfull\n",
      "Epoch 73/400 successfull\n",
      "Epoch 74/400 successfull\n",
      "Epoch 75/400 successfull\n",
      "Epoch 76/400 successfull\n",
      "Epoch 77/400 successfull\n",
      "Epoch 78/400 successfull\n",
      "Epoch 79/400 successfull\n",
      "Epoch 80/400 successfull\n",
      "Epoch 81/400 successfull\n",
      "Epoch 82/400 successfull\n",
      "Epoch 83/400 successfull\n",
      "Epoch 84/400 successfull\n",
      "Epoch 85/400 successfull\n",
      "Epoch 86/400 successfull\n",
      "Epoch 87/400 successfull\n",
      "Epoch 88/400 successfull\n",
      "Epoch 89/400 successfull\n",
      "Epoch 90/400 successfull\n",
      "Epoch 91/400 successfull\n",
      "Epoch 92/400 successfull\n",
      "Epoch 93/400 successfull\n",
      "Epoch 94/400 successfull\n",
      "Epoch 95/400 successfull\n",
      "Epoch 96/400 successfull\n",
      "Epoch 97/400 successfull\n",
      "Epoch 98/400 successfull\n",
      "Epoch 99/400 successfull\n",
      "Epoch 100/400 successfull\n",
      "Epoch 101/400 successfull\n",
      "Epoch 102/400 successfull\n",
      "Epoch 103/400 successfull\n",
      "Epoch 104/400 successfull\n",
      "Epoch 105/400 successfull\n",
      "Epoch 106/400 successfull\n",
      "Epoch 107/400 successfull\n",
      "Epoch 108/400 successfull\n",
      "Epoch 109/400 successfull\n",
      "Epoch 110/400 successfull\n",
      "Epoch 111/400 successfull\n",
      "Epoch 112/400 successfull\n",
      "Epoch 113/400 successfull\n",
      "Epoch 114/400 successfull\n",
      "Epoch 115/400 successfull\n",
      "Epoch 116/400 successfull\n",
      "Epoch 117/400 successfull\n",
      "Epoch 118/400 successfull\n",
      "Epoch 119/400 successfull\n",
      "Epoch 120/400 successfull\n",
      "Epoch 121/400 successfull\n",
      "Epoch 122/400 successfull\n",
      "Epoch 123/400 successfull\n",
      "Epoch 124/400 successfull\n",
      "Epoch 125/400 successfull\n",
      "Epoch 126/400 successfull\n",
      "Epoch 127/400 successfull\n",
      "Epoch 128/400 successfull\n",
      "Epoch 129/400 successfull\n",
      "Epoch 130/400 successfull\n",
      "Epoch 131/400 successfull\n",
      "Epoch 132/400 successfull\n",
      "Epoch 133/400 successfull\n",
      "Epoch 134/400 successfull\n",
      "Epoch 135/400 successfull\n",
      "Epoch 136/400 successfull\n",
      "Epoch 137/400 successfull\n",
      "Epoch 138/400 successfull\n",
      "Epoch 139/400 successfull\n",
      "Epoch 140/400 successfull\n",
      "Epoch 141/400 successfull\n",
      "Epoch 142/400 successfull\n",
      "Epoch 143/400 successfull\n",
      "Epoch 144/400 successfull\n",
      "Epoch 145/400 successfull\n",
      "Epoch 146/400 successfull\n",
      "Epoch 147/400 successfull\n",
      "Epoch 148/400 successfull\n",
      "Epoch 149/400 successfull\n",
      "Epoch 150/400 successfull\n",
      "Epoch 151/400 successfull\n",
      "Epoch 152/400 successfull\n",
      "Epoch 153/400 successfull\n",
      "Epoch 154/400 successfull\n",
      "Epoch 155/400 successfull\n",
      "Epoch 156/400 successfull\n",
      "Epoch 157/400 successfull\n",
      "Epoch 158/400 successfull\n",
      "Epoch 159/400 successfull\n",
      "Epoch 160/400 successfull\n",
      "Epoch 161/400 successfull\n",
      "Epoch 162/400 successfull\n",
      "Epoch 163/400 successfull\n",
      "Epoch 164/400 successfull\n",
      "Epoch 165/400 successfull\n",
      "Epoch 166/400 successfull\n",
      "Epoch 167/400 successfull\n",
      "Epoch 168/400 successfull\n",
      "Epoch 169/400 successfull\n",
      "Epoch 170/400 successfull\n",
      "Epoch 171/400 successfull\n",
      "Epoch 172/400 successfull\n",
      "Epoch 173/400 successfull\n",
      "Epoch 174/400 successfull\n",
      "Epoch 175/400 successfull\n",
      "Epoch 176/400 successfull\n",
      "Epoch 177/400 successfull\n",
      "Epoch 178/400 successfull\n",
      "Epoch 179/400 successfull\n",
      "Epoch 180/400 successfull\n",
      "Epoch 181/400 successfull\n",
      "Epoch 182/400 successfull\n",
      "Epoch 183/400 successfull\n",
      "Epoch 184/400 successfull\n",
      "Epoch 185/400 successfull\n",
      "Epoch 186/400 successfull\n",
      "Epoch 187/400 successfull\n",
      "Epoch 188/400 successfull\n",
      "Epoch 189/400 successfull\n",
      "Epoch 190/400 successfull\n",
      "Epoch 191/400 successfull\n",
      "Epoch 192/400 successfull\n",
      "Epoch 193/400 successfull\n",
      "Epoch 194/400 successfull\n",
      "Epoch 195/400 successfull\n",
      "Epoch 196/400 successfull\n",
      "Epoch 197/400 successfull\n",
      "Epoch 198/400 successfull\n",
      "Epoch 199/400 successfull\n",
      "Epoch 200/400 successfull\n",
      "Epoch 201/400 successfull\n",
      "Epoch 202/400 successfull\n",
      "Epoch 203/400 successfull\n",
      "Epoch 204/400 successfull\n",
      "Epoch 205/400 successfull\n",
      "Epoch 206/400 successfull\n",
      "Epoch 207/400 successfull\n",
      "Epoch 208/400 successfull\n",
      "Epoch 209/400 successfull\n",
      "Epoch 210/400 successfull\n",
      "Epoch 211/400 successfull\n",
      "Epoch 212/400 successfull\n",
      "Epoch 213/400 successfull\n",
      "Epoch 214/400 successfull\n",
      "Epoch 215/400 successfull\n",
      "Epoch 216/400 successfull\n",
      "Epoch 217/400 successfull\n",
      "Epoch 218/400 successfull\n",
      "Epoch 219/400 successfull\n",
      "Epoch 220/400 successfull\n",
      "Epoch 221/400 successfull\n",
      "Epoch 222/400 successfull\n",
      "Epoch 223/400 successfull\n",
      "Epoch 224/400 successfull\n",
      "Epoch 225/400 successfull\n",
      "Epoch 226/400 successfull\n",
      "Epoch 227/400 successfull\n",
      "Epoch 228/400 successfull\n",
      "Epoch 229/400 successfull\n",
      "Epoch 230/400 successfull\n",
      "Epoch 231/400 successfull\n",
      "Epoch 232/400 successfull\n",
      "Epoch 233/400 successfull\n",
      "Epoch 234/400 successfull\n",
      "Epoch 235/400 successfull\n",
      "Epoch 236/400 successfull\n",
      "Epoch 237/400 successfull\n",
      "Epoch 238/400 successfull\n",
      "Epoch 239/400 successfull\n",
      "Epoch 240/400 successfull\n",
      "Epoch 241/400 successfull\n",
      "Epoch 242/400 successfull\n",
      "Epoch 243/400 successfull\n",
      "Epoch 244/400 successfull\n",
      "Epoch 245/400 successfull\n",
      "Epoch 246/400 successfull\n",
      "Epoch 247/400 successfull\n",
      "Epoch 248/400 successfull\n",
      "Epoch 249/400 successfull\n",
      "Epoch 250/400 successfull\n",
      "Epoch 251/400 successfull\n",
      "Epoch 252/400 successfull\n",
      "Epoch 253/400 successfull\n",
      "Epoch 254/400 successfull\n",
      "Epoch 255/400 successfull\n",
      "Epoch 256/400 successfull\n",
      "Epoch 257/400 successfull\n",
      "Epoch 258/400 successfull\n",
      "Epoch 259/400 successfull\n",
      "Epoch 260/400 successfull\n",
      "Epoch 261/400 successfull\n",
      "Epoch 262/400 successfull\n",
      "Epoch 263/400 successfull\n",
      "Epoch 264/400 successfull\n",
      "Epoch 265/400 successfull\n",
      "Epoch 266/400 successfull\n",
      "Epoch 267/400 successfull\n",
      "Epoch 268/400 successfull\n",
      "Epoch 269/400 successfull\n",
      "Epoch 270/400 successfull\n",
      "Epoch 271/400 successfull\n",
      "Epoch 272/400 successfull\n",
      "Epoch 273/400 successfull\n",
      "Epoch 274/400 successfull\n",
      "Epoch 275/400 successfull\n",
      "Epoch 276/400 successfull\n",
      "Epoch 277/400 successfull\n",
      "Epoch 278/400 successfull\n",
      "Epoch 279/400 successfull\n",
      "Epoch 280/400 successfull\n",
      "Epoch 281/400 successfull\n",
      "Epoch 282/400 successfull\n",
      "Epoch 283/400 successfull\n",
      "Epoch 284/400 successfull\n",
      "Epoch 285/400 successfull\n",
      "Epoch 286/400 successfull\n",
      "Epoch 287/400 successfull\n",
      "Epoch 288/400 successfull\n",
      "Epoch 289/400 successfull\n",
      "Epoch 290/400 successfull\n",
      "Epoch 291/400 successfull\n",
      "Epoch 292/400 successfull\n",
      "Epoch 293/400 successfull\n",
      "Epoch 294/400 successfull\n",
      "Epoch 295/400 successfull\n",
      "Epoch 296/400 successfull\n",
      "Epoch 297/400 successfull\n",
      "Epoch 298/400 successfull\n",
      "Epoch 299/400 successfull\n",
      "Epoch 300/400 successfull\n",
      "Epoch 301/400 successfull\n",
      "Epoch 302/400 successfull\n",
      "Epoch 303/400 successfull\n",
      "Epoch 304/400 successfull\n",
      "Epoch 305/400 successfull\n",
      "Epoch 306/400 successfull\n",
      "Epoch 307/400 successfull\n",
      "Epoch 308/400 successfull\n",
      "Epoch 309/400 successfull\n",
      "Epoch 310/400 successfull\n",
      "Epoch 311/400 successfull\n",
      "Epoch 312/400 successfull\n",
      "Epoch 313/400 successfull\n",
      "Epoch 314/400 successfull\n",
      "Epoch 315/400 successfull\n",
      "Epoch 316/400 successfull\n",
      "Epoch 317/400 successfull\n",
      "Epoch 318/400 successfull\n",
      "Epoch 319/400 successfull\n",
      "Epoch 320/400 successfull\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321/400 successfull\n",
      "Epoch 322/400 successfull\n",
      "Epoch 323/400 successfull\n",
      "Epoch 324/400 successfull\n",
      "Epoch 325/400 successfull\n",
      "Epoch 326/400 successfull\n",
      "Epoch 327/400 successfull\n",
      "Epoch 328/400 successfull\n",
      "Epoch 329/400 successfull\n",
      "Epoch 330/400 successfull\n",
      "Epoch 331/400 successfull\n",
      "Epoch 332/400 successfull\n",
      "Epoch 333/400 successfull\n",
      "Epoch 334/400 successfull\n",
      "Epoch 335/400 successfull\n",
      "Epoch 336/400 successfull\n",
      "Epoch 337/400 successfull\n",
      "Epoch 338/400 successfull\n",
      "Epoch 339/400 successfull\n",
      "Epoch 340/400 successfull\n",
      "Epoch 341/400 successfull\n",
      "Epoch 342/400 successfull\n",
      "Epoch 343/400 successfull\n",
      "Epoch 344/400 successfull\n",
      "Epoch 345/400 successfull\n",
      "Epoch 346/400 successfull\n",
      "Epoch 347/400 successfull\n",
      "Epoch 348/400 successfull\n",
      "Epoch 349/400 successfull\n",
      "Epoch 350/400 successfull\n",
      "Epoch 351/400 successfull\n",
      "Epoch 352/400 successfull\n",
      "Epoch 353/400 successfull\n",
      "Epoch 354/400 successfull\n",
      "Epoch 355/400 successfull\n",
      "Epoch 356/400 successfull\n",
      "Epoch 357/400 successfull\n",
      "Epoch 358/400 successfull\n",
      "Epoch 359/400 successfull\n",
      "Epoch 360/400 successfull\n",
      "Epoch 361/400 successfull\n",
      "Epoch 362/400 successfull\n",
      "Epoch 363/400 successfull\n",
      "Epoch 364/400 successfull\n",
      "Epoch 365/400 successfull\n",
      "Epoch 366/400 successfull\n",
      "Epoch 367/400 successfull\n",
      "Epoch 368/400 successfull\n",
      "Epoch 369/400 successfull\n",
      "Epoch 370/400 successfull\n",
      "Epoch 371/400 successfull\n",
      "Epoch 372/400 successfull\n",
      "Epoch 373/400 successfull\n",
      "Epoch 374/400 successfull\n",
      "Epoch 375/400 successfull\n",
      "Epoch 376/400 successfull\n",
      "Epoch 377/400 successfull\n",
      "Epoch 378/400 successfull\n",
      "Epoch 379/400 successfull\n",
      "Epoch 380/400 successfull\n",
      "Epoch 381/400 successfull\n",
      "Epoch 382/400 successfull\n",
      "Epoch 383/400 successfull\n",
      "Epoch 384/400 successfull\n",
      "Epoch 385/400 successfull\n",
      "Epoch 386/400 successfull\n",
      "Epoch 387/400 successfull\n",
      "Epoch 388/400 successfull\n",
      "Epoch 389/400 successfull\n",
      "Epoch 390/400 successfull\n",
      "Epoch 391/400 successfull\n",
      "Epoch 392/400 successfull\n",
      "Epoch 393/400 successfull\n",
      "Epoch 394/400 successfull\n",
      "Epoch 395/400 successfull\n",
      "Epoch 396/400 successfull\n",
      "Epoch 397/400 successfull\n",
      "Epoch 398/400 successfull\n",
      "Epoch 399/400 successfull\n",
      "Epoch 400/400 successfull\n"
     ]
    }
   ],
   "source": [
    "writer_graph = SummaryWriter('runs/Dataloader2/graph')\n",
    "writer_train = SummaryWriter('runs/Dataloader2/train')\n",
    "writer_val = SummaryWriter('runs/Dataloader2/val')\n",
    "writer = {}\n",
    "writer[\"train\"] = writer_train\n",
    "writer[\"val\"] = writer_val\n",
    "\n",
    "\n",
    "#define training params\n",
    "num_epochs = 400\n",
    "learning_rate = 0.008\n",
    "\n",
    "#define loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#collect loss for each batch\n",
    "loss_collected = 0\n",
    "loss_list = {}\n",
    "loss_list['train']=[]\n",
    "loss_list['val']=[]\n",
    "\n",
    "#collect accuracy for each batch\n",
    "correct_prediction_collected = 0\n",
    "accuracy_list={}\n",
    "accuracy_list['train']=[]\n",
    "accuracy_list['val']=[]\n",
    "\n",
    "\n",
    "#information for plotting\n",
    "n_total_steps = len(train_loader)\n",
    "len_data_loader={}\n",
    "len_data_loader['train'] = len(train_loader)\n",
    "len_data_loader['val'] = len(val_loader)\n",
    "\n",
    "\n",
    "############## TENSORBOARD ########################\n",
    "examples = iter(val_loader)\n",
    "example_data, example_targets = examples.next()\n",
    "\n",
    "writer_graph.add_graph(model, example_data.float())\n",
    "###################################################\n",
    "\n",
    "# Train and Validate the model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "\n",
    "    for phase in [\"train\", \"val\"]:\n",
    "        for i, (window, labels) in enumerate(data_loader[phase]):\n",
    "            \n",
    "            if phase == \"val\":\n",
    "                \n",
    "                model.train(False) #no training\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    ########Forward pass########\n",
    "                    outputs = model(window.float())\n",
    "                \n",
    "                    #collect loss\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss_collected += loss\n",
    "                    \n",
    "            else:\n",
    "                \n",
    "                model.train(True) #training \n",
    "                \n",
    "                ########Forward pass########\n",
    "                outputs = model(window.float())\n",
    "\n",
    "                #collect loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss_collected += loss\n",
    "\n",
    "                ########Backward pass########\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            #collect accuracy\n",
    "            output = outputs.argmax(dim=1)\n",
    "            correct_prediction_collected += (output == labels).sum().item()\n",
    "\n",
    "\n",
    "            #plot information during training\n",
    "            #if (i+1) % 20 == 0:\n",
    "            #    print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        ############## TENSORBOARD ########################\n",
    "        running_loss = loss_collected / len_data_loader[phase]\n",
    "        if phase == \"train\":\n",
    "            running_loss = running_loss.detach()\n",
    "        loss_list[phase].append(running_loss)\n",
    "        \n",
    "        \n",
    "        running_accuracy = correct_prediction_collected / len_data_loader[phase] / output.size(0)\n",
    "        accuracy_list[phase].append(running_accuracy)\n",
    "        \n",
    "        \n",
    "        #writer.add_scalar(f'training loss {phase}', epoch)\n",
    "        #writer.add_scalar(f'accuracy {phase}', running_accuracy, epoch)\n",
    "        \n",
    "        writer[phase].add_scalar(f'training loss', running_loss, epoch)\n",
    "        writer[phase].add_scalar(f'accuracy', running_accuracy, epoch)\n",
    "                \n",
    "        correct_prediction_collected = 0\n",
    "        loss_collected = 0.0\n",
    "        ###################################################\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} successfull\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400 successfull\n",
      "Epoch 2/400 successfull\n",
      "Epoch 3/400 successfull\n",
      "Epoch 4/400 successfull\n",
      "Epoch 5/400 successfull\n",
      "Epoch 6/400 successfull\n",
      "Epoch 7/400 successfull\n",
      "Epoch 8/400 successfull\n",
      "Epoch 9/400 successfull\n",
      "Epoch 10/400 successfull\n",
      "Epoch 11/400 successfull\n",
      "Epoch 12/400 successfull\n",
      "Epoch 13/400 successfull\n",
      "Epoch 14/400 successfull\n",
      "Epoch 15/400 successfull\n",
      "Epoch 16/400 successfull\n",
      "Epoch 17/400 successfull\n",
      "Epoch 18/400 successfull\n",
      "Epoch 19/400 successfull\n",
      "Epoch 20/400 successfull\n",
      "Epoch 21/400 successfull\n",
      "Epoch 22/400 successfull\n",
      "Epoch 23/400 successfull\n",
      "Epoch 24/400 successfull\n",
      "Epoch 25/400 successfull\n",
      "Epoch 26/400 successfull\n",
      "Epoch 27/400 successfull\n",
      "Epoch 28/400 successfull\n",
      "Epoch 29/400 successfull\n",
      "Epoch 30/400 successfull\n",
      "Epoch 31/400 successfull\n",
      "Epoch 32/400 successfull\n",
      "Epoch 33/400 successfull\n",
      "Epoch 34/400 successfull\n",
      "Epoch 35/400 successfull\n",
      "Epoch 36/400 successfull\n",
      "Epoch 37/400 successfull\n",
      "Epoch 38/400 successfull\n",
      "Epoch 39/400 successfull\n",
      "Epoch 40/400 successfull\n",
      "Epoch 41/400 successfull\n",
      "Epoch 42/400 successfull\n",
      "Epoch 43/400 successfull\n",
      "Epoch 44/400 successfull\n",
      "Epoch 45/400 successfull\n",
      "Epoch 46/400 successfull\n",
      "Epoch 47/400 successfull\n",
      "Epoch 48/400 successfull\n",
      "Epoch 49/400 successfull\n",
      "Epoch 50/400 successfull\n",
      "Epoch 51/400 successfull\n",
      "Epoch 52/400 successfull\n",
      "Epoch 53/400 successfull\n",
      "Epoch 54/400 successfull\n",
      "Epoch 55/400 successfull\n",
      "Epoch 56/400 successfull\n",
      "Epoch 57/400 successfull\n",
      "Epoch 58/400 successfull\n",
      "Epoch 59/400 successfull\n",
      "Epoch 60/400 successfull\n",
      "Epoch 61/400 successfull\n",
      "Epoch 62/400 successfull\n",
      "Epoch 63/400 successfull\n",
      "Epoch 64/400 successfull\n",
      "Epoch 65/400 successfull\n",
      "Epoch 66/400 successfull\n",
      "Epoch 67/400 successfull\n",
      "Epoch 68/400 successfull\n",
      "Epoch 69/400 successfull\n",
      "Epoch 70/400 successfull\n",
      "Epoch 71/400 successfull\n",
      "Epoch 72/400 successfull\n",
      "Epoch 73/400 successfull\n",
      "Epoch 74/400 successfull\n",
      "Epoch 75/400 successfull\n",
      "Epoch 76/400 successfull\n",
      "Epoch 77/400 successfull\n",
      "Epoch 78/400 successfull\n",
      "Epoch 79/400 successfull\n",
      "Epoch 80/400 successfull\n",
      "Epoch 81/400 successfull\n",
      "Epoch 82/400 successfull\n",
      "Epoch 83/400 successfull\n",
      "Epoch 84/400 successfull\n",
      "Epoch 85/400 successfull\n",
      "Epoch 86/400 successfull\n",
      "Epoch 87/400 successfull\n",
      "Epoch 88/400 successfull\n",
      "Epoch 89/400 successfull\n",
      "Epoch 90/400 successfull\n",
      "Epoch 91/400 successfull\n",
      "Epoch 92/400 successfull\n",
      "Epoch 93/400 successfull\n",
      "Epoch 94/400 successfull\n",
      "Epoch 95/400 successfull\n",
      "Epoch 96/400 successfull\n",
      "Epoch 97/400 successfull\n",
      "Epoch 98/400 successfull\n",
      "Epoch 99/400 successfull\n",
      "Epoch 100/400 successfull\n",
      "Epoch 101/400 successfull\n",
      "Epoch 102/400 successfull\n",
      "Epoch 103/400 successfull\n",
      "Epoch 104/400 successfull\n",
      "Epoch 105/400 successfull\n",
      "Epoch 106/400 successfull\n",
      "Epoch 107/400 successfull\n",
      "Epoch 108/400 successfull\n",
      "Epoch 109/400 successfull\n",
      "Epoch 110/400 successfull\n",
      "Epoch 111/400 successfull\n",
      "Epoch 112/400 successfull\n",
      "Epoch 113/400 successfull\n",
      "Epoch 114/400 successfull\n",
      "Epoch 115/400 successfull\n",
      "Epoch 116/400 successfull\n",
      "Epoch 117/400 successfull\n",
      "Epoch 118/400 successfull\n",
      "Epoch 119/400 successfull\n",
      "Epoch 120/400 successfull\n",
      "Epoch 121/400 successfull\n",
      "Epoch 122/400 successfull\n",
      "Epoch 123/400 successfull\n",
      "Epoch 124/400 successfull\n",
      "Epoch 125/400 successfull\n",
      "Epoch 126/400 successfull\n",
      "Epoch 127/400 successfull\n",
      "Epoch 128/400 successfull\n",
      "Epoch 129/400 successfull\n",
      "Epoch 130/400 successfull\n",
      "Epoch 131/400 successfull\n",
      "Epoch 132/400 successfull\n",
      "Epoch 133/400 successfull\n",
      "Epoch 134/400 successfull\n",
      "Epoch 135/400 successfull\n",
      "Epoch 136/400 successfull\n",
      "Epoch 137/400 successfull\n",
      "Epoch 138/400 successfull\n",
      "Epoch 139/400 successfull\n",
      "Epoch 140/400 successfull\n",
      "Epoch 141/400 successfull\n",
      "Epoch 142/400 successfull\n",
      "Epoch 143/400 successfull\n",
      "Epoch 144/400 successfull\n",
      "Epoch 145/400 successfull\n",
      "Epoch 146/400 successfull\n",
      "Epoch 147/400 successfull\n",
      "Epoch 148/400 successfull\n",
      "Epoch 149/400 successfull\n",
      "Epoch 150/400 successfull\n",
      "Epoch 151/400 successfull\n",
      "Epoch 152/400 successfull\n",
      "Epoch 153/400 successfull\n",
      "Epoch 154/400 successfull\n",
      "Epoch 155/400 successfull\n",
      "Epoch 156/400 successfull\n",
      "Epoch 157/400 successfull\n",
      "Epoch 158/400 successfull\n",
      "Epoch 159/400 successfull\n",
      "Epoch 160/400 successfull\n",
      "Epoch 161/400 successfull\n",
      "Epoch 162/400 successfull\n",
      "Epoch 163/400 successfull\n",
      "Epoch 164/400 successfull\n",
      "Epoch 165/400 successfull\n",
      "Epoch 166/400 successfull\n",
      "Epoch 167/400 successfull\n",
      "Epoch 168/400 successfull\n",
      "Epoch 169/400 successfull\n",
      "Epoch 170/400 successfull\n",
      "Epoch 171/400 successfull\n",
      "Epoch 172/400 successfull\n",
      "Epoch 173/400 successfull\n",
      "Epoch 174/400 successfull\n",
      "Epoch 175/400 successfull\n",
      "Epoch 176/400 successfull\n",
      "Epoch 177/400 successfull\n",
      "Epoch 178/400 successfull\n",
      "Epoch 179/400 successfull\n",
      "Epoch 180/400 successfull\n",
      "Epoch 181/400 successfull\n",
      "Epoch 182/400 successfull\n",
      "Epoch 183/400 successfull\n",
      "Epoch 184/400 successfull\n",
      "Epoch 185/400 successfull\n",
      "Epoch 186/400 successfull\n",
      "Epoch 187/400 successfull\n",
      "Epoch 188/400 successfull\n",
      "Epoch 189/400 successfull\n",
      "Epoch 190/400 successfull\n",
      "Epoch 191/400 successfull\n",
      "Epoch 192/400 successfull\n",
      "Epoch 193/400 successfull\n",
      "Epoch 194/400 successfull\n",
      "Epoch 195/400 successfull\n",
      "Epoch 196/400 successfull\n",
      "Epoch 197/400 successfull\n",
      "Epoch 198/400 successfull\n",
      "Epoch 199/400 successfull\n",
      "Epoch 200/400 successfull\n",
      "Epoch 201/400 successfull\n",
      "Epoch 202/400 successfull\n",
      "Epoch 203/400 successfull\n",
      "Epoch 204/400 successfull\n",
      "Epoch 205/400 successfull\n",
      "Epoch 206/400 successfull\n",
      "Epoch 207/400 successfull\n",
      "Epoch 208/400 successfull\n",
      "Epoch 209/400 successfull\n",
      "Epoch 210/400 successfull\n",
      "Epoch 211/400 successfull\n",
      "Epoch 212/400 successfull\n",
      "Epoch 213/400 successfull\n",
      "Epoch 214/400 successfull\n",
      "Epoch 215/400 successfull\n",
      "Epoch 216/400 successfull\n",
      "Epoch 217/400 successfull\n",
      "Epoch 218/400 successfull\n",
      "Epoch 219/400 successfull\n",
      "Epoch 220/400 successfull\n",
      "Epoch 221/400 successfull\n",
      "Epoch 222/400 successfull\n",
      "Epoch 223/400 successfull\n",
      "Epoch 224/400 successfull\n",
      "Epoch 225/400 successfull\n",
      "Epoch 226/400 successfull\n",
      "Epoch 227/400 successfull\n",
      "Epoch 228/400 successfull\n",
      "Epoch 229/400 successfull\n",
      "Epoch 230/400 successfull\n",
      "Epoch 231/400 successfull\n",
      "Epoch 232/400 successfull\n",
      "Epoch 233/400 successfull\n",
      "Epoch 234/400 successfull\n",
      "Epoch 235/400 successfull\n",
      "Epoch 236/400 successfull\n",
      "Epoch 237/400 successfull\n",
      "Epoch 238/400 successfull\n",
      "Epoch 239/400 successfull\n",
      "Epoch 240/400 successfull\n",
      "Epoch 241/400 successfull\n",
      "Epoch 242/400 successfull\n",
      "Epoch 243/400 successfull\n",
      "Epoch 244/400 successfull\n",
      "Epoch 245/400 successfull\n",
      "Epoch 246/400 successfull\n",
      "Epoch 247/400 successfull\n",
      "Epoch 248/400 successfull\n",
      "Epoch 249/400 successfull\n",
      "Epoch 250/400 successfull\n",
      "Epoch 251/400 successfull\n",
      "Epoch 252/400 successfull\n",
      "Epoch 253/400 successfull\n",
      "Epoch 254/400 successfull\n",
      "Epoch 255/400 successfull\n",
      "Epoch 256/400 successfull\n",
      "Epoch 257/400 successfull\n",
      "Epoch 258/400 successfull\n",
      "Epoch 259/400 successfull\n",
      "Epoch 260/400 successfull\n",
      "Epoch 261/400 successfull\n",
      "Epoch 262/400 successfull\n",
      "Epoch 263/400 successfull\n",
      "Epoch 264/400 successfull\n",
      "Epoch 265/400 successfull\n",
      "Epoch 266/400 successfull\n",
      "Epoch 267/400 successfull\n",
      "Epoch 268/400 successfull\n",
      "Epoch 269/400 successfull\n",
      "Epoch 270/400 successfull\n",
      "Epoch 271/400 successfull\n",
      "Epoch 272/400 successfull\n",
      "Epoch 273/400 successfull\n",
      "Epoch 274/400 successfull\n",
      "Epoch 275/400 successfull\n",
      "Epoch 276/400 successfull\n",
      "Epoch 277/400 successfull\n",
      "Epoch 278/400 successfull\n",
      "Epoch 279/400 successfull\n",
      "Epoch 280/400 successfull\n",
      "Epoch 281/400 successfull\n",
      "Epoch 282/400 successfull\n",
      "Epoch 283/400 successfull\n",
      "Epoch 284/400 successfull\n",
      "Epoch 285/400 successfull\n",
      "Epoch 286/400 successfull\n",
      "Epoch 287/400 successfull\n",
      "Epoch 288/400 successfull\n",
      "Epoch 289/400 successfull\n",
      "Epoch 290/400 successfull\n",
      "Epoch 291/400 successfull\n",
      "Epoch 292/400 successfull\n",
      "Epoch 293/400 successfull\n",
      "Epoch 294/400 successfull\n",
      "Epoch 295/400 successfull\n",
      "Epoch 296/400 successfull\n",
      "Epoch 297/400 successfull\n",
      "Epoch 298/400 successfull\n",
      "Epoch 299/400 successfull\n",
      "Epoch 300/400 successfull\n",
      "Epoch 301/400 successfull\n",
      "Epoch 302/400 successfull\n",
      "Epoch 303/400 successfull\n",
      "Epoch 304/400 successfull\n",
      "Epoch 305/400 successfull\n",
      "Epoch 306/400 successfull\n",
      "Epoch 307/400 successfull\n",
      "Epoch 308/400 successfull\n",
      "Epoch 309/400 successfull\n",
      "Epoch 310/400 successfull\n",
      "Epoch 311/400 successfull\n",
      "Epoch 312/400 successfull\n",
      "Epoch 313/400 successfull\n",
      "Epoch 314/400 successfull\n",
      "Epoch 315/400 successfull\n",
      "Epoch 316/400 successfull\n",
      "Epoch 317/400 successfull\n",
      "Epoch 318/400 successfull\n",
      "Epoch 319/400 successfull\n",
      "Epoch 320/400 successfull\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321/400 successfull\n",
      "Epoch 322/400 successfull\n",
      "Epoch 323/400 successfull\n",
      "Epoch 324/400 successfull\n",
      "Epoch 325/400 successfull\n",
      "Epoch 326/400 successfull\n",
      "Epoch 327/400 successfull\n",
      "Epoch 328/400 successfull\n",
      "Epoch 329/400 successfull\n",
      "Epoch 330/400 successfull\n",
      "Epoch 331/400 successfull\n",
      "Epoch 332/400 successfull\n",
      "Epoch 333/400 successfull\n",
      "Epoch 334/400 successfull\n",
      "Epoch 335/400 successfull\n",
      "Epoch 336/400 successfull\n",
      "Epoch 337/400 successfull\n",
      "Epoch 338/400 successfull\n",
      "Epoch 339/400 successfull\n",
      "Epoch 340/400 successfull\n",
      "Epoch 341/400 successfull\n",
      "Epoch 342/400 successfull\n",
      "Epoch 343/400 successfull\n",
      "Epoch 344/400 successfull\n",
      "Epoch 345/400 successfull\n",
      "Epoch 346/400 successfull\n",
      "Epoch 347/400 successfull\n",
      "Epoch 348/400 successfull\n",
      "Epoch 349/400 successfull\n",
      "Epoch 350/400 successfull\n",
      "Epoch 351/400 successfull\n",
      "Epoch 352/400 successfull\n",
      "Epoch 353/400 successfull\n",
      "Epoch 354/400 successfull\n",
      "Epoch 355/400 successfull\n",
      "Epoch 356/400 successfull\n",
      "Epoch 357/400 successfull\n",
      "Epoch 358/400 successfull\n",
      "Epoch 359/400 successfull\n",
      "Epoch 360/400 successfull\n",
      "Epoch 361/400 successfull\n",
      "Epoch 362/400 successfull\n",
      "Epoch 363/400 successfull\n",
      "Epoch 364/400 successfull\n",
      "Epoch 365/400 successfull\n",
      "Epoch 366/400 successfull\n",
      "Epoch 367/400 successfull\n",
      "Epoch 368/400 successfull\n",
      "Epoch 369/400 successfull\n",
      "Epoch 370/400 successfull\n",
      "Epoch 371/400 successfull\n",
      "Epoch 372/400 successfull\n",
      "Epoch 373/400 successfull\n",
      "Epoch 374/400 successfull\n",
      "Epoch 375/400 successfull\n",
      "Epoch 376/400 successfull\n",
      "Epoch 377/400 successfull\n",
      "Epoch 378/400 successfull\n",
      "Epoch 379/400 successfull\n",
      "Epoch 380/400 successfull\n",
      "Epoch 381/400 successfull\n",
      "Epoch 382/400 successfull\n",
      "Epoch 383/400 successfull\n",
      "Epoch 384/400 successfull\n",
      "Epoch 385/400 successfull\n",
      "Epoch 386/400 successfull\n",
      "Epoch 387/400 successfull\n",
      "Epoch 388/400 successfull\n",
      "Epoch 389/400 successfull\n",
      "Epoch 390/400 successfull\n",
      "Epoch 391/400 successfull\n",
      "Epoch 392/400 successfull\n",
      "Epoch 393/400 successfull\n",
      "Epoch 394/400 successfull\n",
      "Epoch 395/400 successfull\n",
      "Epoch 396/400 successfull\n",
      "Epoch 397/400 successfull\n",
      "Epoch 398/400 successfull\n",
      "Epoch 399/400 successfull\n",
      "Epoch 400/400 successfull\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "writer_graph = SummaryWriter('runs/Dataloader2/graph')\n",
    "writer_train = SummaryWriter('runs/Dataloader2/train')\n",
    "writer_test = SummaryWriter('runs/Dataloader2/test')\n",
    "writer = {}\n",
    "writer[\"train\"] = writer_train\n",
    "writer[\"test\"] = writer_test\n",
    "\n",
    "\n",
    "#define training params\n",
    "num_epochs = 400\n",
    "learning_rate = 0.008\n",
    "\n",
    "#define loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#collect loss for each batch\n",
    "loss_collected = 0\n",
    "loss_list = {}\n",
    "loss_list['train']=[]\n",
    "loss_list['test']=[]\n",
    "\n",
    "#collect accuracy for each batch\n",
    "correct_prediction_collected = 0\n",
    "accuracy_list={}\n",
    "accuracy_list['train']=[]\n",
    "accuracy_list['test']=[]\n",
    "\n",
    "\n",
    "#information for plotting\n",
    "n_total_steps = len(train_loader)\n",
    "len_data_loader={}\n",
    "len_data_loader['train'] = len(train_loader)\n",
    "len_data_loader['test'] = len(test_loader)\n",
    "\n",
    "\n",
    "############## TENSORBOARD ########################\n",
    "examples = iter(test_loader)\n",
    "example_data, example_targets = examples.next()\n",
    "\n",
    "writer_graph.add_graph(model, example_data.float())\n",
    "###################################################\n",
    "\n",
    "# Train and Validate the model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "\n",
    "    for phase in [\"train\"]:\n",
    "        #if phase == \"train\":\n",
    "        #    model.train(True)\n",
    "        #else:\n",
    "        #    model.train(False)\n",
    "    \n",
    "\n",
    "        for i, (window, labels) in enumerate(data_loader[phase]):\n",
    "\n",
    "            \n",
    "           ########Forward pass########\n",
    "            outputs = model(window.float())\n",
    "            \n",
    "            #collect loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_collected += loss\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                ########Backward pass########\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            #collect accuracy\n",
    "            output = outputs.argmax(dim=1)\n",
    "            correct_prediction_collected += (output == labels).sum().item()\n",
    "\n",
    "\n",
    "            #plot information during training\n",
    "            #if (i+1) % 20 == 0:\n",
    "            #    print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        ############## TENSORBOARD ########################\n",
    "        running_loss = loss_collected / len_data_loader[phase]\n",
    "        loss_list[phase].append(running_loss)\n",
    "        \n",
    "        \n",
    "        running_accuracy = correct_prediction_collected / len_data_loader[phase] / output.size(0)\n",
    "        accuracy_list[phase].append(running_accuracy)\n",
    "        \n",
    "        \n",
    "        #writer.add_scalar(f'training loss {phase}', epoch)\n",
    "        #writer.add_scalar(f'accuracy {phase}', running_accuracy, epoch)\n",
    "        \n",
    "        writer[phase].add_scalar(f'training loss', running_loss, epoch)\n",
    "        writer[phase].add_scalar(f'accuracy', running_accuracy, epoch)\n",
    "                \n",
    "        correct_prediction_collected = 0\n",
    "        loss_collected = 0.0\n",
    "        ###################################################\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} successfull\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rg/g_9b4q1j0h94scy_c8tdgd980000gn/T/ipykernel_7052/607772548.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bo-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmarkersize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ro-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmarkersize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.title('Loss')\n",
    "plt.plot(loss_list['train'], 'bo-', label = 'train', linewidth=1,markersize=0.1)\n",
    "plt.plot(loss_list['val'], 'ro-', label = 'val', linewidth=1,markersize=0.1)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig('CNN_NO_DOMAIN_ADAPTION_LOSS_KERNEL_2_1_1')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig2 = plt.figure()\n",
    "\n",
    "plt.plot(accuracy_list['train'], 'bo-', label = 'train', linewidth=1,markersize=0.1)\n",
    "plt.plot(accuracy_list['val'], 'ro-', label = 'val', linewidth=1,markersize=0.1)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig('CNN_NO_DOMAIN_ADAPTION_ACCURACY_KERNEL_2_1_1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 28.21969696969697 %\n",
      "Accuracy of BSD_11: 15.606060606060606 %\n",
      "Accuracy of BSD_21: 29.09090909090909 %\n",
      "Accuracy of BSD_31: 54.24242424242424 %\n",
      "Accuracy of BSD_P1: 13.93939393939394 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    classes = ['BSD_11', 'BSD_21', 'BSD_31', 'BSD_P1']\n",
    "    \n",
    "    #collect information about labels, predictions\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(4)]\n",
    "    n_class_samples = [0 for i in range(4)]\n",
    "    n_class_samples_out = [0 for i in range(4)]\n",
    "    \n",
    "    #iterate through bateches in test_loader\n",
    "    for window, labels in test_loader:\n",
    "        #make predictions for each batch\n",
    "        outputs = model(window.float())\n",
    "        #for each element in batch check if prediction is correct and collect total and correct predictions and labels\n",
    "        for i in range(batch_size):\n",
    "            if len(labels)==4:\n",
    "                label = labels[i]\n",
    "                output = torch.argmax(outputs[i])\n",
    "                if label == output:\n",
    "                    n_correct+=1\n",
    "                    n_class_correct[label]+=1\n",
    "                \n",
    "                n_samples+=1\n",
    "                n_class_samples[label]+=1\n",
    "                n_class_samples_out[output]+=1\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    #calculate total accuracy\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "    \n",
    "    #calculate class accuracy\n",
    "    for i in range(4):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[660, 660, 660, 660]\n",
      "[595, 702, 1117, 226]\n",
      "[103, 192, 358, 92]\n"
     ]
    }
   ],
   "source": [
    "print(n_class_samples)\n",
    "print(n_class_samples_out)\n",
    "print(n_class_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.45454545454545 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"{accuracy_list['val'][-1]*100} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
