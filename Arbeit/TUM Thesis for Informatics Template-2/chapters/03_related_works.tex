% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Related Works}\label{chapter:related_works}
Use with pdfLaTeX and Biber.

\section{Fault feature extraction for DL-based Predictive Maintenance}
\section{Domain adaptation approaches for Predictive Maintenance}

\subsection{Maximum Mean Discrepancy}
\subsection{Generative Adversarial Networks}

Since in this thesis Relu and Selu activation functions were mainly used in hidden and Softmax in final layer, those are described more in detail . The formula \ref{eq:relu} and \ref{eq:selu} show the definition of the Selu and Relu and \ref{eq:softmax} of the softmax activation functions: 

\begin{equation}
Selu(z) = \lambda
\left\{
\begin{matrix}
z \hspace{3.5em}, if \enspace z < 0\\
\alpha e^{z} - \alpha  \hspace{1em}, if \enspace z > 0.
\end{matrix}
\right.
\label{eq:selu}
\end{equation}

