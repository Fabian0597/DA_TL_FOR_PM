% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Related Works}\label{chapter:related_works}


\section{Fault feature extraction for DL-based Predictive Maintenance}

Nonstationary signal analysis is one of the main topics in the field of machinery fault diagnosis. Nonstationarity in this sense means that the signals change their statistical properties over time. Signals can consist of multiple frequencies and change their amplitude while traveling in time. Traditional signal analysis techniques make stationary assumptions. When applying those just statistical average in time or frequency can be extracted \cite{FENG2013}. Fourier transform makes it possible to decompose time signal into sinusoids. Each sinusoid has an associated amplitude, phase, and frequency. A spectogram contains all detected sinusoids with their specific characteristic. Fourier transform for example is just suitable for those signals that have frequencies that are fixed at a particular time. The demand for analysis methods, which allows ascertain the main features of such nonstationary signal is increasing. These features help to extract machine health related information from the signals. General one can separate between different approaches suitable for nonstationary signals which are presented in the following. 

\subsection{Linear time–frequency representation}

\subsection{Bilinear time–frequency distribution}
\subsection{Time-varying higher order spectra}
\subsection{Adaptive parametric time–frequency analysis}
\subsection{Time–frequency ARMA models}
\subsection{Adaptive non-parametric time–frequency analysis}





\section{Domain adaptation approaches for Predictive Maintenance}

\subsection{Maximum Mean Discrepancy}
\subsection{Generative Adversarial Networks}

Since in this thesis Relu and Selu activation functions were mainly used in hidden and Softmax in final layer, those are described more in detail . The formula \ref{eq:relu} and \ref{eq:selu} show the definition of the Selu and Relu and \ref{eq:softmax} of the softmax activation functions: 

\begin{equation}
Selu(z) = \lambda
\left\{
\begin{matrix}
z \hspace{3.5em}, if \enspace z < 0\\
\alpha e^{z} - \alpha  \hspace{1em}, if \enspace z > 0.
\end{matrix}
\right.
\label{eq:selu}
\end{equation}

