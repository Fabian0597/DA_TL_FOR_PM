{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\ndef load_data(data, window_size):\\n    splits = len(data)//window_size\\n    data = data[:splits*window_size]\\n    data = data.reshape(-1,window_size)\\n    #print(np.shape(data))\\n    return data\\n\\ndef del_nan_element(data_with_nan):\\n    nan_val = np.isnan(data_with_nan)\\n    return data_with_nan[nan_val==False]\\n\\ndef concatenate_data_from_BSD_state(folders, data_path, features_of_interest, window_size):\\n    x_data_concatenated = None\\n    y_data_concatenated = None\\n    \\n    first = True\\n    for BSD_path in folders.keys(): #folder path\\n        for file_path in folders[BSD_path]: #file path \\n            \\n            path_BSD_file = os.path.join(data_path, BSD_path, file_path) # concatenate the folder and file path\\n            \\n            #in first iteration get a list if all features\\n            if first == True:\\n                features = get_features(path_BSD_file)\\n            \\n            #load data from files in shape [window_number, 1, window_size]\\n            data_BSD_file = np.genfromtxt(path_BSD_file, dtype = np.dtype(\\'d\\'), delimiter=\\',\\')[1:,:] #write csv in numpy\\n            data_BSD_file = data_BSD_file[:,features.index(features_of_interest)]\\n            data_BSD_file = np.nan_to_num(data_BSD_file)\\n            #data_BSD_file = del_nan_element(data_BSD_file)\\n            data_BSD_file = load_data(data_BSD_file, window_size)\\n            data_BSD_file = np.expand_dims(data_BSD_file, axis = 1)\\n            \\n            #rewrite labels as BSD_condition_1 = 0, BSD_condition_2 = 1, BSD_condition_3 = 2, BSD_condition_P1 = 3\\n            label = BSD_path[-2]\\n            if label == \"P\":\\n                label = int(3)\\n            else:\\n                label =int(int(label)-1)\\n            \\n            #concatenate the data from each file in one numpy array\\n            if  first == True:\\n                x_data_concatenated = np.copy(data_BSD_file)\\n                y_data_concatenated = np.copy(np.asarray([label]*np.shape(data_BSD_file)[0]))\\n                first = False\\n            else:\\n                x_data_concatenated = np.concatenate((x_data_concatenated, data_BSD_file), axis=0)\\n                y_data_concatenated = np.concatenate((y_data_concatenated,np.asarray([label]*np.shape(data_BSD_file)[0])), axis=0)\\n            print(np.shape(x_data_concatenated), np.shape(y_data_concatenated))\\n    \\n    #generate torch array\\n    n_samples = np.shape(x_data_concatenated)[0]\\n    x_data = torch.from_numpy(x_data_concatenated)\\n    y_data = torch.from_numpy(y_data_concatenated)\\n    \\n    return n_samples, x_data, y_data\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#1 Feature\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_data(data, window_size):\n",
    "    splits = len(data)//window_size\n",
    "    data = data[:splits*window_size]\n",
    "    data = data.reshape(-1,window_size)\n",
    "    #print(np.shape(data))\n",
    "    return data\n",
    "\n",
    "def del_nan_element(data_with_nan):\n",
    "    nan_val = np.isnan(data_with_nan)\n",
    "    return data_with_nan[nan_val==False]\n",
    "\n",
    "def concatenate_data_from_BSD_state(folders, data_path, features_of_interest, window_size):\n",
    "    x_data_concatenated = None\n",
    "    y_data_concatenated = None\n",
    "    \n",
    "    first = True\n",
    "    for BSD_path in folders.keys(): #folder path\n",
    "        for file_path in folders[BSD_path]: #file path \n",
    "            \n",
    "            path_BSD_file = os.path.join(data_path, BSD_path, file_path) # concatenate the folder and file path\n",
    "            \n",
    "            #in first iteration get a list if all features\n",
    "            if first == True:\n",
    "                features = get_features(path_BSD_file)\n",
    "            \n",
    "            #load data from files in shape [window_number, 1, window_size]\n",
    "            data_BSD_file = np.genfromtxt(path_BSD_file, dtype = np.dtype('d'), delimiter=',')[1:,:] #write csv in numpy\n",
    "            data_BSD_file = data_BSD_file[:,features.index(features_of_interest)]\n",
    "            data_BSD_file = np.nan_to_num(data_BSD_file)\n",
    "            #data_BSD_file = del_nan_element(data_BSD_file)\n",
    "            data_BSD_file = load_data(data_BSD_file, window_size)\n",
    "            data_BSD_file = np.expand_dims(data_BSD_file, axis = 1)\n",
    "            \n",
    "            #rewrite labels as BSD_condition_1 = 0, BSD_condition_2 = 1, BSD_condition_3 = 2, BSD_condition_P1 = 3\n",
    "            label = BSD_path[-2]\n",
    "            if label == \"P\":\n",
    "                label = int(3)\n",
    "            else:\n",
    "                label =int(int(label)-1)\n",
    "            \n",
    "            #concatenate the data from each file in one numpy array\n",
    "            if  first == True:\n",
    "                x_data_concatenated = np.copy(data_BSD_file)\n",
    "                y_data_concatenated = np.copy(np.asarray([label]*np.shape(data_BSD_file)[0]))\n",
    "                first = False\n",
    "            else:\n",
    "                x_data_concatenated = np.concatenate((x_data_concatenated, data_BSD_file), axis=0)\n",
    "                y_data_concatenated = np.concatenate((y_data_concatenated,np.asarray([label]*np.shape(data_BSD_file)[0])), axis=0)\n",
    "            print(np.shape(x_data_concatenated), np.shape(y_data_concatenated))\n",
    "    \n",
    "    #generate torch array\n",
    "    n_samples = np.shape(x_data_concatenated)[0]\n",
    "    x_data = torch.from_numpy(x_data_concatenated)\n",
    "    y_data = torch.from_numpy(y_data_concatenated)\n",
    "    \n",
    "    return n_samples, x_data, y_data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "def load_data(data, window_size):\n",
    "\n",
    "    Split data in windows of equal size\n",
    "    \n",
    "    INPUT:\n",
    "    @data: data numpy array of shape [elements per file, features]\n",
    "    @window: number of elements per window\n",
    "    \n",
    "    OUTPUT\n",
    "    @data: data numpy array of shape [number_of_windows, elements per window, features]\n",
    "\n",
    "    splits = np.shape(data)[0]//window_size # number of splits\n",
    "    data = data[:splits*window_size] #cut off end of array such that array can be split equaly\n",
    "    data = data.reshape((splits,-1,np.shape(data)[1])) #split array in windows\n",
    "    return data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def load_data(data, window_size, overlap_size):\n",
    "    \"\"\"\n",
    "    Split data in windows of equal size\n",
    "    \n",
    "    INPUT:\n",
    "    @data: data numpy array of shape [elements per file, features]\n",
    "    @window: number of elements per window\n",
    "    \n",
    "    OUTPUT\n",
    "    @data: data numpy array of shape [number_of_windows, elements per window, features]\n",
    "    \"\"\"\n",
    "    num_windows = (data.shape[0] - window_size) // (window_size - overlap_size) + 1\n",
    "    overhang = data.shape[0] - (num_windows*window_size - (num_windows-1)*overlap_size)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_nan_element(data_with_nan):\n",
    "    \"\"\"\n",
    "    Delete all elements in the data which have any nan valued feature\n",
    "    \n",
    "    INPUT:\n",
    "    @data_with_nan: data numpy array containing nan_values\n",
    "    \n",
    "    OUTPUT\n",
    "    @data_with_nan: data numpy array inlcuding just elements per window which do have no nan_vaues in any feature\n",
    "    \"\"\"\n",
    "    nan_val = np.isnan(data_with_nan) #mask for all nan_elements as 2d array [elements_per_window, features]\n",
    "    nan_val = np.any(nan_val,axis = 1) #mask for all nan_rows as 1d array [elements_per_window]\n",
    "    return data_with_nan[nan_val==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder_dictionary(list_of_train_BSD_states, list_of_test_BSD_states, data_path):\n",
    "    \"\"\"\n",
    "    Create a dictionaty for testing and training containing folder names as keys and files as values\n",
    "    \n",
    "    INPUT:\n",
    "    @list_of_train_BSD_states: list containing the training BSD states as string\n",
    "    @list_of_test_BSD_states: list containing the testing BSD states as string\n",
    "    @data_path: data directory containing folders for each BSD state\n",
    "    \n",
    "    OUTPUT\n",
    "    @training_folders: dictionary folders and keys for training\n",
    "    @testing_folders: dictionary folders and keys for testing\n",
    "    \"\"\"\n",
    "    \n",
    "    data_path = data_path\n",
    "    training_folders = {}\n",
    "    testing_folders = {}\n",
    "    \n",
    "    #Sorting the individual folders by findinding the BSD_states in the folder names\n",
    "    for data_path_element in os.listdir(data_path):\n",
    "        if any(element in data_path_element for element in list_of_train_BSD_states): \n",
    "            training_folders[data_path_element]  = os.listdir(os.path.join(data_path,data_path_element))\n",
    "        elif any(element in data_path_element for element in list_of_test_BSD_states): \n",
    "            testing_folders[data_path_element] = os.listdir(os.path.join(data_path,data_path_element))\n",
    "    return training_folders, testing_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(path):\n",
    "    \"\"\"\n",
    "    Creates a list of all feature names\n",
    "    INPUT:\n",
    "    @path: path to any BSD file since the features are the same for all files\n",
    "    \n",
    "    OUTPUT\n",
    "    @features: list of features:\n",
    "    ['C:s_ist/X', 'C:s_soll/X', 'C:s_diff/X', 'C:v_(n_ist)/X', 'C:v_(n_soll)/X', 'C:P_mech./X', 'C:Pos._Diff./X',\n",
    "    'C:I_ist/X', 'C:I_soll/X', 'C:x_bottom', 'C:y_bottom', 'C:z_bottom', 'C:x_nut', 'C:y_nut', 'C:z_nut',\n",
    "    'C:x_top', 'C:y_top', 'C:z_top', 'D:s_ist/X', 'D:s_soll/X', 'D:s_diff/X', 'D:v_(n_ist)/X', 'D:v_(n_soll)/X',\n",
    "    'D:P_mech./X', 'D:Pos._Diff./X', 'D:I_ist/X', 'D:I_soll/X', 'D:x_bottom', 'D:y_bottom', 'D:z_bottom',\n",
    "    'D:x_nut', 'D:y_nut', 'D:z_nut', 'D:x_top', 'D:y_top', 'D:z_top', 'S:x_bottom', 'S:y_bottom', 'S:z_bottom',\n",
    "    'S:x_nut', 'S:y_nut', 'S:z_nut', 'S:x_top', 'S:y_top', 'S:z_top', 'S:Nominal_rotational_speed[rad/s]',\n",
    "    'S:Actual_rotational_speed[µm/s]', 'S:Actual_position_of_the_position_encoder(dy/dt)[µm/s]',\n",
    "    'S:Actual_position_of_the_motor_encoder(dy/dt)[µm/s]']\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "        csvreader = csv.reader(file)\n",
    "        features = next(csvreader)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_data_from_BSD_state(folders, data_path, features_of_interest, window_size):\n",
    "    \"\"\"\n",
    "    Concatenates all the windowed data from each file to one big torch array\n",
    "    INPUT:\n",
    "    @folders: dictionary containing folders (as keys) and files (as values) to downloaded\n",
    "    @data_path: data directory containing folders for each BSD state\n",
    "    @features_of_interest: list of features which should be included for training\n",
    "    @window_size: number of elements per widow\n",
    "    \n",
    "    OUTPUT:\n",
    "    @n_samples: number of total elements from all included files\n",
    "    @x_data: torch array containing all the data elements \n",
    "    @y_data: torch array containing the labels for all elements\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # arrays to collect data and label\n",
    "    x_data_concatenated = None\n",
    "    y_data_concatenated = None\n",
    "    \n",
    "    \n",
    "    iterator = 0\n",
    "    first = True\n",
    "    \n",
    "    \n",
    "    for BSD_path in folders.keys(): #folder path\n",
    "        for file_path in folders[BSD_path]: #file path \n",
    "            path_BSD_file = os.path.join(data_path, BSD_path, file_path) # concatenate the data_path, folder and file path\n",
    "            \n",
    "            #in first iteration get a list if all features\n",
    "            if first == True:\n",
    "                features = get_features(path_BSD_file)\n",
    "            \n",
    "            data_BSD_file = np.genfromtxt(path_BSD_file, dtype = np.dtype('d'), delimiter=',')[1:,:] #write csv in numpy\n",
    "            feature_index_list = np.where(np.isin(features, features_of_interest)) #get index for all features of interest\n",
    "            data_BSD_file = data_BSD_file[:,feature_index_list] #slice numpy array such that just features of interest are included\n",
    "            data_BSD_file = np.squeeze(data_BSD_file, axis = 1) # one unnecessary extra dimension was created while slicing\n",
    "            data_BSD_file = del_nan_element(data_BSD_file) #delete all elements with any nan feature\n",
    "            data_BSD_file = load_data(data_BSD_file, window_size) #window the data\n",
    "            data_BSD_file = np.swapaxes(data_BSD_file,1,2) #swap axes for CNN\n",
    "            \n",
    "            \n",
    "            \n",
    "            #rewrite labels as BSD_condition_1 = 0, BSD_condition_2 = 1, BSD_condition_3 = 2, BSD_condition_P1 = 3\n",
    "            label = BSD_path[-2]\n",
    "            if label == \"P\":\n",
    "                label = int(3)\n",
    "            else:\n",
    "                label =int(int(label)-1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #concatenate the data from each file in one numpy array\n",
    "            if  first == True: #overwrite variable\n",
    "                x_data_concatenated = np.copy(data_BSD_file)\n",
    "                y_data_concatenated = np.copy(np.asarray([label]*np.shape(data_BSD_file)[0]))\n",
    "                first = False\n",
    "            else: #concatenate data numpy arrays\n",
    "                x_data_concatenated = np.concatenate((x_data_concatenated, data_BSD_file), axis=0)\n",
    "                y_data_concatenated = np.concatenate((y_data_concatenated,np.asarray([label]*np.shape(data_BSD_file)[0])), axis=0)\n",
    "            \n",
    "            \n",
    "            iterator +=1\n",
    "            print(f\"{iterator}/{len(folders.keys())*len(folders[list(folders.keys())[0]])} folders downloaded\")\n",
    "            print(f\"downloaded folder: {BSD_path}/{file_path}\")\n",
    "            print(f\"Shape of collected datafram: X_shape: {np.shape(x_data_concatenated)}, Y_shape: {np.shape(y_data_concatenated)}\")\n",
    "    \n",
    "    #generate torch array\n",
    "    n_samples = np.shape(x_data_concatenated)[0]\n",
    "    x_data = torch.from_numpy(x_data_concatenated)\n",
    "    y_data = torch.from_numpy(y_data_concatenated)\n",
    "    \n",
    "    return n_samples, x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/063_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (16, 49, 1024), Y_shape: (16,)\n",
      "2/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/064_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (32, 49, 1024), Y_shape: (32,)\n",
      "3/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/065_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (48, 49, 1024), Y_shape: (48,)\n",
      "4/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/062_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (64, 49, 1024), Y_shape: (64,)\n",
      "5/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/068_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (80, 49, 1024), Y_shape: (80,)\n",
      "6/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/060_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (96, 49, 1024), Y_shape: (96,)\n",
      "7/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/067_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (112, 49, 1024), Y_shape: (112,)\n",
      "8/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/066_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (128, 49, 1024), Y_shape: (128,)\n",
      "9/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/061_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (144, 49, 1024), Y_shape: (144,)\n",
      "10/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/069_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (160, 49, 1024), Y_shape: (160,)\n",
      "11/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/241_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (176, 49, 1024), Y_shape: (176,)\n",
      "12/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/249_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (192, 49, 1024), Y_shape: (192,)\n",
      "13/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/246_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (208, 49, 1024), Y_shape: (208,)\n",
      "14/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/247_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (224, 49, 1024), Y_shape: (224,)\n",
      "15/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/248_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (240, 49, 1024), Y_shape: (240,)\n",
      "16/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/240_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (256, 49, 1024), Y_shape: (256,)\n",
      "17/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/242_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (272, 49, 1024), Y_shape: (272,)\n",
      "18/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/245_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (288, 49, 1024), Y_shape: (288,)\n",
      "19/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/244_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (304, 49, 1024), Y_shape: (304,)\n",
      "20/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/243_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (320, 49, 1024), Y_shape: (320,)\n",
      "21/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/046_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (336, 49, 1024), Y_shape: (336,)\n",
      "22/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/037_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (352, 49, 1024), Y_shape: (352,)\n",
      "23/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/041_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (368, 49, 1024), Y_shape: (368,)\n",
      "24/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/038_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (384, 49, 1024), Y_shape: (384,)\n",
      "25/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/039_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (400, 49, 1024), Y_shape: (400,)\n",
      "26/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/040_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (416, 49, 1024), Y_shape: (416,)\n",
      "27/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/045_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (432, 49, 1024), Y_shape: (432,)\n",
      "28/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/042_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (448, 49, 1024), Y_shape: (448,)\n",
      "29/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/043_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (464, 49, 1024), Y_shape: (464,)\n",
      "30/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/044_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (480, 49, 1024), Y_shape: (480,)\n",
      "31/120 folders downloaded\n",
      "downloaded folder: NR22_20200508_PGS_11_BSD_P1/265_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (496, 49, 1024), Y_shape: (496,)\n",
      "32/120 folders downloaded\n",
      "downloaded folder: NR22_20200508_PGS_11_BSD_P1/270_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (512, 49, 1024), Y_shape: (512,)\n",
      "33/120 folders downloaded\n",
      "downloaded folder: NR22_20200508_PGS_11_BSD_P1/271_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (528, 49, 1024), Y_shape: (528,)\n",
      "34/120 folders downloaded\n",
      "downloaded folder: NR22_20200508_PGS_11_BSD_P1/264_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (544, 49, 1024), Y_shape: (544,)\n",
      "35/120 folders downloaded\n",
      "downloaded folder: NR22_20200508_PGS_11_BSD_P1/263_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (560, 49, 1024), Y_shape: (560,)\n",
      "36/120 folders downloaded\n",
      "downloaded folder: NR22_20200508_PGS_11_BSD_P1/269_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (576, 49, 1024), Y_shape: (576,)\n",
      "37/120 folders downloaded\n",
      "downloaded folder: NR22_20200508_PGS_11_BSD_P1/266_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (592, 49, 1024), Y_shape: (592,)\n",
      "38/120 folders downloaded\n",
      "downloaded folder: NR22_20200508_PGS_11_BSD_P1/267_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (608, 49, 1024), Y_shape: (608,)\n",
      "39/120 folders downloaded\n",
      "downloaded folder: NR22_20200508_PGS_11_BSD_P1/272_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (624, 49, 1024), Y_shape: (624,)\n",
      "40/120 folders downloaded\n",
      "downloaded folder: NR22_20200508_PGS_11_BSD_P1/268_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (640, 49, 1024), Y_shape: (640,)\n",
      "41/120 folders downloaded\n",
      "downloaded folder: NR10_20200502_PGS_21_BSD_31/177_2020_05_02.csv\n",
      "Shape of collected datafram: X_shape: (656, 49, 1024), Y_shape: (656,)\n",
      "42/120 folders downloaded\n",
      "downloaded folder: NR10_20200502_PGS_21_BSD_31/180_2020_05_02.csv\n",
      "Shape of collected datafram: X_shape: (672, 49, 1024), Y_shape: (672,)\n",
      "43/120 folders downloaded\n",
      "downloaded folder: NR10_20200502_PGS_21_BSD_31/178_2020_05_02.csv\n",
      "Shape of collected datafram: X_shape: (688, 49, 1024), Y_shape: (688,)\n",
      "44/120 folders downloaded\n",
      "downloaded folder: NR10_20200502_PGS_21_BSD_31/179_2020_05_02.csv\n",
      "Shape of collected datafram: X_shape: (704, 49, 1024), Y_shape: (704,)\n",
      "45/120 folders downloaded\n",
      "downloaded folder: NR10_20200502_PGS_21_BSD_31/181_2020_05_02.csv\n",
      "Shape of collected datafram: X_shape: (720, 49, 1024), Y_shape: (720,)\n",
      "46/120 folders downloaded\n",
      "downloaded folder: NR10_20200502_PGS_21_BSD_31/176_2020_05_02.csv\n",
      "Shape of collected datafram: X_shape: (736, 49, 1024), Y_shape: (736,)\n",
      "47/120 folders downloaded\n",
      "downloaded folder: NR10_20200502_PGS_21_BSD_31/174_2020_05_02.csv\n",
      "Shape of collected datafram: X_shape: (752, 49, 1024), Y_shape: (752,)\n",
      "48/120 folders downloaded\n",
      "downloaded folder: NR10_20200502_PGS_21_BSD_31/173_2020_05_02.csv\n",
      "Shape of collected datafram: X_shape: (768, 49, 1024), Y_shape: (768,)\n",
      "49/120 folders downloaded\n",
      "downloaded folder: NR10_20200502_PGS_21_BSD_31/172_2020_05_02.csv\n",
      "Shape of collected datafram: X_shape: (784, 49, 1024), Y_shape: (784,)\n",
      "50/120 folders downloaded\n",
      "downloaded folder: NR10_20200502_PGS_21_BSD_31/175_2020_05_02.csv\n",
      "Shape of collected datafram: X_shape: (800, 49, 1024), Y_shape: (800,)\n",
      "51/120 folders downloaded\n",
      "downloaded folder: NR11_20200429_PGS_21_BSD_21/149_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (816, 49, 1024), Y_shape: (816,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/120 folders downloaded\n",
      "downloaded folder: NR11_20200429_PGS_21_BSD_21/154_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (832, 49, 1024), Y_shape: (832,)\n",
      "53/120 folders downloaded\n",
      "downloaded folder: NR11_20200429_PGS_21_BSD_21/153_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (848, 49, 1024), Y_shape: (848,)\n",
      "54/120 folders downloaded\n",
      "downloaded folder: NR11_20200429_PGS_21_BSD_21/152_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (864, 49, 1024), Y_shape: (864,)\n",
      "55/120 folders downloaded\n",
      "downloaded folder: NR11_20200429_PGS_21_BSD_21/155_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (880, 49, 1024), Y_shape: (880,)\n",
      "56/120 folders downloaded\n",
      "downloaded folder: NR11_20200429_PGS_21_BSD_21/157_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (896, 49, 1024), Y_shape: (896,)\n",
      "57/120 folders downloaded\n",
      "downloaded folder: NR11_20200429_PGS_21_BSD_21/150_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (912, 49, 1024), Y_shape: (912,)\n",
      "58/120 folders downloaded\n",
      "downloaded folder: NR11_20200429_PGS_21_BSD_21/158_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (928, 49, 1024), Y_shape: (928,)\n",
      "59/120 folders downloaded\n",
      "downloaded folder: NR11_20200429_PGS_21_BSD_21/151_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (944, 49, 1024), Y_shape: (944,)\n",
      "60/120 folders downloaded\n",
      "downloaded folder: NR11_20200429_PGS_21_BSD_21/156_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (960, 49, 1024), Y_shape: (960,)\n",
      "61/120 folders downloaded\n",
      "downloaded folder: NR20_20200507_PGS_11_BSD_21/220_2020_05_07.csv\n",
      "Shape of collected datafram: X_shape: (976, 49, 1024), Y_shape: (976,)\n",
      "62/120 folders downloaded\n",
      "downloaded folder: NR20_20200507_PGS_11_BSD_21/227_2020_05_07.csv\n",
      "Shape of collected datafram: X_shape: (992, 49, 1024), Y_shape: (992,)\n",
      "63/120 folders downloaded\n",
      "downloaded folder: NR20_20200507_PGS_11_BSD_21/226_2020_05_07.csv\n",
      "Shape of collected datafram: X_shape: (1008, 49, 1024), Y_shape: (1008,)\n",
      "64/120 folders downloaded\n",
      "downloaded folder: NR20_20200507_PGS_11_BSD_21/221_2020_05_07.csv\n",
      "Shape of collected datafram: X_shape: (1024, 49, 1024), Y_shape: (1024,)\n",
      "65/120 folders downloaded\n",
      "downloaded folder: NR20_20200507_PGS_11_BSD_21/223_2020_05_07.csv\n",
      "Shape of collected datafram: X_shape: (1040, 49, 1024), Y_shape: (1040,)\n",
      "66/120 folders downloaded\n",
      "downloaded folder: NR20_20200507_PGS_11_BSD_21/224_2020_05_07.csv\n",
      "Shape of collected datafram: X_shape: (1056, 49, 1024), Y_shape: (1056,)\n",
      "67/120 folders downloaded\n",
      "downloaded folder: NR20_20200507_PGS_11_BSD_21/225_2020_05_07.csv\n",
      "Shape of collected datafram: X_shape: (1072, 49, 1024), Y_shape: (1072,)\n",
      "68/120 folders downloaded\n",
      "downloaded folder: NR20_20200507_PGS_11_BSD_21/222_2020_05_07.csv\n",
      "Shape of collected datafram: X_shape: (1088, 49, 1024), Y_shape: (1088,)\n",
      "69/120 folders downloaded\n",
      "downloaded folder: NR20_20200507_PGS_11_BSD_21/219_2020_05_07.csv\n",
      "Shape of collected datafram: X_shape: (1104, 49, 1024), Y_shape: (1104,)\n",
      "70/120 folders downloaded\n",
      "downloaded folder: NR20_20200507_PGS_11_BSD_21/218_2020_05_07.csv\n",
      "Shape of collected datafram: X_shape: (1120, 49, 1024), Y_shape: (1120,)\n",
      "71/120 folders downloaded\n",
      "downloaded folder: NR12_20200429_PGS_21_BSD_11/130_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1136, 49, 1024), Y_shape: (1136,)\n",
      "72/120 folders downloaded\n",
      "downloaded folder: NR12_20200429_PGS_21_BSD_11/131_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1152, 49, 1024), Y_shape: (1152,)\n",
      "73/120 folders downloaded\n",
      "downloaded folder: NR12_20200429_PGS_21_BSD_11/126_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1168, 49, 1024), Y_shape: (1168,)\n",
      "74/120 folders downloaded\n",
      "downloaded folder: NR12_20200429_PGS_21_BSD_11/133_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1184, 49, 1024), Y_shape: (1184,)\n",
      "75/120 folders downloaded\n",
      "downloaded folder: NR12_20200429_PGS_21_BSD_11/134_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1200, 49, 1024), Y_shape: (1200,)\n",
      "76/120 folders downloaded\n",
      "downloaded folder: NR12_20200429_PGS_21_BSD_11/129_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1216, 49, 1024), Y_shape: (1216,)\n",
      "77/120 folders downloaded\n",
      "downloaded folder: NR12_20200429_PGS_21_BSD_11/128_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1232, 49, 1024), Y_shape: (1232,)\n",
      "78/120 folders downloaded\n",
      "downloaded folder: NR12_20200429_PGS_21_BSD_11/135_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1248, 49, 1024), Y_shape: (1248,)\n",
      "79/120 folders downloaded\n",
      "downloaded folder: NR12_20200429_PGS_21_BSD_11/132_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1264, 49, 1024), Y_shape: (1264,)\n",
      "80/120 folders downloaded\n",
      "downloaded folder: NR12_20200429_PGS_21_BSD_11/127_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1280, 49, 1024), Y_shape: (1280,)\n",
      "81/120 folders downloaded\n",
      "downloaded folder: NR19_20200505_PGS_11_BSD_31/195_2020_05_05.csv\n",
      "Shape of collected datafram: X_shape: (1296, 49, 1024), Y_shape: (1296,)\n",
      "82/120 folders downloaded\n",
      "downloaded folder: NR19_20200505_PGS_11_BSD_31/198_2020_05_05.csv\n",
      "Shape of collected datafram: X_shape: (1312, 49, 1024), Y_shape: (1312,)\n",
      "83/120 folders downloaded\n",
      "downloaded folder: NR19_20200505_PGS_11_BSD_31/197_2020_05_05.csv\n",
      "Shape of collected datafram: X_shape: (1328, 49, 1024), Y_shape: (1328,)\n",
      "84/120 folders downloaded\n",
      "downloaded folder: NR19_20200505_PGS_11_BSD_31/196_2020_05_05.csv\n",
      "Shape of collected datafram: X_shape: (1344, 49, 1024), Y_shape: (1344,)\n",
      "85/120 folders downloaded\n",
      "downloaded folder: NR19_20200505_PGS_11_BSD_31/199_2020_05_05.csv\n",
      "Shape of collected datafram: X_shape: (1360, 49, 1024), Y_shape: (1360,)\n",
      "86/120 folders downloaded\n",
      "downloaded folder: NR19_20200505_PGS_11_BSD_31/204_2020_05_05.csv\n",
      "Shape of collected datafram: X_shape: (1376, 49, 1024), Y_shape: (1376,)\n",
      "87/120 folders downloaded\n",
      "downloaded folder: NR19_20200505_PGS_11_BSD_31/203_2020_05_05.csv\n",
      "Shape of collected datafram: X_shape: (1392, 49, 1024), Y_shape: (1392,)\n",
      "88/120 folders downloaded\n",
      "downloaded folder: NR19_20200505_PGS_11_BSD_31/202_2020_05_05.csv\n",
      "Shape of collected datafram: X_shape: (1408, 49, 1024), Y_shape: (1408,)\n",
      "89/120 folders downloaded\n",
      "downloaded folder: NR19_20200505_PGS_11_BSD_31/200_2020_05_05.csv\n",
      "Shape of collected datafram: X_shape: (1424, 49, 1024), Y_shape: (1424,)\n",
      "90/120 folders downloaded\n",
      "downloaded folder: NR19_20200505_PGS_11_BSD_31/201_2020_05_05.csv\n",
      "Shape of collected datafram: X_shape: (1440, 49, 1024), Y_shape: (1440,)\n",
      "91/120 folders downloaded\n",
      "downloaded folder: NR13_20200428_PGS_21_BSD_P1/106_2020_04_28.csv\n",
      "Shape of collected datafram: X_shape: (1456, 49, 1024), Y_shape: (1456,)\n",
      "92/120 folders downloaded\n",
      "downloaded folder: NR13_20200428_PGS_21_BSD_P1/109_2020_04_28.csv\n",
      "Shape of collected datafram: X_shape: (1472, 49, 1024), Y_shape: (1472,)\n",
      "93/120 folders downloaded\n",
      "downloaded folder: NR13_20200428_PGS_21_BSD_P1/108_2020_04_28.csv\n",
      "Shape of collected datafram: X_shape: (1488, 49, 1024), Y_shape: (1488,)\n",
      "94/120 folders downloaded\n",
      "downloaded folder: NR13_20200428_PGS_21_BSD_P1/107_2020_04_28.csv\n",
      "Shape of collected datafram: X_shape: (1504, 49, 1024), Y_shape: (1504,)\n",
      "95/120 folders downloaded\n",
      "downloaded folder: NR13_20200428_PGS_21_BSD_P1/112_2020_04_28.csv\n",
      "Shape of collected datafram: X_shape: (1520, 49, 1024), Y_shape: (1520,)\n",
      "96/120 folders downloaded\n",
      "downloaded folder: NR13_20200428_PGS_21_BSD_P1/105_2020_04_28.csv\n",
      "Shape of collected datafram: X_shape: (1536, 49, 1024), Y_shape: (1536,)\n",
      "97/120 folders downloaded\n",
      "downloaded folder: NR13_20200428_PGS_21_BSD_P1/110_2020_04_28.csv\n",
      "Shape of collected datafram: X_shape: (1552, 49, 1024), Y_shape: (1552,)\n",
      "98/120 folders downloaded\n",
      "downloaded folder: NR13_20200428_PGS_21_BSD_P1/103_2020_04_28.csv\n",
      "Shape of collected datafram: X_shape: (1568, 49, 1024), Y_shape: (1568,)\n",
      "99/120 folders downloaded\n",
      "downloaded folder: NR13_20200428_PGS_21_BSD_P1/111_2020_04_28.csv\n",
      "Shape of collected datafram: X_shape: (1584, 49, 1024), Y_shape: (1584,)\n",
      "100/120 folders downloaded\n",
      "downloaded folder: NR13_20200428_PGS_21_BSD_P1/104_2020_04_28.csv\n",
      "Shape of collected datafram: X_shape: (1600, 49, 1024), Y_shape: (1600,)\n",
      "101/120 folders downloaded\n",
      "downloaded folder: NR01_20200317_PGS_31_BSD_31/020_2020_03_18.csv\n",
      "Shape of collected datafram: X_shape: (1616, 49, 1024), Y_shape: (1616,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/120 folders downloaded\n",
      "downloaded folder: NR01_20200317_PGS_31_BSD_31/021_2020_03_18.csv\n",
      "Shape of collected datafram: X_shape: (1632, 49, 1024), Y_shape: (1632,)\n",
      "103/120 folders downloaded\n",
      "downloaded folder: NR01_20200317_PGS_31_BSD_31/023_2020_03_18.csv\n",
      "Shape of collected datafram: X_shape: (1648, 49, 1024), Y_shape: (1648,)\n",
      "104/120 folders downloaded\n",
      "downloaded folder: NR01_20200317_PGS_31_BSD_31/022_2020_03_18.csv\n",
      "Shape of collected datafram: X_shape: (1664, 49, 1024), Y_shape: (1664,)\n",
      "105/120 folders downloaded\n",
      "downloaded folder: NR01_20200317_PGS_31_BSD_31/019_2020_03_18.csv\n",
      "Shape of collected datafram: X_shape: (1680, 49, 1024), Y_shape: (1680,)\n",
      "106/120 folders downloaded\n",
      "downloaded folder: NR01_20200317_PGS_31_BSD_31/016_2020_03_18.csv\n",
      "Shape of collected datafram: X_shape: (1696, 49, 1024), Y_shape: (1696,)\n",
      "107/120 folders downloaded\n",
      "downloaded folder: NR01_20200317_PGS_31_BSD_31/017_2020_03_18.csv\n",
      "Shape of collected datafram: X_shape: (1712, 49, 1024), Y_shape: (1712,)\n",
      "108/120 folders downloaded\n",
      "downloaded folder: NR01_20200317_PGS_31_BSD_31/018_2020_03_18.csv\n",
      "Shape of collected datafram: X_shape: (1728, 49, 1024), Y_shape: (1728,)\n",
      "109/120 folders downloaded\n",
      "downloaded folder: NR01_20200317_PGS_31_BSD_31/015_2020_03_18.csv\n",
      "Shape of collected datafram: X_shape: (1744, 49, 1024), Y_shape: (1744,)\n",
      "110/120 folders downloaded\n",
      "downloaded folder: NR01_20200317_PGS_31_BSD_31/014_2020_03_18.csv\n",
      "Shape of collected datafram: X_shape: (1760, 49, 1024), Y_shape: (1760,)\n",
      "111/120 folders downloaded\n",
      "downloaded folder: NR04_20200424_PGS_31_BSD_P1/089_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (1776, 49, 1024), Y_shape: (1776,)\n",
      "112/120 folders downloaded\n",
      "downloaded folder: NR04_20200424_PGS_31_BSD_P1/081_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (1792, 49, 1024), Y_shape: (1792,)\n",
      "113/120 folders downloaded\n",
      "downloaded folder: NR04_20200424_PGS_31_BSD_P1/086_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (1808, 49, 1024), Y_shape: (1808,)\n",
      "114/120 folders downloaded\n",
      "downloaded folder: NR04_20200424_PGS_31_BSD_P1/087_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (1824, 49, 1024), Y_shape: (1824,)\n",
      "115/120 folders downloaded\n",
      "downloaded folder: NR04_20200424_PGS_31_BSD_P1/080_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (1840, 49, 1024), Y_shape: (1840,)\n",
      "116/120 folders downloaded\n",
      "downloaded folder: NR04_20200424_PGS_31_BSD_P1/088_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (1856, 49, 1024), Y_shape: (1856,)\n",
      "117/120 folders downloaded\n",
      "downloaded folder: NR04_20200424_PGS_31_BSD_P1/082_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (1872, 49, 1024), Y_shape: (1872,)\n",
      "118/120 folders downloaded\n",
      "downloaded folder: NR04_20200424_PGS_31_BSD_P1/085_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (1888, 49, 1024), Y_shape: (1888,)\n",
      "119/120 folders downloaded\n",
      "downloaded folder: NR04_20200424_PGS_31_BSD_P1/084_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (1904, 49, 1024), Y_shape: (1904,)\n",
      "120/120 folders downloaded\n",
      "downloaded folder: NR04_20200424_PGS_31_BSD_P1/083_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (1920, 49, 1024), Y_shape: (1920,)\n"
     ]
    }
   ],
   "source": [
    "class TimeSeriesData_Single_Feature(Dataset):\n",
    "    def __init__(self):\n",
    "        window_size = 1024\n",
    "        features_of_interest = ['C:s_ist/X', 'C:s_soll/X', 'C:s_diff/X', 'C:v_(n_ist)/X', 'C:v_(n_soll)/X', 'C:P_mech./X', 'C:Pos._Diff./X',\n",
    "    'C:I_ist/X', 'C:I_soll/X', 'C:x_bottom', 'C:y_bottom', 'C:z_bottom', 'C:x_nut', 'C:y_nut', 'C:z_nut',\n",
    "    'C:x_top', 'C:y_top', 'C:z_top', 'D:s_ist/X', 'D:s_soll/X', 'D:s_diff/X', 'D:v_(n_ist)/X', 'D:v_(n_soll)/X',\n",
    "    'D:P_mech./X', 'D:Pos._Diff./X', 'D:I_ist/X', 'D:I_soll/X', 'D:x_bottom', 'D:y_bottom', 'D:z_bottom',\n",
    "    'D:x_nut', 'D:y_nut', 'D:z_nut', 'D:x_top', 'D:y_top', 'D:z_top', 'S:x_bottom', 'S:y_bottom', 'S:z_bottom',\n",
    "    'S:x_nut', 'S:y_nut', 'S:z_nut', 'S:x_top', 'S:y_top', 'S:z_top', 'S:Nominal_rotational_speed[rad/s]',\n",
    "    'S:Actual_rotational_speed[µm/s]', 'S:Actual_position_of_the_position_encoder(dy/dt)[µm/s]',\n",
    "    'S:Actual_position_of_the_motor_encoder(dy/dt)[µm/s]']\n",
    "        number_of_files_per_BDS_state = 10\n",
    "        list_of_train_BSD_states = [\"BSD_31\", \"BSD_21\", \"BSD_11\", \"BSD_P1\"]\n",
    "        list_of_test_BSD_states = [\"BSD_32\", \"BSD_22\", \"BSD_12\", \"BSD_P2\"]\n",
    "        data_path = os.path.join(os.path.abspath(os.path.join(os.getcwd(), os.pardir)), \"data\")\n",
    "        \n",
    "        training_folders, testing_folders = create_folder_dictionary(list_of_train_BSD_states, list_of_test_BSD_states, data_path)\n",
    "        \n",
    "        self.n_samples, self.x_data, self.y_data = concatenate_data_from_BSD_state(training_folders, data_path, features_of_interest, window_size)\n",
    "        \n",
    "                  \n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "dataset = TimeSeriesData_Single_Feature()\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "class TimeSeriesData_Single_Feature(Dataset):\n",
    "    def __init__(self):\n",
    "        window_size = 1024\n",
    "        feature = 'D:y_bottom'\n",
    "        \n",
    "        data_path = os.path.join(os.path.abspath(os.path.join(os.getcwd(), os.pardir)), \"data\")\n",
    "        training_folders = {}\n",
    "        testing_folders = {}\n",
    "        for element in os.listdir(data_path):\n",
    "            if \"BSD_31\" in element or \"BSD_21\"  in element or \"BSD_11\" in element  or \"BSD_P1\" in element: \n",
    "                training_folders[element]  = os.listdir(os.path.join(data_path,element))\n",
    "            elif \"csv\" in element:\n",
    "                pass\n",
    "            else:\n",
    "                testing_folders[element] = os.listdir(os.path.join(data_path,element))\n",
    "                \n",
    "\n",
    "\n",
    "        #data_BSD11_collected = np.empty((0,1, window_size))\n",
    "        #data_BSD21_collected = np.empty((0,1, window_size))\n",
    "        #data_BSD31_collected = np.empty((0,1, window_size))\n",
    "        \n",
    "        \n",
    "        data_BSD11_collected = None\n",
    "        data_BSD21_collected = None\n",
    "        data_BSD31_collected = None\n",
    "\n",
    "        for i in range(len(training_folders['NR03_20200424_PGS_31_BSD_11'])):                \n",
    "            \n",
    "            path_BSD11 = os.path.join(data_path, 'NR03_20200424_PGS_31_BSD_11', training_folders['NR03_20200424_PGS_31_BSD_11'][i])\n",
    "            path_BSD21 = os.path.join(data_path, 'NR02_20200423_PGS_31_BSD_21', training_folders['NR02_20200423_PGS_31_BSD_21'][i])\n",
    "            path_BSD31 = os.path.join(data_path, 'NR01_20200317_PGS_31_BSD_31', training_folders['NR01_20200317_PGS_31_BSD_31'][i])\n",
    "\n",
    "            if i == 0:\n",
    "                with open(path_BSD11, 'r') as file:\n",
    "                    csvreader = csv.reader(file)\n",
    "                    features = next(csvreader)\n",
    "            \n",
    "            data_BSD11 = np.genfromtxt(path_BSD11, dtype = np.dtype('d'), delimiter=',')[1:,:] #write csv in numpy\n",
    "            data_BSD21 = np.genfromtxt(path_BSD21, dtype = np.dtype('d'), delimiter=',')[1:,:] #write csv in numpy\n",
    "            data_BSD31 = np.genfromtxt(path_BSD31, dtype = np.dtype('d'), delimiter=',')[1:,:] #write csv in numpy\n",
    "            \n",
    "\n",
    "            data_BSD11_loaded = load_data(del_nan_element(data_BSD11[:,features.index(feature)]), window_size)\n",
    "            data_BSD11_loaded = np.expand_dims(data_BSD11_loaded, axis = 1)\n",
    "            if i == 0:\n",
    "                data_BSD11_collected = np.copy(data_BSD11_loaded)\n",
    "            else:\n",
    "                data_BSD11_collected = np.concatenate((data_BSD11_collected, data_BSD11_loaded), axis=0)\n",
    "                \n",
    "\n",
    "            data_BSD21_loaded = load_data(del_nan_element(data_BSD21[:,features.index(feature)]), window_size)\n",
    "            data_BSD21_loaded = np.expand_dims(data_BSD21_loaded, axis = 1)\n",
    "            if i == 0:\n",
    "                data_BSD21_collected = np.copy(data_BSD21_loaded)\n",
    "            else:\n",
    "                data_BSD21_collected = np.concatenate((data_BSD21_collected, data_BSD21_loaded), axis=0)\n",
    "            \n",
    "            \n",
    "            data_BSD31_loaded = load_data(del_nan_element(data_BSD31[:,features.index(feature)]), window_size)\n",
    "            data_BSD31_loaded = np.expand_dims(data_BSD31_loaded, axis = 1)\n",
    "            if i == 0:\n",
    "                data_BSD31_collected = np.copy(data_BSD31_loaded)\n",
    "            else:\n",
    "                data_BSD31_collected = np.concatenate((data_BSD31_collected, data_BSD31_loaded), axis=0)\n",
    "            \n",
    "            \n",
    "            print(f\"number of loaded train files: {i}/ {len(training_folders['NR03_20200424_PGS_31_BSD_11'])-1}\")\n",
    "        \n",
    "        y_BSD11 = np.asarray([0]*np.shape(data_BSD11_collected)[0])\n",
    "        y_BSD21 = np.asarray([1]*np.shape(data_BSD21_collected)[0])\n",
    "        y_BSD31 = np.asarray([2]*np.shape(data_BSD31_collected)[0])\n",
    "                  \n",
    "                  \n",
    "        data_x = np.concatenate((data_BSD11_collected, data_BSD21_collected, data_BSD31_collected), axis=0)\n",
    "        data_y = np.concatenate((y_BSD11, y_BSD21, y_BSD31), axis=0)\n",
    "        \n",
    "        self.x_data = torch.from_numpy(data_x)\n",
    "        self.y_data = torch.from_numpy(data_y)\n",
    "        self.n_samples = np.shape(data_x)[0]\n",
    "        \n",
    "                  \n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TimeSeriesData_Single_Feature()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size,hidden_size,num_layers):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        formula [(W−K+2P)/S]+1.\n",
    "        \"\"\"\n",
    "        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=2, stride=1)#input: 1024\n",
    "        self.conv2 = nn.Conv1d(64,32,kernel_size=1, stride = 1, padding=1)#input: [(1025-2+2*0)/1]+1 = 1023\n",
    "        self.batch1 =nn.BatchNorm1d(32)#input: [(1023-1+2*1)/1]+1 = 1025\n",
    "        self.conv3 = nn.Conv1d(32,32,kernel_size=1, stride = 1, padding=1) #input:1025\n",
    "        self.batch2 =nn.BatchNorm1d(32)#input: [(1025-2+0)/1]+1 = 1027\n",
    "        self.LSTM = nn.LSTM(input_size=1027, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        #self.fc1 = nn.Linear(32*hidden_size, output_size)\n",
    "        self.fc1 = nn.Linear(32*1027, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.selu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = F.selu(self.batch1(x))\n",
    "        x = self.conv3(x)\n",
    "        x = F.selu(self.batch2(x))\n",
    "        #x, h = self.LSTM(x) \n",
    "        x = torch.reshape(x,(x.shape[0],x.shape[1]*x.shape[2]))\n",
    "        x = self.fc1(x)\n",
    "        output = x\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNLSTM(\n",
      "  (conv1): Conv1d(49, 64, kernel_size=(2,), stride=(1,))\n",
      "  (conv2): Conv1d(64, 32, kernel_size=(1,), stride=(1,), padding=(1,))\n",
      "  (batch1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv1d(32, 32, kernel_size=(1,), stride=(1,), padding=(1,))\n",
      "  (batch2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (LSTM): LSTM(1027, 1000, num_layers=2, batch_first=True)\n",
      "  (fc1): Linear(in_features=32864, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 49\n",
    "output_size = 4\n",
    "hidden_size = 1000\n",
    "num_layers = 2\n",
    "\n",
    "model = CNNLSTM(input_size, output_size,hidden_size, num_layers)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Step [20/384], Loss: 176.2216\n",
      "Epoch [1/300], Step [40/384], Loss: 115.6225\n",
      "Epoch [1/300], Step [60/384], Loss: 113.7861\n",
      "Epoch [1/300], Step [80/384], Loss: 6.2788\n",
      "Epoch [1/300], Step [100/384], Loss: 336.2570\n",
      "Epoch [1/300], Step [120/384], Loss: 1048.8601\n",
      "Epoch [1/300], Step [140/384], Loss: 208.6692\n",
      "Epoch [1/300], Step [160/384], Loss: 358.9186\n",
      "Epoch [1/300], Step [180/384], Loss: 132.0727\n",
      "Epoch [1/300], Step [200/384], Loss: 449.2500\n",
      "Epoch [1/300], Step [220/384], Loss: 185.4740\n",
      "Epoch [1/300], Step [240/384], Loss: 84.5571\n",
      "Epoch [1/300], Step [260/384], Loss: 209.4806\n",
      "Epoch [1/300], Step [280/384], Loss: 389.3882\n",
      "Epoch [1/300], Step [300/384], Loss: 100.8899\n",
      "Epoch [1/300], Step [320/384], Loss: 95.0956\n",
      "Epoch [1/300], Step [340/384], Loss: 352.0946\n",
      "Epoch [1/300], Step [360/384], Loss: 270.8908\n",
      "Epoch [1/300], Step [380/384], Loss: 28.8085\n",
      "Epoch [2/300], Step [20/384], Loss: 201.6741\n",
      "Epoch [2/300], Step [40/384], Loss: 243.2604\n",
      "Epoch [2/300], Step [60/384], Loss: 349.5515\n",
      "Epoch [2/300], Step [80/384], Loss: 850.5218\n",
      "Epoch [2/300], Step [100/384], Loss: 375.6591\n",
      "Epoch [2/300], Step [120/384], Loss: 195.1398\n",
      "Epoch [2/300], Step [140/384], Loss: 303.9767\n",
      "Epoch [2/300], Step [160/384], Loss: 243.0852\n",
      "Epoch [2/300], Step [180/384], Loss: 343.2138\n",
      "Epoch [2/300], Step [200/384], Loss: 180.3932\n",
      "Epoch [2/300], Step [220/384], Loss: 74.0069\n",
      "Epoch [2/300], Step [240/384], Loss: 798.5521\n",
      "Epoch [2/300], Step [260/384], Loss: 245.9857\n",
      "Epoch [2/300], Step [280/384], Loss: 136.3091\n",
      "Epoch [2/300], Step [300/384], Loss: 57.6484\n",
      "Epoch [2/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [2/300], Step [340/384], Loss: 462.1468\n",
      "Epoch [2/300], Step [360/384], Loss: 412.8425\n",
      "Epoch [2/300], Step [380/384], Loss: 186.5971\n",
      "Epoch [3/300], Step [20/384], Loss: 54.3235\n",
      "Epoch [3/300], Step [40/384], Loss: 273.8480\n",
      "Epoch [3/300], Step [60/384], Loss: 124.8888\n",
      "Epoch [3/300], Step [80/384], Loss: 546.7826\n",
      "Epoch [3/300], Step [100/384], Loss: 645.2771\n",
      "Epoch [3/300], Step [120/384], Loss: 252.0900\n",
      "Epoch [3/300], Step [140/384], Loss: 214.8069\n",
      "Epoch [3/300], Step [160/384], Loss: 420.0273\n",
      "Epoch [3/300], Step [180/384], Loss: 107.8010\n",
      "Epoch [3/300], Step [200/384], Loss: 315.9930\n",
      "Epoch [3/300], Step [220/384], Loss: 73.1513\n",
      "Epoch [3/300], Step [240/384], Loss: 674.7268\n",
      "Epoch [3/300], Step [260/384], Loss: 184.8130\n",
      "Epoch [3/300], Step [280/384], Loss: 217.2345\n",
      "Epoch [3/300], Step [300/384], Loss: 226.2684\n",
      "Epoch [3/300], Step [320/384], Loss: 490.1893\n",
      "Epoch [3/300], Step [340/384], Loss: 325.7801\n",
      "Epoch [3/300], Step [360/384], Loss: 133.6876\n",
      "Epoch [3/300], Step [380/384], Loss: 327.1679\n",
      "Epoch [4/300], Step [20/384], Loss: 389.5373\n",
      "Epoch [4/300], Step [40/384], Loss: 140.1238\n",
      "Epoch [4/300], Step [60/384], Loss: 596.8061\n",
      "Epoch [4/300], Step [80/384], Loss: 170.4752\n",
      "Epoch [4/300], Step [100/384], Loss: 176.6543\n",
      "Epoch [4/300], Step [120/384], Loss: 90.7874\n",
      "Epoch [4/300], Step [140/384], Loss: 252.5672\n",
      "Epoch [4/300], Step [160/384], Loss: 233.7051\n",
      "Epoch [4/300], Step [180/384], Loss: 238.5690\n",
      "Epoch [4/300], Step [200/384], Loss: 186.4563\n",
      "Epoch [4/300], Step [220/384], Loss: 322.2566\n",
      "Epoch [4/300], Step [240/384], Loss: 69.3368\n",
      "Epoch [4/300], Step [260/384], Loss: 36.2346\n",
      "Epoch [4/300], Step [280/384], Loss: 191.7397\n",
      "Epoch [4/300], Step [300/384], Loss: 153.3120\n",
      "Epoch [4/300], Step [320/384], Loss: 98.8276\n",
      "Epoch [4/300], Step [340/384], Loss: 268.0759\n",
      "Epoch [4/300], Step [360/384], Loss: 139.0225\n",
      "Epoch [4/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [5/300], Step [20/384], Loss: 3.3205\n",
      "Epoch [5/300], Step [40/384], Loss: 404.5790\n",
      "Epoch [5/300], Step [60/384], Loss: 121.1616\n",
      "Epoch [5/300], Step [80/384], Loss: 335.6649\n",
      "Epoch [5/300], Step [100/384], Loss: 184.9553\n",
      "Epoch [5/300], Step [120/384], Loss: 13.5513\n",
      "Epoch [5/300], Step [140/384], Loss: 294.4239\n",
      "Epoch [5/300], Step [160/384], Loss: 49.8862\n",
      "Epoch [5/300], Step [180/384], Loss: 178.8657\n",
      "Epoch [5/300], Step [200/384], Loss: 142.7625\n",
      "Epoch [5/300], Step [220/384], Loss: 202.9361\n",
      "Epoch [5/300], Step [240/384], Loss: 446.6769\n",
      "Epoch [5/300], Step [260/384], Loss: 168.6686\n",
      "Epoch [5/300], Step [280/384], Loss: 142.3040\n",
      "Epoch [5/300], Step [300/384], Loss: 65.0047\n",
      "Epoch [5/300], Step [320/384], Loss: 120.6942\n",
      "Epoch [5/300], Step [340/384], Loss: 30.5632\n",
      "Epoch [5/300], Step [360/384], Loss: 279.5207\n",
      "Epoch [5/300], Step [380/384], Loss: 170.4192\n",
      "Epoch [6/300], Step [20/384], Loss: 494.0749\n",
      "Epoch [6/300], Step [40/384], Loss: 274.7858\n",
      "Epoch [6/300], Step [60/384], Loss: 189.8079\n",
      "Epoch [6/300], Step [80/384], Loss: 347.3825\n",
      "Epoch [6/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [6/300], Step [120/384], Loss: 220.6780\n",
      "Epoch [6/300], Step [140/384], Loss: 43.5644\n",
      "Epoch [6/300], Step [160/384], Loss: 132.6863\n",
      "Epoch [6/300], Step [180/384], Loss: 42.7672\n",
      "Epoch [6/300], Step [200/384], Loss: 174.1095\n",
      "Epoch [6/300], Step [220/384], Loss: 285.3135\n",
      "Epoch [6/300], Step [240/384], Loss: 314.9284\n",
      "Epoch [6/300], Step [260/384], Loss: 230.6310\n",
      "Epoch [6/300], Step [280/384], Loss: 272.7675\n",
      "Epoch [6/300], Step [300/384], Loss: 268.5583\n",
      "Epoch [6/300], Step [320/384], Loss: 207.8508\n",
      "Epoch [6/300], Step [340/384], Loss: 64.2267\n",
      "Epoch [6/300], Step [360/384], Loss: 469.4888\n",
      "Epoch [6/300], Step [380/384], Loss: 224.2397\n",
      "Epoch [7/300], Step [20/384], Loss: 104.3498\n",
      "Epoch [7/300], Step [40/384], Loss: 155.4766\n",
      "Epoch [7/300], Step [60/384], Loss: 112.1083\n",
      "Epoch [7/300], Step [80/384], Loss: 509.2223\n",
      "Epoch [7/300], Step [100/384], Loss: 29.5501\n",
      "Epoch [7/300], Step [120/384], Loss: 82.8133\n",
      "Epoch [7/300], Step [140/384], Loss: 186.8637\n",
      "Epoch [7/300], Step [160/384], Loss: 169.5114\n",
      "Epoch [7/300], Step [180/384], Loss: 292.1115\n",
      "Epoch [7/300], Step [200/384], Loss: 210.3516\n",
      "Epoch [7/300], Step [220/384], Loss: 129.0535\n",
      "Epoch [7/300], Step [240/384], Loss: 477.2210\n",
      "Epoch [7/300], Step [260/384], Loss: 412.3375\n",
      "Epoch [7/300], Step [280/384], Loss: 50.5470\n",
      "Epoch [7/300], Step [300/384], Loss: 185.0083\n",
      "Epoch [7/300], Step [320/384], Loss: 194.1347\n",
      "Epoch [7/300], Step [340/384], Loss: 221.8221\n",
      "Epoch [7/300], Step [360/384], Loss: 138.9414\n",
      "Epoch [7/300], Step [380/384], Loss: 468.2784\n",
      "Epoch [8/300], Step [20/384], Loss: 151.7154\n",
      "Epoch [8/300], Step [40/384], Loss: 222.2658\n",
      "Epoch [8/300], Step [60/384], Loss: 76.7399\n",
      "Epoch [8/300], Step [80/384], Loss: 190.1827\n",
      "Epoch [8/300], Step [100/384], Loss: 61.1448\n",
      "Epoch [8/300], Step [120/384], Loss: 86.5241\n",
      "Epoch [8/300], Step [140/384], Loss: 54.1831\n",
      "Epoch [8/300], Step [160/384], Loss: 21.9387\n",
      "Epoch [8/300], Step [180/384], Loss: 140.0605\n",
      "Epoch [8/300], Step [200/384], Loss: 214.6870\n",
      "Epoch [8/300], Step [220/384], Loss: 109.7477\n",
      "Epoch [8/300], Step [240/384], Loss: 38.6229\n",
      "Epoch [8/300], Step [260/384], Loss: 500.5539\n",
      "Epoch [8/300], Step [280/384], Loss: 42.1405\n",
      "Epoch [8/300], Step [300/384], Loss: 127.3064\n",
      "Epoch [8/300], Step [320/384], Loss: 64.9748\n",
      "Epoch [8/300], Step [340/384], Loss: 246.6681\n",
      "Epoch [8/300], Step [360/384], Loss: 209.2066\n",
      "Epoch [8/300], Step [380/384], Loss: 234.8328\n",
      "Epoch [9/300], Step [20/384], Loss: 257.5560\n",
      "Epoch [9/300], Step [40/384], Loss: 675.7160\n",
      "Epoch [9/300], Step [60/384], Loss: 36.1283\n",
      "Epoch [9/300], Step [80/384], Loss: 339.5772\n",
      "Epoch [9/300], Step [100/384], Loss: 411.8564\n",
      "Epoch [9/300], Step [120/384], Loss: 236.5098\n",
      "Epoch [9/300], Step [140/384], Loss: 112.9347\n",
      "Epoch [9/300], Step [160/384], Loss: 61.2569\n",
      "Epoch [9/300], Step [180/384], Loss: 109.8526\n",
      "Epoch [9/300], Step [200/384], Loss: 47.2060\n",
      "Epoch [9/300], Step [220/384], Loss: 121.6198\n",
      "Epoch [9/300], Step [240/384], Loss: 691.6053\n",
      "Epoch [9/300], Step [260/384], Loss: 179.2279\n",
      "Epoch [9/300], Step [280/384], Loss: 8.6242\n",
      "Epoch [9/300], Step [300/384], Loss: 493.7797\n",
      "Epoch [9/300], Step [320/384], Loss: 232.0803\n",
      "Epoch [9/300], Step [340/384], Loss: 260.1858\n",
      "Epoch [9/300], Step [360/384], Loss: 272.2631\n",
      "Epoch [9/300], Step [380/384], Loss: 536.5875\n",
      "Epoch [10/300], Step [20/384], Loss: 238.9319\n",
      "Epoch [10/300], Step [40/384], Loss: 37.3160\n",
      "Epoch [10/300], Step [60/384], Loss: 346.6078\n",
      "Epoch [10/300], Step [80/384], Loss: 269.3884\n",
      "Epoch [10/300], Step [100/384], Loss: 431.4458\n",
      "Epoch [10/300], Step [120/384], Loss: 141.5560\n",
      "Epoch [10/300], Step [140/384], Loss: 67.7078\n",
      "Epoch [10/300], Step [160/384], Loss: 222.5265\n",
      "Epoch [10/300], Step [180/384], Loss: 145.3696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Step [200/384], Loss: 276.3358\n",
      "Epoch [10/300], Step [220/384], Loss: 86.9628\n",
      "Epoch [10/300], Step [240/384], Loss: 89.7641\n",
      "Epoch [10/300], Step [260/384], Loss: 373.2453\n",
      "Epoch [10/300], Step [280/384], Loss: 36.4367\n",
      "Epoch [10/300], Step [300/384], Loss: 236.5370\n",
      "Epoch [10/300], Step [320/384], Loss: 175.7839\n",
      "Epoch [10/300], Step [340/384], Loss: 296.2721\n",
      "Epoch [10/300], Step [360/384], Loss: 91.3030\n",
      "Epoch [10/300], Step [380/384], Loss: 53.0387\n",
      "Epoch [11/300], Step [20/384], Loss: 150.1892\n",
      "Epoch [11/300], Step [40/384], Loss: 226.7455\n",
      "Epoch [11/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [11/300], Step [80/384], Loss: 152.6787\n",
      "Epoch [11/300], Step [100/384], Loss: 92.1328\n",
      "Epoch [11/300], Step [120/384], Loss: 56.0465\n",
      "Epoch [11/300], Step [140/384], Loss: 291.5152\n",
      "Epoch [11/300], Step [160/384], Loss: 162.8896\n",
      "Epoch [11/300], Step [180/384], Loss: 372.0794\n",
      "Epoch [11/300], Step [200/384], Loss: 177.1702\n",
      "Epoch [11/300], Step [220/384], Loss: 58.3453\n",
      "Epoch [11/300], Step [240/384], Loss: 152.3551\n",
      "Epoch [11/300], Step [260/384], Loss: 118.3514\n",
      "Epoch [11/300], Step [280/384], Loss: 50.0812\n",
      "Epoch [11/300], Step [300/384], Loss: 303.9891\n",
      "Epoch [11/300], Step [320/384], Loss: 186.6983\n",
      "Epoch [11/300], Step [340/384], Loss: 195.0657\n",
      "Epoch [11/300], Step [360/384], Loss: 95.6604\n",
      "Epoch [11/300], Step [380/384], Loss: 30.2537\n",
      "Epoch [12/300], Step [20/384], Loss: 178.2685\n",
      "Epoch [12/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [12/300], Step [60/384], Loss: 518.9617\n",
      "Epoch [12/300], Step [80/384], Loss: 291.7367\n",
      "Epoch [12/300], Step [100/384], Loss: 118.8459\n",
      "Epoch [12/300], Step [120/384], Loss: 79.7891\n",
      "Epoch [12/300], Step [140/384], Loss: 114.7451\n",
      "Epoch [12/300], Step [160/384], Loss: 214.3064\n",
      "Epoch [12/300], Step [180/384], Loss: 109.0106\n",
      "Epoch [12/300], Step [200/384], Loss: 493.7366\n",
      "Epoch [12/300], Step [220/384], Loss: 662.8901\n",
      "Epoch [12/300], Step [240/384], Loss: 235.3129\n",
      "Epoch [12/300], Step [260/384], Loss: 442.6215\n",
      "Epoch [12/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [12/300], Step [300/384], Loss: 179.3795\n",
      "Epoch [12/300], Step [320/384], Loss: 111.7829\n",
      "Epoch [12/300], Step [340/384], Loss: 81.6074\n",
      "Epoch [12/300], Step [360/384], Loss: 569.5897\n",
      "Epoch [12/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [13/300], Step [20/384], Loss: 250.4509\n",
      "Epoch [13/300], Step [40/384], Loss: 249.1758\n",
      "Epoch [13/300], Step [60/384], Loss: 117.3707\n",
      "Epoch [13/300], Step [80/384], Loss: 93.3445\n",
      "Epoch [13/300], Step [100/384], Loss: 86.2428\n",
      "Epoch [13/300], Step [120/384], Loss: 94.1673\n",
      "Epoch [13/300], Step [140/384], Loss: 87.0645\n",
      "Epoch [13/300], Step [160/384], Loss: 345.6119\n",
      "Epoch [13/300], Step [180/384], Loss: 232.7471\n",
      "Epoch [13/300], Step [200/384], Loss: 95.5795\n",
      "Epoch [13/300], Step [220/384], Loss: 205.1053\n",
      "Epoch [13/300], Step [240/384], Loss: 226.7629\n",
      "Epoch [13/300], Step [260/384], Loss: 10.9011\n",
      "Epoch [13/300], Step [280/384], Loss: 316.1799\n",
      "Epoch [13/300], Step [300/384], Loss: 269.7089\n",
      "Epoch [13/300], Step [320/384], Loss: 87.7009\n",
      "Epoch [13/300], Step [340/384], Loss: 245.7417\n",
      "Epoch [13/300], Step [360/384], Loss: 39.7925\n",
      "Epoch [13/300], Step [380/384], Loss: 215.2525\n",
      "Epoch [14/300], Step [20/384], Loss: 617.6694\n",
      "Epoch [14/300], Step [40/384], Loss: 38.2819\n",
      "Epoch [14/300], Step [60/384], Loss: 55.4974\n",
      "Epoch [14/300], Step [80/384], Loss: 417.8641\n",
      "Epoch [14/300], Step [100/384], Loss: 91.1175\n",
      "Epoch [14/300], Step [120/384], Loss: 139.8832\n",
      "Epoch [14/300], Step [140/384], Loss: 61.5604\n",
      "Epoch [14/300], Step [160/384], Loss: 196.0593\n",
      "Epoch [14/300], Step [180/384], Loss: 73.7247\n",
      "Epoch [14/300], Step [200/384], Loss: 275.5109\n",
      "Epoch [14/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [14/300], Step [240/384], Loss: 349.1252\n",
      "Epoch [14/300], Step [260/384], Loss: 165.9750\n",
      "Epoch [14/300], Step [280/384], Loss: 435.9018\n",
      "Epoch [14/300], Step [300/384], Loss: 178.5326\n",
      "Epoch [14/300], Step [320/384], Loss: 743.6645\n",
      "Epoch [14/300], Step [340/384], Loss: 72.9626\n",
      "Epoch [14/300], Step [360/384], Loss: 59.4830\n",
      "Epoch [14/300], Step [380/384], Loss: 133.7612\n",
      "Epoch [15/300], Step [20/384], Loss: 167.2068\n",
      "Epoch [15/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [15/300], Step [60/384], Loss: 436.3457\n",
      "Epoch [15/300], Step [80/384], Loss: 43.0774\n",
      "Epoch [15/300], Step [100/384], Loss: 171.1128\n",
      "Epoch [15/300], Step [120/384], Loss: 303.5412\n",
      "Epoch [15/300], Step [140/384], Loss: 71.9913\n",
      "Epoch [15/300], Step [160/384], Loss: 32.9095\n",
      "Epoch [15/300], Step [180/384], Loss: 260.0076\n",
      "Epoch [15/300], Step [200/384], Loss: 158.8285\n",
      "Epoch [15/300], Step [220/384], Loss: 363.2318\n",
      "Epoch [15/300], Step [240/384], Loss: 7.0952\n",
      "Epoch [15/300], Step [260/384], Loss: 151.5861\n",
      "Epoch [15/300], Step [280/384], Loss: 207.8067\n",
      "Epoch [15/300], Step [300/384], Loss: 204.4653\n",
      "Epoch [15/300], Step [320/384], Loss: 145.0028\n",
      "Epoch [15/300], Step [340/384], Loss: 135.0941\n",
      "Epoch [15/300], Step [360/384], Loss: 179.5454\n",
      "Epoch [15/300], Step [380/384], Loss: 161.1415\n",
      "Epoch [16/300], Step [20/384], Loss: 109.8119\n",
      "Epoch [16/300], Step [40/384], Loss: 153.4469\n",
      "Epoch [16/300], Step [60/384], Loss: 147.4045\n",
      "Epoch [16/300], Step [80/384], Loss: 30.5962\n",
      "Epoch [16/300], Step [100/384], Loss: 174.3658\n",
      "Epoch [16/300], Step [120/384], Loss: 302.9380\n",
      "Epoch [16/300], Step [140/384], Loss: 406.7729\n",
      "Epoch [16/300], Step [160/384], Loss: 8.7759\n",
      "Epoch [16/300], Step [180/384], Loss: 159.8936\n",
      "Epoch [16/300], Step [200/384], Loss: 154.4903\n",
      "Epoch [16/300], Step [220/384], Loss: 78.9265\n",
      "Epoch [16/300], Step [240/384], Loss: 83.3685\n",
      "Epoch [16/300], Step [260/384], Loss: 202.4635\n",
      "Epoch [16/300], Step [280/384], Loss: 177.3757\n",
      "Epoch [16/300], Step [300/384], Loss: 70.2712\n",
      "Epoch [16/300], Step [320/384], Loss: 231.3077\n",
      "Epoch [16/300], Step [340/384], Loss: 566.0844\n",
      "Epoch [16/300], Step [360/384], Loss: 142.7024\n",
      "Epoch [16/300], Step [380/384], Loss: 154.3559\n",
      "Epoch [17/300], Step [20/384], Loss: 138.1393\n",
      "Epoch [17/300], Step [40/384], Loss: 103.5656\n",
      "Epoch [17/300], Step [60/384], Loss: 19.4599\n",
      "Epoch [17/300], Step [80/384], Loss: 62.2327\n",
      "Epoch [17/300], Step [100/384], Loss: 174.7430\n",
      "Epoch [17/300], Step [120/384], Loss: 290.6155\n",
      "Epoch [17/300], Step [140/384], Loss: 2.5739\n",
      "Epoch [17/300], Step [160/384], Loss: 348.2929\n",
      "Epoch [17/300], Step [180/384], Loss: 321.4147\n",
      "Epoch [17/300], Step [200/384], Loss: 278.7358\n",
      "Epoch [17/300], Step [220/384], Loss: 736.8032\n",
      "Epoch [17/300], Step [240/384], Loss: 152.9982\n",
      "Epoch [17/300], Step [260/384], Loss: 75.6152\n",
      "Epoch [17/300], Step [280/384], Loss: 157.6841\n",
      "Epoch [17/300], Step [300/384], Loss: 140.5795\n",
      "Epoch [17/300], Step [320/384], Loss: 157.7913\n",
      "Epoch [17/300], Step [340/384], Loss: 65.0504\n",
      "Epoch [17/300], Step [360/384], Loss: 188.4867\n",
      "Epoch [17/300], Step [380/384], Loss: 202.7452\n",
      "Epoch [18/300], Step [20/384], Loss: 97.4462\n",
      "Epoch [18/300], Step [40/384], Loss: 198.7834\n",
      "Epoch [18/300], Step [60/384], Loss: 125.7231\n",
      "Epoch [18/300], Step [80/384], Loss: 173.9565\n",
      "Epoch [18/300], Step [100/384], Loss: 105.6226\n",
      "Epoch [18/300], Step [120/384], Loss: 41.7366\n",
      "Epoch [18/300], Step [140/384], Loss: 200.4183\n",
      "Epoch [18/300], Step [160/384], Loss: 106.6475\n",
      "Epoch [18/300], Step [180/384], Loss: 253.9745\n",
      "Epoch [18/300], Step [200/384], Loss: 1.8944\n",
      "Epoch [18/300], Step [220/384], Loss: 23.8834\n",
      "Epoch [18/300], Step [240/384], Loss: 122.4074\n",
      "Epoch [18/300], Step [260/384], Loss: 74.5936\n",
      "Epoch [18/300], Step [280/384], Loss: 51.3304\n",
      "Epoch [18/300], Step [300/384], Loss: 35.4478\n",
      "Epoch [18/300], Step [320/384], Loss: 48.0543\n",
      "Epoch [18/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [18/300], Step [360/384], Loss: 206.1163\n",
      "Epoch [18/300], Step [380/384], Loss: 564.8677\n",
      "Epoch [19/300], Step [20/384], Loss: 96.3618\n",
      "Epoch [19/300], Step [40/384], Loss: 384.7192\n",
      "Epoch [19/300], Step [60/384], Loss: 207.3313\n",
      "Epoch [19/300], Step [80/384], Loss: 87.7501\n",
      "Epoch [19/300], Step [100/384], Loss: 64.4355\n",
      "Epoch [19/300], Step [120/384], Loss: 137.1384\n",
      "Epoch [19/300], Step [140/384], Loss: 162.3237\n",
      "Epoch [19/300], Step [160/384], Loss: 406.8105\n",
      "Epoch [19/300], Step [180/384], Loss: 174.8435\n",
      "Epoch [19/300], Step [200/384], Loss: 154.2420\n",
      "Epoch [19/300], Step [220/384], Loss: 253.0269\n",
      "Epoch [19/300], Step [240/384], Loss: 257.2250\n",
      "Epoch [19/300], Step [260/384], Loss: 86.2830\n",
      "Epoch [19/300], Step [280/384], Loss: 596.7119\n",
      "Epoch [19/300], Step [300/384], Loss: 115.7748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/300], Step [320/384], Loss: 359.3248\n",
      "Epoch [19/300], Step [340/384], Loss: 248.4505\n",
      "Epoch [19/300], Step [360/384], Loss: 49.6567\n",
      "Epoch [19/300], Step [380/384], Loss: 0.0681\n",
      "Epoch [20/300], Step [20/384], Loss: 150.2954\n",
      "Epoch [20/300], Step [40/384], Loss: 0.4534\n",
      "Epoch [20/300], Step [60/384], Loss: 109.6936\n",
      "Epoch [20/300], Step [80/384], Loss: 185.9288\n",
      "Epoch [20/300], Step [100/384], Loss: 126.8151\n",
      "Epoch [20/300], Step [120/384], Loss: 159.6874\n",
      "Epoch [20/300], Step [140/384], Loss: 5.4599\n",
      "Epoch [20/300], Step [160/384], Loss: 46.2326\n",
      "Epoch [20/300], Step [180/384], Loss: 160.0697\n",
      "Epoch [20/300], Step [200/384], Loss: 50.2716\n",
      "Epoch [20/300], Step [220/384], Loss: 446.4021\n",
      "Epoch [20/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [20/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [20/300], Step [280/384], Loss: 93.6576\n",
      "Epoch [20/300], Step [300/384], Loss: 250.4300\n",
      "Epoch [20/300], Step [320/384], Loss: 128.5643\n",
      "Epoch [20/300], Step [340/384], Loss: 148.0122\n",
      "Epoch [20/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [20/300], Step [380/384], Loss: 175.5372\n",
      "Epoch [21/300], Step [20/384], Loss: 2.6602\n",
      "Epoch [21/300], Step [40/384], Loss: 123.1169\n",
      "Epoch [21/300], Step [60/384], Loss: 64.2307\n",
      "Epoch [21/300], Step [80/384], Loss: 290.1529\n",
      "Epoch [21/300], Step [100/384], Loss: 587.0939\n",
      "Epoch [21/300], Step [120/384], Loss: 75.3737\n",
      "Epoch [21/300], Step [140/384], Loss: 176.9836\n",
      "Epoch [21/300], Step [160/384], Loss: 51.1969\n",
      "Epoch [21/300], Step [180/384], Loss: 49.9031\n",
      "Epoch [21/300], Step [200/384], Loss: 108.7431\n",
      "Epoch [21/300], Step [220/384], Loss: 296.5892\n",
      "Epoch [21/300], Step [240/384], Loss: 256.3574\n",
      "Epoch [21/300], Step [260/384], Loss: 65.7953\n",
      "Epoch [21/300], Step [280/384], Loss: 368.4022\n",
      "Epoch [21/300], Step [300/384], Loss: 250.3077\n",
      "Epoch [21/300], Step [320/384], Loss: 40.0371\n",
      "Epoch [21/300], Step [340/384], Loss: 182.0664\n",
      "Epoch [21/300], Step [360/384], Loss: 87.4288\n",
      "Epoch [21/300], Step [380/384], Loss: 107.4687\n",
      "Epoch [22/300], Step [20/384], Loss: 74.6897\n",
      "Epoch [22/300], Step [40/384], Loss: 129.4213\n",
      "Epoch [22/300], Step [60/384], Loss: 143.7151\n",
      "Epoch [22/300], Step [80/384], Loss: 67.1471\n",
      "Epoch [22/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [22/300], Step [120/384], Loss: 248.6510\n",
      "Epoch [22/300], Step [140/384], Loss: 351.8398\n",
      "Epoch [22/300], Step [160/384], Loss: 271.0641\n",
      "Epoch [22/300], Step [180/384], Loss: 55.5297\n",
      "Epoch [22/300], Step [200/384], Loss: 82.0863\n",
      "Epoch [22/300], Step [220/384], Loss: 36.8083\n",
      "Epoch [22/300], Step [240/384], Loss: 273.1543\n",
      "Epoch [22/300], Step [260/384], Loss: 18.4048\n",
      "Epoch [22/300], Step [280/384], Loss: 135.0772\n",
      "Epoch [22/300], Step [300/384], Loss: 22.7653\n",
      "Epoch [22/300], Step [320/384], Loss: 157.5126\n",
      "Epoch [22/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [22/300], Step [360/384], Loss: 273.3184\n",
      "Epoch [22/300], Step [380/384], Loss: 60.5820\n",
      "Epoch [23/300], Step [20/384], Loss: 245.0354\n",
      "Epoch [23/300], Step [40/384], Loss: 81.6001\n",
      "Epoch [23/300], Step [60/384], Loss: 48.4174\n",
      "Epoch [23/300], Step [80/384], Loss: 77.6823\n",
      "Epoch [23/300], Step [100/384], Loss: 89.5435\n",
      "Epoch [23/300], Step [120/384], Loss: 426.8755\n",
      "Epoch [23/300], Step [140/384], Loss: 77.6296\n",
      "Epoch [23/300], Step [160/384], Loss: 170.5035\n",
      "Epoch [23/300], Step [180/384], Loss: 225.8786\n",
      "Epoch [23/300], Step [200/384], Loss: 58.9414\n",
      "Epoch [23/300], Step [220/384], Loss: 230.5089\n",
      "Epoch [23/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [23/300], Step [260/384], Loss: 188.3714\n",
      "Epoch [23/300], Step [280/384], Loss: 103.9957\n",
      "Epoch [23/300], Step [300/384], Loss: 86.3530\n",
      "Epoch [23/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [23/300], Step [340/384], Loss: 25.4019\n",
      "Epoch [23/300], Step [360/384], Loss: 43.0518\n",
      "Epoch [23/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [24/300], Step [20/384], Loss: 50.5299\n",
      "Epoch [24/300], Step [40/384], Loss: 46.9272\n",
      "Epoch [24/300], Step [60/384], Loss: 69.1504\n",
      "Epoch [24/300], Step [80/384], Loss: 293.1129\n",
      "Epoch [24/300], Step [100/384], Loss: 0.4742\n",
      "Epoch [24/300], Step [120/384], Loss: 119.8317\n",
      "Epoch [24/300], Step [140/384], Loss: 93.7912\n",
      "Epoch [24/300], Step [160/384], Loss: 445.9630\n",
      "Epoch [24/300], Step [180/384], Loss: 73.7091\n",
      "Epoch [24/300], Step [200/384], Loss: 266.2441\n",
      "Epoch [24/300], Step [220/384], Loss: 350.1360\n",
      "Epoch [24/300], Step [240/384], Loss: 207.1621\n",
      "Epoch [24/300], Step [260/384], Loss: 120.8028\n",
      "Epoch [24/300], Step [280/384], Loss: 101.9425\n",
      "Epoch [24/300], Step [300/384], Loss: 255.9015\n",
      "Epoch [24/300], Step [320/384], Loss: 127.8611\n",
      "Epoch [24/300], Step [340/384], Loss: 188.3579\n",
      "Epoch [24/300], Step [360/384], Loss: 99.0078\n",
      "Epoch [24/300], Step [380/384], Loss: 162.5776\n",
      "Epoch [25/300], Step [20/384], Loss: 59.4945\n",
      "Epoch [25/300], Step [40/384], Loss: 3.7974\n",
      "Epoch [25/300], Step [60/384], Loss: 202.7737\n",
      "Epoch [25/300], Step [80/384], Loss: 28.5396\n",
      "Epoch [25/300], Step [100/384], Loss: 54.3520\n",
      "Epoch [25/300], Step [120/384], Loss: 74.6927\n",
      "Epoch [25/300], Step [140/384], Loss: 59.4582\n",
      "Epoch [25/300], Step [160/384], Loss: 31.7448\n",
      "Epoch [25/300], Step [180/384], Loss: 131.1904\n",
      "Epoch [25/300], Step [200/384], Loss: 143.9957\n",
      "Epoch [25/300], Step [220/384], Loss: 91.6459\n",
      "Epoch [25/300], Step [240/384], Loss: 127.4933\n",
      "Epoch [25/300], Step [260/384], Loss: 71.8903\n",
      "Epoch [25/300], Step [280/384], Loss: 4.5746\n",
      "Epoch [25/300], Step [300/384], Loss: 70.2044\n",
      "Epoch [25/300], Step [320/384], Loss: 145.4636\n",
      "Epoch [25/300], Step [340/384], Loss: 131.0608\n",
      "Epoch [25/300], Step [360/384], Loss: 76.7719\n",
      "Epoch [25/300], Step [380/384], Loss: 46.6378\n",
      "Epoch [26/300], Step [20/384], Loss: 114.6686\n",
      "Epoch [26/300], Step [40/384], Loss: 146.6223\n",
      "Epoch [26/300], Step [60/384], Loss: 76.2225\n",
      "Epoch [26/300], Step [80/384], Loss: 210.9285\n",
      "Epoch [26/300], Step [100/384], Loss: 29.1208\n",
      "Epoch [26/300], Step [120/384], Loss: 121.7205\n",
      "Epoch [26/300], Step [140/384], Loss: 184.2811\n",
      "Epoch [26/300], Step [160/384], Loss: 15.7362\n",
      "Epoch [26/300], Step [180/384], Loss: 90.6074\n",
      "Epoch [26/300], Step [200/384], Loss: 99.6091\n",
      "Epoch [26/300], Step [220/384], Loss: 86.2639\n",
      "Epoch [26/300], Step [240/384], Loss: 174.0945\n",
      "Epoch [26/300], Step [260/384], Loss: 151.3852\n",
      "Epoch [26/300], Step [280/384], Loss: 119.0429\n",
      "Epoch [26/300], Step [300/384], Loss: 209.2583\n",
      "Epoch [26/300], Step [320/384], Loss: 83.4108\n",
      "Epoch [26/300], Step [340/384], Loss: 44.2871\n",
      "Epoch [26/300], Step [360/384], Loss: 82.5447\n",
      "Epoch [26/300], Step [380/384], Loss: 234.1992\n",
      "Epoch [27/300], Step [20/384], Loss: 108.9398\n",
      "Epoch [27/300], Step [40/384], Loss: 17.3118\n",
      "Epoch [27/300], Step [60/384], Loss: 104.2850\n",
      "Epoch [27/300], Step [80/384], Loss: 35.7234\n",
      "Epoch [27/300], Step [100/384], Loss: 35.4985\n",
      "Epoch [27/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [27/300], Step [140/384], Loss: 154.4293\n",
      "Epoch [27/300], Step [160/384], Loss: 81.7106\n",
      "Epoch [27/300], Step [180/384], Loss: 163.7350\n",
      "Epoch [27/300], Step [200/384], Loss: 61.6390\n",
      "Epoch [27/300], Step [220/384], Loss: 91.4536\n",
      "Epoch [27/300], Step [240/384], Loss: 98.1848\n",
      "Epoch [27/300], Step [260/384], Loss: 15.9609\n",
      "Epoch [27/300], Step [280/384], Loss: 230.5779\n",
      "Epoch [27/300], Step [300/384], Loss: 0.0001\n",
      "Epoch [27/300], Step [320/384], Loss: 86.4329\n",
      "Epoch [27/300], Step [340/384], Loss: 83.6172\n",
      "Epoch [27/300], Step [360/384], Loss: 94.5831\n",
      "Epoch [27/300], Step [380/384], Loss: 96.3718\n",
      "Epoch [28/300], Step [20/384], Loss: 151.8625\n",
      "Epoch [28/300], Step [40/384], Loss: 227.7462\n",
      "Epoch [28/300], Step [60/384], Loss: 232.9529\n",
      "Epoch [28/300], Step [80/384], Loss: 24.3610\n",
      "Epoch [28/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [28/300], Step [120/384], Loss: 109.9825\n",
      "Epoch [28/300], Step [140/384], Loss: 344.7684\n",
      "Epoch [28/300], Step [160/384], Loss: 33.4232\n",
      "Epoch [28/300], Step [180/384], Loss: 111.6733\n",
      "Epoch [28/300], Step [200/384], Loss: 94.7195\n",
      "Epoch [28/300], Step [220/384], Loss: 172.9983\n",
      "Epoch [28/300], Step [240/384], Loss: 113.3232\n",
      "Epoch [28/300], Step [260/384], Loss: 469.4404\n",
      "Epoch [28/300], Step [280/384], Loss: 184.3857\n",
      "Epoch [28/300], Step [300/384], Loss: 5.7405\n",
      "Epoch [28/300], Step [320/384], Loss: 131.8803\n",
      "Epoch [28/300], Step [340/384], Loss: 179.2761\n",
      "Epoch [28/300], Step [360/384], Loss: 28.5501\n",
      "Epoch [28/300], Step [380/384], Loss: 197.9741\n",
      "Epoch [29/300], Step [20/384], Loss: 119.5774\n",
      "Epoch [29/300], Step [40/384], Loss: 95.1705\n",
      "Epoch [29/300], Step [60/384], Loss: 33.9871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/300], Step [80/384], Loss: 257.8250\n",
      "Epoch [29/300], Step [100/384], Loss: 161.9098\n",
      "Epoch [29/300], Step [120/384], Loss: 12.4032\n",
      "Epoch [29/300], Step [140/384], Loss: 231.7072\n",
      "Epoch [29/300], Step [160/384], Loss: 6.7877\n",
      "Epoch [29/300], Step [180/384], Loss: 320.6642\n",
      "Epoch [29/300], Step [200/384], Loss: 67.3881\n",
      "Epoch [29/300], Step [220/384], Loss: 67.6325\n",
      "Epoch [29/300], Step [240/384], Loss: 196.2690\n",
      "Epoch [29/300], Step [260/384], Loss: 144.8867\n",
      "Epoch [29/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [29/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [29/300], Step [320/384], Loss: 106.7221\n",
      "Epoch [29/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [29/300], Step [360/384], Loss: 143.5776\n",
      "Epoch [29/300], Step [380/384], Loss: 307.0196\n",
      "Epoch [30/300], Step [20/384], Loss: 18.9656\n",
      "Epoch [30/300], Step [40/384], Loss: 14.8899\n",
      "Epoch [30/300], Step [60/384], Loss: 88.0900\n",
      "Epoch [30/300], Step [80/384], Loss: 65.4928\n",
      "Epoch [30/300], Step [100/384], Loss: 91.9470\n",
      "Epoch [30/300], Step [120/384], Loss: 69.9266\n",
      "Epoch [30/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [30/300], Step [160/384], Loss: 84.5350\n",
      "Epoch [30/300], Step [180/384], Loss: 145.1059\n",
      "Epoch [30/300], Step [200/384], Loss: 130.3328\n",
      "Epoch [30/300], Step [220/384], Loss: 141.2254\n",
      "Epoch [30/300], Step [240/384], Loss: 155.2219\n",
      "Epoch [30/300], Step [260/384], Loss: 116.8571\n",
      "Epoch [30/300], Step [280/384], Loss: 53.3687\n",
      "Epoch [30/300], Step [300/384], Loss: 25.4981\n",
      "Epoch [30/300], Step [320/384], Loss: 102.6812\n",
      "Epoch [30/300], Step [340/384], Loss: 104.4010\n",
      "Epoch [30/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [30/300], Step [380/384], Loss: 317.2103\n",
      "Epoch [31/300], Step [20/384], Loss: 72.9750\n",
      "Epoch [31/300], Step [40/384], Loss: 150.3014\n",
      "Epoch [31/300], Step [60/384], Loss: 140.0467\n",
      "Epoch [31/300], Step [80/384], Loss: 109.7776\n",
      "Epoch [31/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [31/300], Step [120/384], Loss: 15.8643\n",
      "Epoch [31/300], Step [140/384], Loss: 0.1355\n",
      "Epoch [31/300], Step [160/384], Loss: 102.7873\n",
      "Epoch [31/300], Step [180/384], Loss: 56.4608\n",
      "Epoch [31/300], Step [200/384], Loss: 208.8964\n",
      "Epoch [31/300], Step [220/384], Loss: 267.1677\n",
      "Epoch [31/300], Step [240/384], Loss: 66.0787\n",
      "Epoch [31/300], Step [260/384], Loss: 196.8488\n",
      "Epoch [31/300], Step [280/384], Loss: 17.2387\n",
      "Epoch [31/300], Step [300/384], Loss: 184.6409\n",
      "Epoch [31/300], Step [320/384], Loss: 61.9558\n",
      "Epoch [31/300], Step [340/384], Loss: 156.0003\n",
      "Epoch [31/300], Step [360/384], Loss: 71.6517\n",
      "Epoch [31/300], Step [380/384], Loss: 190.8963\n",
      "Epoch [32/300], Step [20/384], Loss: 88.7198\n",
      "Epoch [32/300], Step [40/384], Loss: 43.5977\n",
      "Epoch [32/300], Step [60/384], Loss: 60.5862\n",
      "Epoch [32/300], Step [80/384], Loss: 4.7817\n",
      "Epoch [32/300], Step [100/384], Loss: 122.0228\n",
      "Epoch [32/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [32/300], Step [140/384], Loss: 31.6111\n",
      "Epoch [32/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [32/300], Step [180/384], Loss: 306.2880\n",
      "Epoch [32/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [32/300], Step [220/384], Loss: 79.5067\n",
      "Epoch [32/300], Step [240/384], Loss: 31.9818\n",
      "Epoch [32/300], Step [260/384], Loss: 179.4103\n",
      "Epoch [32/300], Step [280/384], Loss: 40.4783\n",
      "Epoch [32/300], Step [300/384], Loss: 161.4251\n",
      "Epoch [32/300], Step [320/384], Loss: 5.0346\n",
      "Epoch [32/300], Step [340/384], Loss: 241.0260\n",
      "Epoch [32/300], Step [360/384], Loss: 187.7399\n",
      "Epoch [32/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [33/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [33/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [33/300], Step [60/384], Loss: 8.5562\n",
      "Epoch [33/300], Step [80/384], Loss: 105.8519\n",
      "Epoch [33/300], Step [100/384], Loss: 257.9515\n",
      "Epoch [33/300], Step [120/384], Loss: 6.8573\n",
      "Epoch [33/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [33/300], Step [160/384], Loss: 111.2326\n",
      "Epoch [33/300], Step [180/384], Loss: 186.1254\n",
      "Epoch [33/300], Step [200/384], Loss: 32.0614\n",
      "Epoch [33/300], Step [220/384], Loss: 133.5048\n",
      "Epoch [33/300], Step [240/384], Loss: 113.6908\n",
      "Epoch [33/300], Step [260/384], Loss: 94.3877\n",
      "Epoch [33/300], Step [280/384], Loss: 117.3197\n",
      "Epoch [33/300], Step [300/384], Loss: 105.2714\n",
      "Epoch [33/300], Step [320/384], Loss: 113.1846\n",
      "Epoch [33/300], Step [340/384], Loss: 225.6490\n",
      "Epoch [33/300], Step [360/384], Loss: 42.5283\n",
      "Epoch [33/300], Step [380/384], Loss: 147.1091\n",
      "Epoch [34/300], Step [20/384], Loss: 3.3497\n",
      "Epoch [34/300], Step [40/384], Loss: 10.5878\n",
      "Epoch [34/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [34/300], Step [80/384], Loss: 41.0120\n",
      "Epoch [34/300], Step [100/384], Loss: 69.6530\n",
      "Epoch [34/300], Step [120/384], Loss: 89.6641\n",
      "Epoch [34/300], Step [140/384], Loss: 70.1198\n",
      "Epoch [34/300], Step [160/384], Loss: 38.0585\n",
      "Epoch [34/300], Step [180/384], Loss: 275.2012\n",
      "Epoch [34/300], Step [200/384], Loss: 297.0486\n",
      "Epoch [34/300], Step [220/384], Loss: 25.6077\n",
      "Epoch [34/300], Step [240/384], Loss: 60.7206\n",
      "Epoch [34/300], Step [260/384], Loss: 220.7108\n",
      "Epoch [34/300], Step [280/384], Loss: 73.0816\n",
      "Epoch [34/300], Step [300/384], Loss: 147.9256\n",
      "Epoch [34/300], Step [320/384], Loss: 102.3729\n",
      "Epoch [34/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [34/300], Step [360/384], Loss: 117.9923\n",
      "Epoch [34/300], Step [380/384], Loss: 487.5767\n",
      "Epoch [35/300], Step [20/384], Loss: 138.6015\n",
      "Epoch [35/300], Step [40/384], Loss: 260.3244\n",
      "Epoch [35/300], Step [60/384], Loss: 270.0410\n",
      "Epoch [35/300], Step [80/384], Loss: 159.8192\n",
      "Epoch [35/300], Step [100/384], Loss: 400.3156\n",
      "Epoch [35/300], Step [120/384], Loss: 124.7187\n",
      "Epoch [35/300], Step [140/384], Loss: 29.3846\n",
      "Epoch [35/300], Step [160/384], Loss: 326.3336\n",
      "Epoch [35/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [35/300], Step [200/384], Loss: 187.7767\n",
      "Epoch [35/300], Step [220/384], Loss: 60.1612\n",
      "Epoch [35/300], Step [240/384], Loss: 128.3257\n",
      "Epoch [35/300], Step [260/384], Loss: 127.3537\n",
      "Epoch [35/300], Step [280/384], Loss: 36.7064\n",
      "Epoch [35/300], Step [300/384], Loss: 142.3373\n",
      "Epoch [35/300], Step [320/384], Loss: 15.4772\n",
      "Epoch [35/300], Step [340/384], Loss: 27.2524\n",
      "Epoch [35/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [35/300], Step [380/384], Loss: 202.8041\n",
      "Epoch [36/300], Step [20/384], Loss: 180.4080\n",
      "Epoch [36/300], Step [40/384], Loss: 26.1095\n",
      "Epoch [36/300], Step [60/384], Loss: 5.3419\n",
      "Epoch [36/300], Step [80/384], Loss: 54.3517\n",
      "Epoch [36/300], Step [100/384], Loss: 9.3334\n",
      "Epoch [36/300], Step [120/384], Loss: 151.1285\n",
      "Epoch [36/300], Step [140/384], Loss: 101.2521\n",
      "Epoch [36/300], Step [160/384], Loss: 20.4565\n",
      "Epoch [36/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [36/300], Step [200/384], Loss: 72.1639\n",
      "Epoch [36/300], Step [220/384], Loss: 133.2766\n",
      "Epoch [36/300], Step [240/384], Loss: 202.9572\n",
      "Epoch [36/300], Step [260/384], Loss: 213.7754\n",
      "Epoch [36/300], Step [280/384], Loss: 11.9865\n",
      "Epoch [36/300], Step [300/384], Loss: 19.0259\n",
      "Epoch [36/300], Step [320/384], Loss: 219.0126\n",
      "Epoch [36/300], Step [340/384], Loss: 92.1379\n",
      "Epoch [36/300], Step [360/384], Loss: 195.6929\n",
      "Epoch [36/300], Step [380/384], Loss: 295.5316\n",
      "Epoch [37/300], Step [20/384], Loss: 163.1707\n",
      "Epoch [37/300], Step [40/384], Loss: 86.1668\n",
      "Epoch [37/300], Step [60/384], Loss: 0.0002\n",
      "Epoch [37/300], Step [80/384], Loss: 99.5007\n",
      "Epoch [37/300], Step [100/384], Loss: 66.7284\n",
      "Epoch [37/300], Step [120/384], Loss: 101.6274\n",
      "Epoch [37/300], Step [140/384], Loss: 6.0114\n",
      "Epoch [37/300], Step [160/384], Loss: 318.1346\n",
      "Epoch [37/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [37/300], Step [200/384], Loss: 46.3632\n",
      "Epoch [37/300], Step [220/384], Loss: 44.9009\n",
      "Epoch [37/300], Step [240/384], Loss: 115.2408\n",
      "Epoch [37/300], Step [260/384], Loss: 20.4895\n",
      "Epoch [37/300], Step [280/384], Loss: 57.0389\n",
      "Epoch [37/300], Step [300/384], Loss: 144.8847\n",
      "Epoch [37/300], Step [320/384], Loss: 275.6130\n",
      "Epoch [37/300], Step [340/384], Loss: 50.1443\n",
      "Epoch [37/300], Step [360/384], Loss: 8.6799\n",
      "Epoch [37/300], Step [380/384], Loss: 267.7484\n",
      "Epoch [38/300], Step [20/384], Loss: 91.1939\n",
      "Epoch [38/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [38/300], Step [60/384], Loss: 88.0874\n",
      "Epoch [38/300], Step [80/384], Loss: 293.6462\n",
      "Epoch [38/300], Step [100/384], Loss: 1.6872\n",
      "Epoch [38/300], Step [120/384], Loss: 158.2612\n",
      "Epoch [38/300], Step [140/384], Loss: 304.5048\n",
      "Epoch [38/300], Step [160/384], Loss: 329.3831\n",
      "Epoch [38/300], Step [180/384], Loss: 6.6390\n",
      "Epoch [38/300], Step [200/384], Loss: 44.8620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/300], Step [220/384], Loss: 125.0580\n",
      "Epoch [38/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [38/300], Step [260/384], Loss: 102.9547\n",
      "Epoch [38/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [38/300], Step [300/384], Loss: 11.6156\n",
      "Epoch [38/300], Step [320/384], Loss: 107.9752\n",
      "Epoch [38/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [38/300], Step [360/384], Loss: 628.1061\n",
      "Epoch [38/300], Step [380/384], Loss: 0.0001\n",
      "Epoch [39/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [39/300], Step [40/384], Loss: 98.0942\n",
      "Epoch [39/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [39/300], Step [80/384], Loss: 362.2211\n",
      "Epoch [39/300], Step [100/384], Loss: 109.0098\n",
      "Epoch [39/300], Step [120/384], Loss: 24.3264\n",
      "Epoch [39/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [39/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [39/300], Step [180/384], Loss: 58.9440\n",
      "Epoch [39/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [39/300], Step [220/384], Loss: 8.4491\n",
      "Epoch [39/300], Step [240/384], Loss: 161.5174\n",
      "Epoch [39/300], Step [260/384], Loss: 166.7390\n",
      "Epoch [39/300], Step [280/384], Loss: 446.8716\n",
      "Epoch [39/300], Step [300/384], Loss: 181.9461\n",
      "Epoch [39/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [39/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [39/300], Step [360/384], Loss: 5.7845\n",
      "Epoch [39/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [40/300], Step [20/384], Loss: 307.6023\n",
      "Epoch [40/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [40/300], Step [60/384], Loss: 212.0967\n",
      "Epoch [40/300], Step [80/384], Loss: 115.1640\n",
      "Epoch [40/300], Step [100/384], Loss: 32.2064\n",
      "Epoch [40/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [40/300], Step [140/384], Loss: 77.8825\n",
      "Epoch [40/300], Step [160/384], Loss: 160.0981\n",
      "Epoch [40/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [40/300], Step [200/384], Loss: 138.9784\n",
      "Epoch [40/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [40/300], Step [240/384], Loss: 10.9485\n",
      "Epoch [40/300], Step [260/384], Loss: 0.0109\n",
      "Epoch [40/300], Step [280/384], Loss: 129.6023\n",
      "Epoch [40/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [40/300], Step [320/384], Loss: 161.3838\n",
      "Epoch [40/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [40/300], Step [360/384], Loss: 131.6275\n",
      "Epoch [40/300], Step [380/384], Loss: 147.7845\n",
      "Epoch [41/300], Step [20/384], Loss: 121.6972\n",
      "Epoch [41/300], Step [40/384], Loss: 258.6354\n",
      "Epoch [41/300], Step [60/384], Loss: 207.0736\n",
      "Epoch [41/300], Step [80/384], Loss: 315.5312\n",
      "Epoch [41/300], Step [100/384], Loss: 62.7190\n",
      "Epoch [41/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [41/300], Step [140/384], Loss: 76.9172\n",
      "Epoch [41/300], Step [160/384], Loss: 110.7868\n",
      "Epoch [41/300], Step [180/384], Loss: 53.8324\n",
      "Epoch [41/300], Step [200/384], Loss: 72.4736\n",
      "Epoch [41/300], Step [220/384], Loss: 87.0483\n",
      "Epoch [41/300], Step [240/384], Loss: 55.1106\n",
      "Epoch [41/300], Step [260/384], Loss: 261.5837\n",
      "Epoch [41/300], Step [280/384], Loss: 110.3416\n",
      "Epoch [41/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [41/300], Step [320/384], Loss: 70.0456\n",
      "Epoch [41/300], Step [340/384], Loss: 3.3458\n",
      "Epoch [41/300], Step [360/384], Loss: 42.2118\n",
      "Epoch [41/300], Step [380/384], Loss: 7.2172\n",
      "Epoch [42/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [42/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [42/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [42/300], Step [80/384], Loss: 62.9235\n",
      "Epoch [42/300], Step [100/384], Loss: 103.3883\n",
      "Epoch [42/300], Step [120/384], Loss: 181.9811\n",
      "Epoch [42/300], Step [140/384], Loss: 227.5811\n",
      "Epoch [42/300], Step [160/384], Loss: 72.1510\n",
      "Epoch [42/300], Step [180/384], Loss: 287.2948\n",
      "Epoch [42/300], Step [200/384], Loss: 14.6529\n",
      "Epoch [42/300], Step [220/384], Loss: 57.0674\n",
      "Epoch [42/300], Step [240/384], Loss: 308.4879\n",
      "Epoch [42/300], Step [260/384], Loss: 41.6243\n",
      "Epoch [42/300], Step [280/384], Loss: 5.5431\n",
      "Epoch [42/300], Step [300/384], Loss: 7.9970\n",
      "Epoch [42/300], Step [320/384], Loss: 325.1140\n",
      "Epoch [42/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [42/300], Step [360/384], Loss: 167.2375\n",
      "Epoch [42/300], Step [380/384], Loss: 157.4824\n",
      "Epoch [43/300], Step [20/384], Loss: 95.9203\n",
      "Epoch [43/300], Step [40/384], Loss: 49.0786\n",
      "Epoch [43/300], Step [60/384], Loss: 55.2254\n",
      "Epoch [43/300], Step [80/384], Loss: 73.8141\n",
      "Epoch [43/300], Step [100/384], Loss: 93.8285\n",
      "Epoch [43/300], Step [120/384], Loss: 40.3857\n",
      "Epoch [43/300], Step [140/384], Loss: 0.0006\n",
      "Epoch [43/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [43/300], Step [180/384], Loss: 79.9590\n",
      "Epoch [43/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [43/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [43/300], Step [240/384], Loss: 53.2976\n",
      "Epoch [43/300], Step [260/384], Loss: 75.7803\n",
      "Epoch [43/300], Step [280/384], Loss: 33.5086\n",
      "Epoch [43/300], Step [300/384], Loss: 148.1070\n",
      "Epoch [43/300], Step [320/384], Loss: 50.5333\n",
      "Epoch [43/300], Step [340/384], Loss: 117.2775\n",
      "Epoch [43/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [43/300], Step [380/384], Loss: 74.7501\n",
      "Epoch [44/300], Step [20/384], Loss: 30.0282\n",
      "Epoch [44/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [44/300], Step [60/384], Loss: 65.3431\n",
      "Epoch [44/300], Step [80/384], Loss: 380.3859\n",
      "Epoch [44/300], Step [100/384], Loss: 149.8314\n",
      "Epoch [44/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [44/300], Step [140/384], Loss: 90.0505\n",
      "Epoch [44/300], Step [160/384], Loss: 159.4499\n",
      "Epoch [44/300], Step [180/384], Loss: 5.5686\n",
      "Epoch [44/300], Step [200/384], Loss: 25.9576\n",
      "Epoch [44/300], Step [220/384], Loss: 91.2832\n",
      "Epoch [44/300], Step [240/384], Loss: 28.8238\n",
      "Epoch [44/300], Step [260/384], Loss: 23.8051\n",
      "Epoch [44/300], Step [280/384], Loss: 35.0746\n",
      "Epoch [44/300], Step [300/384], Loss: 27.6379\n",
      "Epoch [44/300], Step [320/384], Loss: 91.7633\n",
      "Epoch [44/300], Step [340/384], Loss: 115.0102\n",
      "Epoch [44/300], Step [360/384], Loss: 153.4454\n",
      "Epoch [44/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [45/300], Step [20/384], Loss: 132.5942\n",
      "Epoch [45/300], Step [40/384], Loss: 30.5013\n",
      "Epoch [45/300], Step [60/384], Loss: 59.3860\n",
      "Epoch [45/300], Step [80/384], Loss: 153.9508\n",
      "Epoch [45/300], Step [100/384], Loss: 124.4641\n",
      "Epoch [45/300], Step [120/384], Loss: 471.2003\n",
      "Epoch [45/300], Step [140/384], Loss: 154.7044\n",
      "Epoch [45/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [45/300], Step [180/384], Loss: 78.9253\n",
      "Epoch [45/300], Step [200/384], Loss: 40.8703\n",
      "Epoch [45/300], Step [220/384], Loss: 186.0222\n",
      "Epoch [45/300], Step [240/384], Loss: 124.6547\n",
      "Epoch [45/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [45/300], Step [280/384], Loss: 86.3708\n",
      "Epoch [45/300], Step [300/384], Loss: 315.8322\n",
      "Epoch [45/300], Step [320/384], Loss: 64.0735\n",
      "Epoch [45/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [45/300], Step [360/384], Loss: 219.9309\n",
      "Epoch [45/300], Step [380/384], Loss: 101.9434\n",
      "Epoch [46/300], Step [20/384], Loss: 429.8867\n",
      "Epoch [46/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [46/300], Step [60/384], Loss: 148.3350\n",
      "Epoch [46/300], Step [80/384], Loss: 141.2552\n",
      "Epoch [46/300], Step [100/384], Loss: 87.6016\n",
      "Epoch [46/300], Step [120/384], Loss: 138.7081\n",
      "Epoch [46/300], Step [140/384], Loss: 64.3568\n",
      "Epoch [46/300], Step [160/384], Loss: 228.2247\n",
      "Epoch [46/300], Step [180/384], Loss: 110.4064\n",
      "Epoch [46/300], Step [200/384], Loss: 316.6724\n",
      "Epoch [46/300], Step [220/384], Loss: 109.1551\n",
      "Epoch [46/300], Step [240/384], Loss: 53.4269\n",
      "Epoch [46/300], Step [260/384], Loss: 142.9789\n",
      "Epoch [46/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [46/300], Step [300/384], Loss: 78.8186\n",
      "Epoch [46/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [46/300], Step [340/384], Loss: 315.3385\n",
      "Epoch [46/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [46/300], Step [380/384], Loss: 76.4834\n",
      "Epoch [47/300], Step [20/384], Loss: 321.2791\n",
      "Epoch [47/300], Step [40/384], Loss: 61.1465\n",
      "Epoch [47/300], Step [60/384], Loss: 152.0431\n",
      "Epoch [47/300], Step [80/384], Loss: 0.0825\n",
      "Epoch [47/300], Step [100/384], Loss: 169.8571\n",
      "Epoch [47/300], Step [120/384], Loss: 16.5633\n",
      "Epoch [47/300], Step [140/384], Loss: 133.3089\n",
      "Epoch [47/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [47/300], Step [180/384], Loss: 148.5605\n",
      "Epoch [47/300], Step [200/384], Loss: 20.5748\n",
      "Epoch [47/300], Step [220/384], Loss: 118.8211\n",
      "Epoch [47/300], Step [240/384], Loss: 144.9861\n",
      "Epoch [47/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [47/300], Step [280/384], Loss: 147.8470\n",
      "Epoch [47/300], Step [300/384], Loss: 41.8039\n",
      "Epoch [47/300], Step [320/384], Loss: 129.3103\n",
      "Epoch [47/300], Step [340/384], Loss: 145.5976\n",
      "Epoch [47/300], Step [360/384], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/300], Step [380/384], Loss: 55.6347\n",
      "Epoch [48/300], Step [20/384], Loss: 30.1338\n",
      "Epoch [48/300], Step [40/384], Loss: 112.0056\n",
      "Epoch [48/300], Step [60/384], Loss: 22.2891\n",
      "Epoch [48/300], Step [80/384], Loss: 34.7957\n",
      "Epoch [48/300], Step [100/384], Loss: 37.1973\n",
      "Epoch [48/300], Step [120/384], Loss: 50.7145\n",
      "Epoch [48/300], Step [140/384], Loss: 75.5331\n",
      "Epoch [48/300], Step [160/384], Loss: 29.3721\n",
      "Epoch [48/300], Step [180/384], Loss: 104.0832\n",
      "Epoch [48/300], Step [200/384], Loss: 450.2816\n",
      "Epoch [48/300], Step [220/384], Loss: 95.4420\n",
      "Epoch [48/300], Step [240/384], Loss: 40.4856\n",
      "Epoch [48/300], Step [260/384], Loss: 124.1520\n",
      "Epoch [48/300], Step [280/384], Loss: 246.4839\n",
      "Epoch [48/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [48/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [48/300], Step [340/384], Loss: 96.2543\n",
      "Epoch [48/300], Step [360/384], Loss: 36.3025\n",
      "Epoch [48/300], Step [380/384], Loss: 63.7787\n",
      "Epoch [49/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [49/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [49/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [49/300], Step [80/384], Loss: 78.8578\n",
      "Epoch [49/300], Step [100/384], Loss: 37.3237\n",
      "Epoch [49/300], Step [120/384], Loss: 32.2312\n",
      "Epoch [49/300], Step [140/384], Loss: 6.4895\n",
      "Epoch [49/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [49/300], Step [180/384], Loss: 2.3127\n",
      "Epoch [49/300], Step [200/384], Loss: 15.8454\n",
      "Epoch [49/300], Step [220/384], Loss: 154.6017\n",
      "Epoch [49/300], Step [240/384], Loss: 3.0957\n",
      "Epoch [49/300], Step [260/384], Loss: 300.2104\n",
      "Epoch [49/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [49/300], Step [300/384], Loss: 109.7878\n",
      "Epoch [49/300], Step [320/384], Loss: 45.3584\n",
      "Epoch [49/300], Step [340/384], Loss: 18.7804\n",
      "Epoch [49/300], Step [360/384], Loss: 24.9458\n",
      "Epoch [49/300], Step [380/384], Loss: 41.1900\n",
      "Epoch [50/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [50/300], Step [40/384], Loss: 82.7377\n",
      "Epoch [50/300], Step [60/384], Loss: 257.8342\n",
      "Epoch [50/300], Step [80/384], Loss: 40.5056\n",
      "Epoch [50/300], Step [100/384], Loss: 84.2535\n",
      "Epoch [50/300], Step [120/384], Loss: 117.9198\n",
      "Epoch [50/300], Step [140/384], Loss: 138.1550\n",
      "Epoch [50/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [50/300], Step [180/384], Loss: 81.2297\n",
      "Epoch [50/300], Step [200/384], Loss: 55.2515\n",
      "Epoch [50/300], Step [220/384], Loss: 65.4224\n",
      "Epoch [50/300], Step [240/384], Loss: 96.3130\n",
      "Epoch [50/300], Step [260/384], Loss: 146.7942\n",
      "Epoch [50/300], Step [280/384], Loss: 22.4308\n",
      "Epoch [50/300], Step [300/384], Loss: 198.9269\n",
      "Epoch [50/300], Step [320/384], Loss: 36.4783\n",
      "Epoch [50/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [50/300], Step [360/384], Loss: 1.7592\n",
      "Epoch [50/300], Step [380/384], Loss: 75.3250\n",
      "Epoch [51/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [51/300], Step [40/384], Loss: 84.1631\n",
      "Epoch [51/300], Step [60/384], Loss: 197.6818\n",
      "Epoch [51/300], Step [80/384], Loss: 58.0499\n",
      "Epoch [51/300], Step [100/384], Loss: 70.8186\n",
      "Epoch [51/300], Step [120/384], Loss: 161.1505\n",
      "Epoch [51/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [51/300], Step [160/384], Loss: 73.0955\n",
      "Epoch [51/300], Step [180/384], Loss: 100.5811\n",
      "Epoch [51/300], Step [200/384], Loss: 183.3816\n",
      "Epoch [51/300], Step [220/384], Loss: 453.9019\n",
      "Epoch [51/300], Step [240/384], Loss: 36.9826\n",
      "Epoch [51/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [51/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [51/300], Step [300/384], Loss: 203.7511\n",
      "Epoch [51/300], Step [320/384], Loss: 9.5759\n",
      "Epoch [51/300], Step [340/384], Loss: 207.8572\n",
      "Epoch [51/300], Step [360/384], Loss: 49.3606\n",
      "Epoch [51/300], Step [380/384], Loss: 0.0016\n",
      "Epoch [52/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [52/300], Step [40/384], Loss: 2.2832\n",
      "Epoch [52/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [52/300], Step [80/384], Loss: 153.9470\n",
      "Epoch [52/300], Step [100/384], Loss: 92.8232\n",
      "Epoch [52/300], Step [120/384], Loss: 270.2037\n",
      "Epoch [52/300], Step [140/384], Loss: 0.0001\n",
      "Epoch [52/300], Step [160/384], Loss: 166.1202\n",
      "Epoch [52/300], Step [180/384], Loss: 43.9027\n",
      "Epoch [52/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [52/300], Step [220/384], Loss: 104.9897\n",
      "Epoch [52/300], Step [240/384], Loss: 22.1745\n",
      "Epoch [52/300], Step [260/384], Loss: 185.8336\n",
      "Epoch [52/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [52/300], Step [300/384], Loss: 166.3181\n",
      "Epoch [52/300], Step [320/384], Loss: 129.0204\n",
      "Epoch [52/300], Step [340/384], Loss: 0.0686\n",
      "Epoch [52/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [52/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [53/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [53/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [53/300], Step [60/384], Loss: 334.9306\n",
      "Epoch [53/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [53/300], Step [100/384], Loss: 102.6333\n",
      "Epoch [53/300], Step [120/384], Loss: 106.9947\n",
      "Epoch [53/300], Step [140/384], Loss: 140.7551\n",
      "Epoch [53/300], Step [160/384], Loss: 38.4290\n",
      "Epoch [53/300], Step [180/384], Loss: 92.2988\n",
      "Epoch [53/300], Step [200/384], Loss: 424.5173\n",
      "Epoch [53/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [53/300], Step [240/384], Loss: 42.4011\n",
      "Epoch [53/300], Step [260/384], Loss: 8.6573\n",
      "Epoch [53/300], Step [280/384], Loss: 99.6300\n",
      "Epoch [53/300], Step [300/384], Loss: 101.5575\n",
      "Epoch [53/300], Step [320/384], Loss: 16.0357\n",
      "Epoch [53/300], Step [340/384], Loss: 49.7614\n",
      "Epoch [53/300], Step [360/384], Loss: 179.6669\n",
      "Epoch [53/300], Step [380/384], Loss: 229.6108\n",
      "Epoch [54/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [54/300], Step [40/384], Loss: 1.5458\n",
      "Epoch [54/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [54/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [54/300], Step [100/384], Loss: 123.0071\n",
      "Epoch [54/300], Step [120/384], Loss: 21.3090\n",
      "Epoch [54/300], Step [140/384], Loss: 75.2058\n",
      "Epoch [54/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [54/300], Step [180/384], Loss: 95.0709\n",
      "Epoch [54/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [54/300], Step [220/384], Loss: 84.0615\n",
      "Epoch [54/300], Step [240/384], Loss: 138.8819\n",
      "Epoch [54/300], Step [260/384], Loss: 94.2608\n",
      "Epoch [54/300], Step [280/384], Loss: 155.3508\n",
      "Epoch [54/300], Step [300/384], Loss: 53.2630\n",
      "Epoch [54/300], Step [320/384], Loss: 84.1573\n",
      "Epoch [54/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [54/300], Step [360/384], Loss: 199.1540\n",
      "Epoch [54/300], Step [380/384], Loss: 75.0099\n",
      "Epoch [55/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [55/300], Step [40/384], Loss: 38.7155\n",
      "Epoch [55/300], Step [60/384], Loss: 101.9861\n",
      "Epoch [55/300], Step [80/384], Loss: 28.6357\n",
      "Epoch [55/300], Step [100/384], Loss: 48.7091\n",
      "Epoch [55/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [55/300], Step [140/384], Loss: 218.0234\n",
      "Epoch [55/300], Step [160/384], Loss: 200.8034\n",
      "Epoch [55/300], Step [180/384], Loss: 23.9385\n",
      "Epoch [55/300], Step [200/384], Loss: 193.5120\n",
      "Epoch [55/300], Step [220/384], Loss: 50.4818\n",
      "Epoch [55/300], Step [240/384], Loss: 21.5660\n",
      "Epoch [55/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [55/300], Step [280/384], Loss: 27.8417\n",
      "Epoch [55/300], Step [300/384], Loss: 54.0682\n",
      "Epoch [55/300], Step [320/384], Loss: 68.5840\n",
      "Epoch [55/300], Step [340/384], Loss: 35.9891\n",
      "Epoch [55/300], Step [360/384], Loss: 7.0874\n",
      "Epoch [55/300], Step [380/384], Loss: 57.8763\n",
      "Epoch [56/300], Step [20/384], Loss: 191.7871\n",
      "Epoch [56/300], Step [40/384], Loss: 28.3759\n",
      "Epoch [56/300], Step [60/384], Loss: 47.3752\n",
      "Epoch [56/300], Step [80/384], Loss: 77.3268\n",
      "Epoch [56/300], Step [100/384], Loss: 42.7876\n",
      "Epoch [56/300], Step [120/384], Loss: 80.5761\n",
      "Epoch [56/300], Step [140/384], Loss: 12.4520\n",
      "Epoch [56/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [56/300], Step [180/384], Loss: 84.8118\n",
      "Epoch [56/300], Step [200/384], Loss: 29.6915\n",
      "Epoch [56/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [56/300], Step [240/384], Loss: 103.0034\n",
      "Epoch [56/300], Step [260/384], Loss: 54.2277\n",
      "Epoch [56/300], Step [280/384], Loss: 49.2766\n",
      "Epoch [56/300], Step [300/384], Loss: 72.2779\n",
      "Epoch [56/300], Step [320/384], Loss: 40.7813\n",
      "Epoch [56/300], Step [340/384], Loss: 82.2994\n",
      "Epoch [56/300], Step [360/384], Loss: 65.6094\n",
      "Epoch [56/300], Step [380/384], Loss: 251.0393\n",
      "Epoch [57/300], Step [20/384], Loss: 2.1984\n",
      "Epoch [57/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [57/300], Step [60/384], Loss: 100.1609\n",
      "Epoch [57/300], Step [80/384], Loss: 72.8382\n",
      "Epoch [57/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [57/300], Step [120/384], Loss: 156.1157\n",
      "Epoch [57/300], Step [140/384], Loss: 12.0667\n",
      "Epoch [57/300], Step [160/384], Loss: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/300], Step [180/384], Loss: 102.4313\n",
      "Epoch [57/300], Step [200/384], Loss: 57.7094\n",
      "Epoch [57/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [57/300], Step [240/384], Loss: 6.2863\n",
      "Epoch [57/300], Step [260/384], Loss: 34.8714\n",
      "Epoch [57/300], Step [280/384], Loss: 77.2534\n",
      "Epoch [57/300], Step [300/384], Loss: 50.1218\n",
      "Epoch [57/300], Step [320/384], Loss: 82.4629\n",
      "Epoch [57/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [57/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [57/300], Step [380/384], Loss: 186.9247\n",
      "Epoch [58/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [58/300], Step [40/384], Loss: 48.5505\n",
      "Epoch [58/300], Step [60/384], Loss: 44.4882\n",
      "Epoch [58/300], Step [80/384], Loss: 11.7859\n",
      "Epoch [58/300], Step [100/384], Loss: 232.9289\n",
      "Epoch [58/300], Step [120/384], Loss: 83.7987\n",
      "Epoch [58/300], Step [140/384], Loss: 246.0247\n",
      "Epoch [58/300], Step [160/384], Loss: 8.1886\n",
      "Epoch [58/300], Step [180/384], Loss: 57.5824\n",
      "Epoch [58/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [58/300], Step [220/384], Loss: 189.4086\n",
      "Epoch [58/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [58/300], Step [260/384], Loss: 218.1467\n",
      "Epoch [58/300], Step [280/384], Loss: 283.4836\n",
      "Epoch [58/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [58/300], Step [320/384], Loss: 17.9073\n",
      "Epoch [58/300], Step [340/384], Loss: 180.1748\n",
      "Epoch [58/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [58/300], Step [380/384], Loss: 114.6713\n",
      "Epoch [59/300], Step [20/384], Loss: 2.4927\n",
      "Epoch [59/300], Step [40/384], Loss: 79.4808\n",
      "Epoch [59/300], Step [60/384], Loss: 9.3752\n",
      "Epoch [59/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [59/300], Step [100/384], Loss: 87.2032\n",
      "Epoch [59/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [59/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [59/300], Step [160/384], Loss: 104.0518\n",
      "Epoch [59/300], Step [180/384], Loss: 78.6653\n",
      "Epoch [59/300], Step [200/384], Loss: 40.4824\n",
      "Epoch [59/300], Step [220/384], Loss: 71.7725\n",
      "Epoch [59/300], Step [240/384], Loss: 46.4555\n",
      "Epoch [59/300], Step [260/384], Loss: 118.5781\n",
      "Epoch [59/300], Step [280/384], Loss: 53.3246\n",
      "Epoch [59/300], Step [300/384], Loss: 47.8846\n",
      "Epoch [59/300], Step [320/384], Loss: 38.7404\n",
      "Epoch [59/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [59/300], Step [360/384], Loss: 81.3120\n",
      "Epoch [59/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [60/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [60/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [60/300], Step [60/384], Loss: 79.1709\n",
      "Epoch [60/300], Step [80/384], Loss: 48.8955\n",
      "Epoch [60/300], Step [100/384], Loss: 47.0113\n",
      "Epoch [60/300], Step [120/384], Loss: 15.4591\n",
      "Epoch [60/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [60/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [60/300], Step [180/384], Loss: 89.0220\n",
      "Epoch [60/300], Step [200/384], Loss: 85.3666\n",
      "Epoch [60/300], Step [220/384], Loss: 47.3842\n",
      "Epoch [60/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [60/300], Step [260/384], Loss: 255.5572\n",
      "Epoch [60/300], Step [280/384], Loss: 228.0692\n",
      "Epoch [60/300], Step [300/384], Loss: 102.2633\n",
      "Epoch [60/300], Step [320/384], Loss: 46.9423\n",
      "Epoch [60/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [60/300], Step [360/384], Loss: 37.5340\n",
      "Epoch [60/300], Step [380/384], Loss: 7.6372\n",
      "Epoch [61/300], Step [20/384], Loss: 53.1964\n",
      "Epoch [61/300], Step [40/384], Loss: 11.3428\n",
      "Epoch [61/300], Step [60/384], Loss: 7.4894\n",
      "Epoch [61/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [61/300], Step [100/384], Loss: 9.6617\n",
      "Epoch [61/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [61/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [61/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [61/300], Step [180/384], Loss: 27.8426\n",
      "Epoch [61/300], Step [200/384], Loss: 76.6244\n",
      "Epoch [61/300], Step [220/384], Loss: 244.4476\n",
      "Epoch [61/300], Step [240/384], Loss: 34.3118\n",
      "Epoch [61/300], Step [260/384], Loss: 53.6960\n",
      "Epoch [61/300], Step [280/384], Loss: 87.5563\n",
      "Epoch [61/300], Step [300/384], Loss: 52.9033\n",
      "Epoch [61/300], Step [320/384], Loss: 7.5564\n",
      "Epoch [61/300], Step [340/384], Loss: 158.1003\n",
      "Epoch [61/300], Step [360/384], Loss: 236.0518\n",
      "Epoch [61/300], Step [380/384], Loss: 179.3501\n",
      "Epoch [62/300], Step [20/384], Loss: 31.0777\n",
      "Epoch [62/300], Step [40/384], Loss: 15.4982\n",
      "Epoch [62/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [62/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [62/300], Step [100/384], Loss: 216.9443\n",
      "Epoch [62/300], Step [120/384], Loss: 62.8801\n",
      "Epoch [62/300], Step [140/384], Loss: 81.3817\n",
      "Epoch [62/300], Step [160/384], Loss: 462.2114\n",
      "Epoch [62/300], Step [180/384], Loss: 23.3870\n",
      "Epoch [62/300], Step [200/384], Loss: 92.1099\n",
      "Epoch [62/300], Step [220/384], Loss: 207.2503\n",
      "Epoch [62/300], Step [240/384], Loss: 71.6001\n",
      "Epoch [62/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [62/300], Step [280/384], Loss: 188.9628\n",
      "Epoch [62/300], Step [300/384], Loss: 95.5296\n",
      "Epoch [62/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [62/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [62/300], Step [360/384], Loss: 17.6896\n",
      "Epoch [62/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [63/300], Step [20/384], Loss: 9.8153\n",
      "Epoch [63/300], Step [40/384], Loss: 62.4496\n",
      "Epoch [63/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [63/300], Step [80/384], Loss: 178.7128\n",
      "Epoch [63/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [63/300], Step [120/384], Loss: 122.1531\n",
      "Epoch [63/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [63/300], Step [160/384], Loss: 121.9469\n",
      "Epoch [63/300], Step [180/384], Loss: 10.9009\n",
      "Epoch [63/300], Step [200/384], Loss: 204.4181\n",
      "Epoch [63/300], Step [220/384], Loss: 54.6643\n",
      "Epoch [63/300], Step [240/384], Loss: 48.0028\n",
      "Epoch [63/300], Step [260/384], Loss: 3.1738\n",
      "Epoch [63/300], Step [280/384], Loss: 51.8823\n",
      "Epoch [63/300], Step [300/384], Loss: 40.6552\n",
      "Epoch [63/300], Step [320/384], Loss: 154.3153\n",
      "Epoch [63/300], Step [340/384], Loss: 104.3571\n",
      "Epoch [63/300], Step [360/384], Loss: 135.7605\n",
      "Epoch [63/300], Step [380/384], Loss: 58.2669\n",
      "Epoch [64/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [64/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [64/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [64/300], Step [80/384], Loss: 119.2351\n",
      "Epoch [64/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [64/300], Step [120/384], Loss: 22.1651\n",
      "Epoch [64/300], Step [140/384], Loss: 0.2478\n",
      "Epoch [64/300], Step [160/384], Loss: 59.9564\n",
      "Epoch [64/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [64/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [64/300], Step [220/384], Loss: 85.3310\n",
      "Epoch [64/300], Step [240/384], Loss: 36.0267\n",
      "Epoch [64/300], Step [260/384], Loss: 72.6099\n",
      "Epoch [64/300], Step [280/384], Loss: 72.3345\n",
      "Epoch [64/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [64/300], Step [320/384], Loss: 111.8250\n",
      "Epoch [64/300], Step [340/384], Loss: 1.2922\n",
      "Epoch [64/300], Step [360/384], Loss: 31.1898\n",
      "Epoch [64/300], Step [380/384], Loss: 169.5939\n",
      "Epoch [65/300], Step [20/384], Loss: 19.4605\n",
      "Epoch [65/300], Step [40/384], Loss: 22.8889\n",
      "Epoch [65/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [65/300], Step [80/384], Loss: 59.5593\n",
      "Epoch [65/300], Step [100/384], Loss: 151.7005\n",
      "Epoch [65/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [65/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [65/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [65/300], Step [180/384], Loss: 31.3905\n",
      "Epoch [65/300], Step [200/384], Loss: 59.6083\n",
      "Epoch [65/300], Step [220/384], Loss: 229.9875\n",
      "Epoch [65/300], Step [240/384], Loss: 126.8123\n",
      "Epoch [65/300], Step [260/384], Loss: 18.4873\n",
      "Epoch [65/300], Step [280/384], Loss: 61.5124\n",
      "Epoch [65/300], Step [300/384], Loss: 22.0437\n",
      "Epoch [65/300], Step [320/384], Loss: 64.6199\n",
      "Epoch [65/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [65/300], Step [360/384], Loss: 21.5718\n",
      "Epoch [65/300], Step [380/384], Loss: 78.7092\n",
      "Epoch [66/300], Step [20/384], Loss: 40.4845\n",
      "Epoch [66/300], Step [40/384], Loss: 43.4753\n",
      "Epoch [66/300], Step [60/384], Loss: 182.5287\n",
      "Epoch [66/300], Step [80/384], Loss: 211.7655\n",
      "Epoch [66/300], Step [100/384], Loss: 0.0006\n",
      "Epoch [66/300], Step [120/384], Loss: 313.3156\n",
      "Epoch [66/300], Step [140/384], Loss: 1.7596\n",
      "Epoch [66/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [66/300], Step [180/384], Loss: 28.2675\n",
      "Epoch [66/300], Step [200/384], Loss: 24.7020\n",
      "Epoch [66/300], Step [220/384], Loss: 137.9791\n",
      "Epoch [66/300], Step [240/384], Loss: 234.0736\n",
      "Epoch [66/300], Step [260/384], Loss: 37.8598\n",
      "Epoch [66/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [66/300], Step [300/384], Loss: 71.1363\n",
      "Epoch [66/300], Step [320/384], Loss: 334.8923\n",
      "Epoch [66/300], Step [340/384], Loss: 101.3881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/300], Step [360/384], Loss: 155.6901\n",
      "Epoch [66/300], Step [380/384], Loss: 68.0506\n",
      "Epoch [67/300], Step [20/384], Loss: 25.4169\n",
      "Epoch [67/300], Step [40/384], Loss: 139.4520\n",
      "Epoch [67/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [67/300], Step [80/384], Loss: 3.3155\n",
      "Epoch [67/300], Step [100/384], Loss: 115.2611\n",
      "Epoch [67/300], Step [120/384], Loss: 25.8489\n",
      "Epoch [67/300], Step [140/384], Loss: 49.5979\n",
      "Epoch [67/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [67/300], Step [180/384], Loss: 56.7750\n",
      "Epoch [67/300], Step [200/384], Loss: 80.5860\n",
      "Epoch [67/300], Step [220/384], Loss: 64.1458\n",
      "Epoch [67/300], Step [240/384], Loss: 183.0991\n",
      "Epoch [67/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [67/300], Step [280/384], Loss: 0.4935\n",
      "Epoch [67/300], Step [300/384], Loss: 93.8463\n",
      "Epoch [67/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [67/300], Step [340/384], Loss: 20.0808\n",
      "Epoch [67/300], Step [360/384], Loss: 69.5208\n",
      "Epoch [67/300], Step [380/384], Loss: 111.2256\n",
      "Epoch [68/300], Step [20/384], Loss: 180.4377\n",
      "Epoch [68/300], Step [40/384], Loss: 14.7224\n",
      "Epoch [68/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [68/300], Step [80/384], Loss: 86.6629\n",
      "Epoch [68/300], Step [100/384], Loss: 82.0386\n",
      "Epoch [68/300], Step [120/384], Loss: 258.0665\n",
      "Epoch [68/300], Step [140/384], Loss: 10.6867\n",
      "Epoch [68/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [68/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [68/300], Step [200/384], Loss: 47.8958\n",
      "Epoch [68/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [68/300], Step [240/384], Loss: 84.1368\n",
      "Epoch [68/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [68/300], Step [280/384], Loss: 33.9462\n",
      "Epoch [68/300], Step [300/384], Loss: 22.9050\n",
      "Epoch [68/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [68/300], Step [340/384], Loss: 218.9568\n",
      "Epoch [68/300], Step [360/384], Loss: 117.6458\n",
      "Epoch [68/300], Step [380/384], Loss: 211.3847\n",
      "Epoch [69/300], Step [20/384], Loss: 217.4095\n",
      "Epoch [69/300], Step [40/384], Loss: 62.9965\n",
      "Epoch [69/300], Step [60/384], Loss: 73.7112\n",
      "Epoch [69/300], Step [80/384], Loss: 206.7039\n",
      "Epoch [69/300], Step [100/384], Loss: 16.9983\n",
      "Epoch [69/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [69/300], Step [140/384], Loss: 362.7579\n",
      "Epoch [69/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [69/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [69/300], Step [200/384], Loss: 14.0285\n",
      "Epoch [69/300], Step [220/384], Loss: 113.7238\n",
      "Epoch [69/300], Step [240/384], Loss: 156.1603\n",
      "Epoch [69/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [69/300], Step [280/384], Loss: 50.3343\n",
      "Epoch [69/300], Step [300/384], Loss: 4.1363\n",
      "Epoch [69/300], Step [320/384], Loss: 1.3688\n",
      "Epoch [69/300], Step [340/384], Loss: 52.7865\n",
      "Epoch [69/300], Step [360/384], Loss: 41.6700\n",
      "Epoch [69/300], Step [380/384], Loss: 21.5326\n",
      "Epoch [70/300], Step [20/384], Loss: 134.8825\n",
      "Epoch [70/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [70/300], Step [60/384], Loss: 166.7563\n",
      "Epoch [70/300], Step [80/384], Loss: 5.5039\n",
      "Epoch [70/300], Step [100/384], Loss: 29.6732\n",
      "Epoch [70/300], Step [120/384], Loss: 143.3180\n",
      "Epoch [70/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [70/300], Step [160/384], Loss: 142.5186\n",
      "Epoch [70/300], Step [180/384], Loss: 36.6342\n",
      "Epoch [70/300], Step [200/384], Loss: 107.5929\n",
      "Epoch [70/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [70/300], Step [240/384], Loss: 146.7318\n",
      "Epoch [70/300], Step [260/384], Loss: 20.5739\n",
      "Epoch [70/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [70/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [70/300], Step [320/384], Loss: 150.6487\n",
      "Epoch [70/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [70/300], Step [360/384], Loss: 246.9895\n",
      "Epoch [70/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [71/300], Step [20/384], Loss: 10.2428\n",
      "Epoch [71/300], Step [40/384], Loss: 265.6096\n",
      "Epoch [71/300], Step [60/384], Loss: 23.5688\n",
      "Epoch [71/300], Step [80/384], Loss: 0.5093\n",
      "Epoch [71/300], Step [100/384], Loss: 91.9024\n",
      "Epoch [71/300], Step [120/384], Loss: 7.9107\n",
      "Epoch [71/300], Step [140/384], Loss: 110.3330\n",
      "Epoch [71/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [71/300], Step [180/384], Loss: 127.6235\n",
      "Epoch [71/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [71/300], Step [220/384], Loss: 127.9988\n",
      "Epoch [71/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [71/300], Step [260/384], Loss: 25.4400\n",
      "Epoch [71/300], Step [280/384], Loss: 51.4226\n",
      "Epoch [71/300], Step [300/384], Loss: 150.3018\n",
      "Epoch [71/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [71/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [71/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [71/300], Step [380/384], Loss: 51.8547\n",
      "Epoch [72/300], Step [20/384], Loss: 85.5165\n",
      "Epoch [72/300], Step [40/384], Loss: 157.3034\n",
      "Epoch [72/300], Step [60/384], Loss: 157.1334\n",
      "Epoch [72/300], Step [80/384], Loss: 169.8214\n",
      "Epoch [72/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [72/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [72/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [72/300], Step [160/384], Loss: 0.0732\n",
      "Epoch [72/300], Step [180/384], Loss: 128.4946\n",
      "Epoch [72/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [72/300], Step [220/384], Loss: 94.0845\n",
      "Epoch [72/300], Step [240/384], Loss: 1.4318\n",
      "Epoch [72/300], Step [260/384], Loss: 145.9271\n",
      "Epoch [72/300], Step [280/384], Loss: 80.7504\n",
      "Epoch [72/300], Step [300/384], Loss: 32.2113\n",
      "Epoch [72/300], Step [320/384], Loss: 93.5387\n",
      "Epoch [72/300], Step [340/384], Loss: 101.0822\n",
      "Epoch [72/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [72/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [73/300], Step [20/384], Loss: 87.2490\n",
      "Epoch [73/300], Step [40/384], Loss: 33.8326\n",
      "Epoch [73/300], Step [60/384], Loss: 34.1119\n",
      "Epoch [73/300], Step [80/384], Loss: 83.8204\n",
      "Epoch [73/300], Step [100/384], Loss: 81.0813\n",
      "Epoch [73/300], Step [120/384], Loss: 150.2840\n",
      "Epoch [73/300], Step [140/384], Loss: 53.8167\n",
      "Epoch [73/300], Step [160/384], Loss: 81.1233\n",
      "Epoch [73/300], Step [180/384], Loss: 32.2716\n",
      "Epoch [73/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [73/300], Step [220/384], Loss: 35.5802\n",
      "Epoch [73/300], Step [240/384], Loss: 151.2717\n",
      "Epoch [73/300], Step [260/384], Loss: 62.6097\n",
      "Epoch [73/300], Step [280/384], Loss: 37.6639\n",
      "Epoch [73/300], Step [300/384], Loss: 49.8131\n",
      "Epoch [73/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [73/300], Step [340/384], Loss: 90.6795\n",
      "Epoch [73/300], Step [360/384], Loss: 66.2374\n",
      "Epoch [73/300], Step [380/384], Loss: 119.7005\n",
      "Epoch [74/300], Step [20/384], Loss: 75.0303\n",
      "Epoch [74/300], Step [40/384], Loss: 75.1447\n",
      "Epoch [74/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [74/300], Step [80/384], Loss: 134.8909\n",
      "Epoch [74/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [74/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [74/300], Step [140/384], Loss: 24.8303\n",
      "Epoch [74/300], Step [160/384], Loss: 4.3941\n",
      "Epoch [74/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [74/300], Step [200/384], Loss: 81.8846\n",
      "Epoch [74/300], Step [220/384], Loss: 474.7761\n",
      "Epoch [74/300], Step [240/384], Loss: 94.7487\n",
      "Epoch [74/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [74/300], Step [280/384], Loss: 71.8345\n",
      "Epoch [74/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [74/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [74/300], Step [340/384], Loss: 1.4965\n",
      "Epoch [74/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [74/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [75/300], Step [20/384], Loss: 167.7974\n",
      "Epoch [75/300], Step [40/384], Loss: 13.0158\n",
      "Epoch [75/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [75/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [75/300], Step [100/384], Loss: 306.3192\n",
      "Epoch [75/300], Step [120/384], Loss: 85.9857\n",
      "Epoch [75/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [75/300], Step [160/384], Loss: 148.5405\n",
      "Epoch [75/300], Step [180/384], Loss: 37.9668\n",
      "Epoch [75/300], Step [200/384], Loss: 12.2244\n",
      "Epoch [75/300], Step [220/384], Loss: 104.1767\n",
      "Epoch [75/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [75/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [75/300], Step [280/384], Loss: 57.5877\n",
      "Epoch [75/300], Step [300/384], Loss: 0.0005\n",
      "Epoch [75/300], Step [320/384], Loss: 6.3309\n",
      "Epoch [75/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [75/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [75/300], Step [380/384], Loss: 43.9740\n",
      "Epoch [76/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [76/300], Step [40/384], Loss: 184.3599\n",
      "Epoch [76/300], Step [60/384], Loss: 45.7226\n",
      "Epoch [76/300], Step [80/384], Loss: 123.1090\n",
      "Epoch [76/300], Step [100/384], Loss: 110.7800\n",
      "Epoch [76/300], Step [120/384], Loss: 85.5767\n",
      "Epoch [76/300], Step [140/384], Loss: 21.7856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/300], Step [160/384], Loss: 6.0358\n",
      "Epoch [76/300], Step [180/384], Loss: 220.8714\n",
      "Epoch [76/300], Step [200/384], Loss: 29.0244\n",
      "Epoch [76/300], Step [220/384], Loss: 297.9139\n",
      "Epoch [76/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [76/300], Step [260/384], Loss: 108.7749\n",
      "Epoch [76/300], Step [280/384], Loss: 78.6718\n",
      "Epoch [76/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [76/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [76/300], Step [340/384], Loss: 49.3459\n",
      "Epoch [76/300], Step [360/384], Loss: 82.8215\n",
      "Epoch [76/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [77/300], Step [20/384], Loss: 15.5923\n",
      "Epoch [77/300], Step [40/384], Loss: 126.2380\n",
      "Epoch [77/300], Step [60/384], Loss: 106.5623\n",
      "Epoch [77/300], Step [80/384], Loss: 43.8139\n",
      "Epoch [77/300], Step [100/384], Loss: 14.3859\n",
      "Epoch [77/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [77/300], Step [140/384], Loss: 0.6506\n",
      "Epoch [77/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [77/300], Step [180/384], Loss: 25.6812\n",
      "Epoch [77/300], Step [200/384], Loss: 80.1728\n",
      "Epoch [77/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [77/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [77/300], Step [260/384], Loss: 124.5174\n",
      "Epoch [77/300], Step [280/384], Loss: 209.1363\n",
      "Epoch [77/300], Step [300/384], Loss: 115.2260\n",
      "Epoch [77/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [77/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [77/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [77/300], Step [380/384], Loss: 40.3294\n",
      "Epoch [78/300], Step [20/384], Loss: 208.6060\n",
      "Epoch [78/300], Step [40/384], Loss: 110.7069\n",
      "Epoch [78/300], Step [60/384], Loss: 59.4815\n",
      "Epoch [78/300], Step [80/384], Loss: 19.1605\n",
      "Epoch [78/300], Step [100/384], Loss: 195.8350\n",
      "Epoch [78/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [78/300], Step [140/384], Loss: 42.2350\n",
      "Epoch [78/300], Step [160/384], Loss: 50.6995\n",
      "Epoch [78/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [78/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [78/300], Step [220/384], Loss: 76.1518\n",
      "Epoch [78/300], Step [240/384], Loss: 37.9023\n",
      "Epoch [78/300], Step [260/384], Loss: 1.6869\n",
      "Epoch [78/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [78/300], Step [300/384], Loss: 71.6096\n",
      "Epoch [78/300], Step [320/384], Loss: 13.8590\n",
      "Epoch [78/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [78/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [78/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [79/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [79/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [79/300], Step [60/384], Loss: 138.4618\n",
      "Epoch [79/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [79/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [79/300], Step [120/384], Loss: 61.2289\n",
      "Epoch [79/300], Step [140/384], Loss: 52.2354\n",
      "Epoch [79/300], Step [160/384], Loss: 28.7673\n",
      "Epoch [79/300], Step [180/384], Loss: 184.3380\n",
      "Epoch [79/300], Step [200/384], Loss: 47.1996\n",
      "Epoch [79/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [79/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [79/300], Step [260/384], Loss: 273.1205\n",
      "Epoch [79/300], Step [280/384], Loss: 83.8701\n",
      "Epoch [79/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [79/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [79/300], Step [340/384], Loss: 55.2104\n",
      "Epoch [79/300], Step [360/384], Loss: 191.6665\n",
      "Epoch [79/300], Step [380/384], Loss: 47.5153\n",
      "Epoch [80/300], Step [20/384], Loss: 30.3866\n",
      "Epoch [80/300], Step [40/384], Loss: 43.6220\n",
      "Epoch [80/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [80/300], Step [80/384], Loss: 52.2690\n",
      "Epoch [80/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [80/300], Step [120/384], Loss: 46.5352\n",
      "Epoch [80/300], Step [140/384], Loss: 30.9134\n",
      "Epoch [80/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [80/300], Step [180/384], Loss: 113.7371\n",
      "Epoch [80/300], Step [200/384], Loss: 126.9483\n",
      "Epoch [80/300], Step [220/384], Loss: 4.5103\n",
      "Epoch [80/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [80/300], Step [260/384], Loss: 38.3503\n",
      "Epoch [80/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [80/300], Step [300/384], Loss: 23.2875\n",
      "Epoch [80/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [80/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [80/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [80/300], Step [380/384], Loss: 75.4802\n",
      "Epoch [81/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [81/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [81/300], Step [60/384], Loss: 11.4852\n",
      "Epoch [81/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [81/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [81/300], Step [120/384], Loss: 42.0537\n",
      "Epoch [81/300], Step [140/384], Loss: 42.8291\n",
      "Epoch [81/300], Step [160/384], Loss: 70.9959\n",
      "Epoch [81/300], Step [180/384], Loss: 191.9485\n",
      "Epoch [81/300], Step [200/384], Loss: 82.3039\n",
      "Epoch [81/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [81/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [81/300], Step [260/384], Loss: 0.0002\n",
      "Epoch [81/300], Step [280/384], Loss: 38.1296\n",
      "Epoch [81/300], Step [300/384], Loss: 72.0371\n",
      "Epoch [81/300], Step [320/384], Loss: 27.7806\n",
      "Epoch [81/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [81/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [81/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [82/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [82/300], Step [40/384], Loss: 74.5952\n",
      "Epoch [82/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [82/300], Step [80/384], Loss: 28.0271\n",
      "Epoch [82/300], Step [100/384], Loss: 9.8496\n",
      "Epoch [82/300], Step [120/384], Loss: 4.3295\n",
      "Epoch [82/300], Step [140/384], Loss: 18.0696\n",
      "Epoch [82/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [82/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [82/300], Step [200/384], Loss: 182.2394\n",
      "Epoch [82/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [82/300], Step [240/384], Loss: 35.2491\n",
      "Epoch [82/300], Step [260/384], Loss: 11.2761\n",
      "Epoch [82/300], Step [280/384], Loss: 107.5044\n",
      "Epoch [82/300], Step [300/384], Loss: 93.4779\n",
      "Epoch [82/300], Step [320/384], Loss: 16.1024\n",
      "Epoch [82/300], Step [340/384], Loss: 85.2377\n",
      "Epoch [82/300], Step [360/384], Loss: 22.2949\n",
      "Epoch [82/300], Step [380/384], Loss: 27.4834\n",
      "Epoch [83/300], Step [20/384], Loss: 85.6634\n",
      "Epoch [83/300], Step [40/384], Loss: 43.0145\n",
      "Epoch [83/300], Step [60/384], Loss: 115.4053\n",
      "Epoch [83/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [83/300], Step [100/384], Loss: 138.9471\n",
      "Epoch [83/300], Step [120/384], Loss: 8.7451\n",
      "Epoch [83/300], Step [140/384], Loss: 55.7103\n",
      "Epoch [83/300], Step [160/384], Loss: 114.8140\n",
      "Epoch [83/300], Step [180/384], Loss: 20.6005\n",
      "Epoch [83/300], Step [200/384], Loss: 29.5717\n",
      "Epoch [83/300], Step [220/384], Loss: 33.5524\n",
      "Epoch [83/300], Step [240/384], Loss: 13.2571\n",
      "Epoch [83/300], Step [260/384], Loss: 25.9022\n",
      "Epoch [83/300], Step [280/384], Loss: 26.5759\n",
      "Epoch [83/300], Step [300/384], Loss: 36.4350\n",
      "Epoch [83/300], Step [320/384], Loss: 28.4690\n",
      "Epoch [83/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [83/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [83/300], Step [380/384], Loss: 29.1847\n",
      "Epoch [84/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [84/300], Step [40/384], Loss: 88.9170\n",
      "Epoch [84/300], Step [60/384], Loss: 100.9569\n",
      "Epoch [84/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [84/300], Step [100/384], Loss: 49.0661\n",
      "Epoch [84/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [84/300], Step [140/384], Loss: 64.0749\n",
      "Epoch [84/300], Step [160/384], Loss: 204.4935\n",
      "Epoch [84/300], Step [180/384], Loss: 0.5359\n",
      "Epoch [84/300], Step [200/384], Loss: 76.9700\n",
      "Epoch [84/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [84/300], Step [240/384], Loss: 22.2050\n",
      "Epoch [84/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [84/300], Step [280/384], Loss: 137.0802\n",
      "Epoch [84/300], Step [300/384], Loss: 51.4224\n",
      "Epoch [84/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [84/300], Step [340/384], Loss: 82.1224\n",
      "Epoch [84/300], Step [360/384], Loss: 79.6013\n",
      "Epoch [84/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [85/300], Step [20/384], Loss: 20.7998\n",
      "Epoch [85/300], Step [40/384], Loss: 33.6197\n",
      "Epoch [85/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [85/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [85/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [85/300], Step [120/384], Loss: 21.9714\n",
      "Epoch [85/300], Step [140/384], Loss: 127.6898\n",
      "Epoch [85/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [85/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [85/300], Step [200/384], Loss: 15.2341\n",
      "Epoch [85/300], Step [220/384], Loss: 64.9477\n",
      "Epoch [85/300], Step [240/384], Loss: 114.5341\n",
      "Epoch [85/300], Step [260/384], Loss: 16.4496\n",
      "Epoch [85/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [85/300], Step [300/384], Loss: 92.3387\n",
      "Epoch [85/300], Step [320/384], Loss: 163.6268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [85/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [85/300], Step [360/384], Loss: 15.0121\n",
      "Epoch [85/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [86/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [86/300], Step [40/384], Loss: 328.7221\n",
      "Epoch [86/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [86/300], Step [80/384], Loss: 16.4029\n",
      "Epoch [86/300], Step [100/384], Loss: 68.8052\n",
      "Epoch [86/300], Step [120/384], Loss: 61.0865\n",
      "Epoch [86/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [86/300], Step [160/384], Loss: 38.1624\n",
      "Epoch [86/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [86/300], Step [200/384], Loss: 61.2875\n",
      "Epoch [86/300], Step [220/384], Loss: 134.2867\n",
      "Epoch [86/300], Step [240/384], Loss: 85.7211\n",
      "Epoch [86/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [86/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [86/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [86/300], Step [320/384], Loss: 43.9081\n",
      "Epoch [86/300], Step [340/384], Loss: 147.1568\n",
      "Epoch [86/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [86/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [87/300], Step [20/384], Loss: 155.1027\n",
      "Epoch [87/300], Step [40/384], Loss: 8.0046\n",
      "Epoch [87/300], Step [60/384], Loss: 54.7684\n",
      "Epoch [87/300], Step [80/384], Loss: 8.7204\n",
      "Epoch [87/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [87/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [87/300], Step [140/384], Loss: 93.9326\n",
      "Epoch [87/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [87/300], Step [180/384], Loss: 16.4525\n",
      "Epoch [87/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [87/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [87/300], Step [240/384], Loss: 14.5532\n",
      "Epoch [87/300], Step [260/384], Loss: 53.7351\n",
      "Epoch [87/300], Step [280/384], Loss: 4.7921\n",
      "Epoch [87/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [87/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [87/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [87/300], Step [360/384], Loss: 34.2409\n",
      "Epoch [87/300], Step [380/384], Loss: 48.0175\n",
      "Epoch [88/300], Step [20/384], Loss: 80.1161\n",
      "Epoch [88/300], Step [40/384], Loss: 35.5883\n",
      "Epoch [88/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [88/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [88/300], Step [100/384], Loss: 26.2363\n",
      "Epoch [88/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [88/300], Step [140/384], Loss: 7.5006\n",
      "Epoch [88/300], Step [160/384], Loss: 34.3296\n",
      "Epoch [88/300], Step [180/384], Loss: 85.3033\n",
      "Epoch [88/300], Step [200/384], Loss: 105.4772\n",
      "Epoch [88/300], Step [220/384], Loss: 65.3649\n",
      "Epoch [88/300], Step [240/384], Loss: 10.7377\n",
      "Epoch [88/300], Step [260/384], Loss: 76.9844\n",
      "Epoch [88/300], Step [280/384], Loss: 5.9519\n",
      "Epoch [88/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [88/300], Step [320/384], Loss: 96.4594\n",
      "Epoch [88/300], Step [340/384], Loss: 69.8595\n",
      "Epoch [88/300], Step [360/384], Loss: 14.8896\n",
      "Epoch [88/300], Step [380/384], Loss: 29.2365\n",
      "Epoch [89/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [89/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [89/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [89/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [89/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [89/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [89/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [89/300], Step [160/384], Loss: 16.9625\n",
      "Epoch [89/300], Step [180/384], Loss: 58.8199\n",
      "Epoch [89/300], Step [200/384], Loss: 39.2365\n",
      "Epoch [89/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [89/300], Step [240/384], Loss: 26.7795\n",
      "Epoch [89/300], Step [260/384], Loss: 56.8482\n",
      "Epoch [89/300], Step [280/384], Loss: 76.7590\n",
      "Epoch [89/300], Step [300/384], Loss: 67.9078\n",
      "Epoch [89/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [89/300], Step [340/384], Loss: 132.1880\n",
      "Epoch [89/300], Step [360/384], Loss: 4.4805\n",
      "Epoch [89/300], Step [380/384], Loss: 154.3117\n",
      "Epoch [90/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [90/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [90/300], Step [60/384], Loss: 0.9931\n",
      "Epoch [90/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [90/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [90/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [90/300], Step [140/384], Loss: 87.8745\n",
      "Epoch [90/300], Step [160/384], Loss: 160.7132\n",
      "Epoch [90/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [90/300], Step [200/384], Loss: 9.1272\n",
      "Epoch [90/300], Step [220/384], Loss: 19.9111\n",
      "Epoch [90/300], Step [240/384], Loss: 62.1893\n",
      "Epoch [90/300], Step [260/384], Loss: 41.5550\n",
      "Epoch [90/300], Step [280/384], Loss: 222.8473\n",
      "Epoch [90/300], Step [300/384], Loss: 105.9073\n",
      "Epoch [90/300], Step [320/384], Loss: 44.6727\n",
      "Epoch [90/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [90/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [90/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [91/300], Step [20/384], Loss: 0.4583\n",
      "Epoch [91/300], Step [40/384], Loss: 127.1471\n",
      "Epoch [91/300], Step [60/384], Loss: 81.4866\n",
      "Epoch [91/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [91/300], Step [100/384], Loss: 84.4382\n",
      "Epoch [91/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [91/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [91/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [91/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [91/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [91/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [91/300], Step [240/384], Loss: 189.9404\n",
      "Epoch [91/300], Step [260/384], Loss: 230.9382\n",
      "Epoch [91/300], Step [280/384], Loss: 395.3034\n",
      "Epoch [91/300], Step [300/384], Loss: 220.2010\n",
      "Epoch [91/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [91/300], Step [340/384], Loss: 32.8086\n",
      "Epoch [91/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [91/300], Step [380/384], Loss: 66.1616\n",
      "Epoch [92/300], Step [20/384], Loss: 50.3976\n",
      "Epoch [92/300], Step [40/384], Loss: 51.4799\n",
      "Epoch [92/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [92/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [92/300], Step [100/384], Loss: 231.6527\n",
      "Epoch [92/300], Step [120/384], Loss: 46.1736\n",
      "Epoch [92/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [92/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [92/300], Step [180/384], Loss: 10.2722\n",
      "Epoch [92/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [92/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [92/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [92/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [92/300], Step [280/384], Loss: 28.9339\n",
      "Epoch [92/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [92/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [92/300], Step [340/384], Loss: 86.5083\n",
      "Epoch [92/300], Step [360/384], Loss: 78.6421\n",
      "Epoch [92/300], Step [380/384], Loss: 236.1492\n",
      "Epoch [93/300], Step [20/384], Loss: 66.7117\n",
      "Epoch [93/300], Step [40/384], Loss: 133.3955\n",
      "Epoch [93/300], Step [60/384], Loss: 45.0988\n",
      "Epoch [93/300], Step [80/384], Loss: 0.0003\n",
      "Epoch [93/300], Step [100/384], Loss: 140.1609\n",
      "Epoch [93/300], Step [120/384], Loss: 165.2259\n",
      "Epoch [93/300], Step [140/384], Loss: 97.6903\n",
      "Epoch [93/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [93/300], Step [180/384], Loss: 51.8618\n",
      "Epoch [93/300], Step [200/384], Loss: 53.3315\n",
      "Epoch [93/300], Step [220/384], Loss: 68.2378\n",
      "Epoch [93/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [93/300], Step [260/384], Loss: 146.5941\n",
      "Epoch [93/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [93/300], Step [300/384], Loss: 111.9665\n",
      "Epoch [93/300], Step [320/384], Loss: 129.4773\n",
      "Epoch [93/300], Step [340/384], Loss: 92.5084\n",
      "Epoch [93/300], Step [360/384], Loss: 35.1119\n",
      "Epoch [93/300], Step [380/384], Loss: 40.2942\n",
      "Epoch [94/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [94/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [94/300], Step [60/384], Loss: 256.0029\n",
      "Epoch [94/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [94/300], Step [100/384], Loss: 47.4605\n",
      "Epoch [94/300], Step [120/384], Loss: 31.7297\n",
      "Epoch [94/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [94/300], Step [160/384], Loss: 90.5780\n",
      "Epoch [94/300], Step [180/384], Loss: 33.1341\n",
      "Epoch [94/300], Step [200/384], Loss: 58.2547\n",
      "Epoch [94/300], Step [220/384], Loss: 83.8669\n",
      "Epoch [94/300], Step [240/384], Loss: 9.0248\n",
      "Epoch [94/300], Step [260/384], Loss: 279.7922\n",
      "Epoch [94/300], Step [280/384], Loss: 126.1589\n",
      "Epoch [94/300], Step [300/384], Loss: 111.4855\n",
      "Epoch [94/300], Step [320/384], Loss: 146.5792\n",
      "Epoch [94/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [94/300], Step [360/384], Loss: 90.2102\n",
      "Epoch [94/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [95/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [95/300], Step [40/384], Loss: 52.6759\n",
      "Epoch [95/300], Step [60/384], Loss: 422.3705\n",
      "Epoch [95/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [95/300], Step [100/384], Loss: 0.0001\n",
      "Epoch [95/300], Step [120/384], Loss: 111.0694\n",
      "Epoch [95/300], Step [140/384], Loss: 27.4825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [95/300], Step [160/384], Loss: 93.2480\n",
      "Epoch [95/300], Step [180/384], Loss: 51.2864\n",
      "Epoch [95/300], Step [200/384], Loss: 50.1717\n",
      "Epoch [95/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [95/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [95/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [95/300], Step [280/384], Loss: 24.4874\n",
      "Epoch [95/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [95/300], Step [320/384], Loss: 72.5661\n",
      "Epoch [95/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [95/300], Step [360/384], Loss: 0.0743\n",
      "Epoch [95/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [96/300], Step [20/384], Loss: 37.7313\n",
      "Epoch [96/300], Step [40/384], Loss: 125.5909\n",
      "Epoch [96/300], Step [60/384], Loss: 85.4690\n",
      "Epoch [96/300], Step [80/384], Loss: 42.0548\n",
      "Epoch [96/300], Step [100/384], Loss: 10.7889\n",
      "Epoch [96/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [96/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [96/300], Step [160/384], Loss: 0.0017\n",
      "Epoch [96/300], Step [180/384], Loss: 262.6348\n",
      "Epoch [96/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [96/300], Step [220/384], Loss: 189.0514\n",
      "Epoch [96/300], Step [240/384], Loss: 9.4994\n",
      "Epoch [96/300], Step [260/384], Loss: 68.8359\n",
      "Epoch [96/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [96/300], Step [300/384], Loss: 88.3265\n",
      "Epoch [96/300], Step [320/384], Loss: 10.7392\n",
      "Epoch [96/300], Step [340/384], Loss: 365.5456\n",
      "Epoch [96/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [96/300], Step [380/384], Loss: 79.4389\n",
      "Epoch [97/300], Step [20/384], Loss: 84.0818\n",
      "Epoch [97/300], Step [40/384], Loss: 219.8388\n",
      "Epoch [97/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [97/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [97/300], Step [100/384], Loss: 202.2066\n",
      "Epoch [97/300], Step [120/384], Loss: 4.0061\n",
      "Epoch [97/300], Step [140/384], Loss: 20.8106\n",
      "Epoch [97/300], Step [160/384], Loss: 23.9030\n",
      "Epoch [97/300], Step [180/384], Loss: 50.9821\n",
      "Epoch [97/300], Step [200/384], Loss: 3.2949\n",
      "Epoch [97/300], Step [220/384], Loss: 193.0567\n",
      "Epoch [97/300], Step [240/384], Loss: 132.6898\n",
      "Epoch [97/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [97/300], Step [280/384], Loss: 40.8051\n",
      "Epoch [97/300], Step [300/384], Loss: 11.1507\n",
      "Epoch [97/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [97/300], Step [340/384], Loss: 124.7134\n",
      "Epoch [97/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [97/300], Step [380/384], Loss: 93.9502\n",
      "Epoch [98/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [98/300], Step [40/384], Loss: 79.9479\n",
      "Epoch [98/300], Step [60/384], Loss: 73.0303\n",
      "Epoch [98/300], Step [80/384], Loss: 22.8031\n",
      "Epoch [98/300], Step [100/384], Loss: 61.6303\n",
      "Epoch [98/300], Step [120/384], Loss: 7.6019\n",
      "Epoch [98/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [98/300], Step [160/384], Loss: 208.4387\n",
      "Epoch [98/300], Step [180/384], Loss: 38.7343\n",
      "Epoch [98/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [98/300], Step [220/384], Loss: 12.2145\n",
      "Epoch [98/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [98/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [98/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [98/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [98/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [98/300], Step [340/384], Loss: 2.7686\n",
      "Epoch [98/300], Step [360/384], Loss: 166.7246\n",
      "Epoch [98/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [99/300], Step [20/384], Loss: 72.5641\n",
      "Epoch [99/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [99/300], Step [60/384], Loss: 0.2305\n",
      "Epoch [99/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [99/300], Step [100/384], Loss: 28.4621\n",
      "Epoch [99/300], Step [120/384], Loss: 24.5948\n",
      "Epoch [99/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [99/300], Step [160/384], Loss: 26.2230\n",
      "Epoch [99/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [99/300], Step [200/384], Loss: 11.1845\n",
      "Epoch [99/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [99/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [99/300], Step [260/384], Loss: 60.6739\n",
      "Epoch [99/300], Step [280/384], Loss: 42.0678\n",
      "Epoch [99/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [99/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [99/300], Step [340/384], Loss: 62.8496\n",
      "Epoch [99/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [99/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [40/384], Loss: 107.9855\n",
      "Epoch [100/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [80/384], Loss: 119.2414\n",
      "Epoch [100/300], Step [100/384], Loss: 53.8131\n",
      "Epoch [100/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [140/384], Loss: 78.8761\n",
      "Epoch [100/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [220/384], Loss: 93.1910\n",
      "Epoch [100/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [340/384], Loss: 175.3764\n",
      "Epoch [100/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [101/300], Step [20/384], Loss: 194.4340\n",
      "Epoch [101/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [101/300], Step [60/384], Loss: 113.6777\n",
      "Epoch [101/300], Step [80/384], Loss: 42.6122\n",
      "Epoch [101/300], Step [100/384], Loss: 40.7615\n",
      "Epoch [101/300], Step [120/384], Loss: 83.6606\n",
      "Epoch [101/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [101/300], Step [160/384], Loss: 24.1339\n",
      "Epoch [101/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [101/300], Step [200/384], Loss: 68.4329\n",
      "Epoch [101/300], Step [220/384], Loss: 284.9458\n",
      "Epoch [101/300], Step [240/384], Loss: 288.0898\n",
      "Epoch [101/300], Step [260/384], Loss: 38.7319\n",
      "Epoch [101/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [101/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [101/300], Step [320/384], Loss: 147.7413\n",
      "Epoch [101/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [101/300], Step [360/384], Loss: 89.6494\n",
      "Epoch [101/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [102/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [102/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [102/300], Step [60/384], Loss: 97.1346\n",
      "Epoch [102/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [102/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [102/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [102/300], Step [140/384], Loss: 3.2466\n",
      "Epoch [102/300], Step [160/384], Loss: 50.4398\n",
      "Epoch [102/300], Step [180/384], Loss: 38.8910\n",
      "Epoch [102/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [102/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [102/300], Step [240/384], Loss: 33.0115\n",
      "Epoch [102/300], Step [260/384], Loss: 245.9346\n",
      "Epoch [102/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [102/300], Step [300/384], Loss: 9.3002\n",
      "Epoch [102/300], Step [320/384], Loss: 53.8813\n",
      "Epoch [102/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [102/300], Step [360/384], Loss: 70.9953\n",
      "Epoch [102/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [103/300], Step [20/384], Loss: 21.9619\n",
      "Epoch [103/300], Step [40/384], Loss: 70.9856\n",
      "Epoch [103/300], Step [60/384], Loss: 6.0236\n",
      "Epoch [103/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [103/300], Step [100/384], Loss: 63.9678\n",
      "Epoch [103/300], Step [120/384], Loss: 13.9914\n",
      "Epoch [103/300], Step [140/384], Loss: 94.6331\n",
      "Epoch [103/300], Step [160/384], Loss: 274.8376\n",
      "Epoch [103/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [103/300], Step [200/384], Loss: 54.6469\n",
      "Epoch [103/300], Step [220/384], Loss: 28.0808\n",
      "Epoch [103/300], Step [240/384], Loss: 0.9039\n",
      "Epoch [103/300], Step [260/384], Loss: 85.3985\n",
      "Epoch [103/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [103/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [103/300], Step [320/384], Loss: 158.3892\n",
      "Epoch [103/300], Step [340/384], Loss: 129.0583\n",
      "Epoch [103/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [103/300], Step [380/384], Loss: 31.3260\n",
      "Epoch [104/300], Step [20/384], Loss: 24.9287\n",
      "Epoch [104/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [104/300], Step [60/384], Loss: 15.7748\n",
      "Epoch [104/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [104/300], Step [100/384], Loss: 189.4914\n",
      "Epoch [104/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [104/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [104/300], Step [160/384], Loss: 76.7661\n",
      "Epoch [104/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [104/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [104/300], Step [220/384], Loss: 239.0041\n",
      "Epoch [104/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [104/300], Step [260/384], Loss: 136.2757\n",
      "Epoch [104/300], Step [280/384], Loss: 35.8143\n",
      "Epoch [104/300], Step [300/384], Loss: 23.8530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [104/300], Step [320/384], Loss: 93.3487\n",
      "Epoch [104/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [104/300], Step [360/384], Loss: 73.7971\n",
      "Epoch [104/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [105/300], Step [20/384], Loss: 9.1306\n",
      "Epoch [105/300], Step [40/384], Loss: 95.8574\n",
      "Epoch [105/300], Step [60/384], Loss: 59.1402\n",
      "Epoch [105/300], Step [80/384], Loss: 100.2684\n",
      "Epoch [105/300], Step [100/384], Loss: 62.0372\n",
      "Epoch [105/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [105/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [105/300], Step [160/384], Loss: 151.1213\n",
      "Epoch [105/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [105/300], Step [200/384], Loss: 45.9449\n",
      "Epoch [105/300], Step [220/384], Loss: 43.5175\n",
      "Epoch [105/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [105/300], Step [260/384], Loss: 54.6713\n",
      "Epoch [105/300], Step [280/384], Loss: 20.6017\n",
      "Epoch [105/300], Step [300/384], Loss: 132.0236\n",
      "Epoch [105/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [105/300], Step [340/384], Loss: 20.1071\n",
      "Epoch [105/300], Step [360/384], Loss: 21.5073\n",
      "Epoch [105/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [20/384], Loss: 0.0018\n",
      "Epoch [106/300], Step [40/384], Loss: 21.6799\n",
      "Epoch [106/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [80/384], Loss: 116.5103\n",
      "Epoch [106/300], Step [100/384], Loss: 38.7043\n",
      "Epoch [106/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [160/384], Loss: 38.3814\n",
      "Epoch [106/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [220/384], Loss: 44.9524\n",
      "Epoch [106/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [280/384], Loss: 38.5466\n",
      "Epoch [106/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [380/384], Loss: 208.2040\n",
      "Epoch [107/300], Step [20/384], Loss: 57.3649\n",
      "Epoch [107/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [107/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [107/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [107/300], Step [100/384], Loss: 208.2249\n",
      "Epoch [107/300], Step [120/384], Loss: 106.8203\n",
      "Epoch [107/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [107/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [107/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [107/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [107/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [107/300], Step [240/384], Loss: 56.7473\n",
      "Epoch [107/300], Step [260/384], Loss: 83.3159\n",
      "Epoch [107/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [107/300], Step [300/384], Loss: 35.7117\n",
      "Epoch [107/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [107/300], Step [340/384], Loss: 41.4176\n",
      "Epoch [107/300], Step [360/384], Loss: 26.5051\n",
      "Epoch [107/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [108/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [108/300], Step [40/384], Loss: 167.4801\n",
      "Epoch [108/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [108/300], Step [80/384], Loss: 66.0742\n",
      "Epoch [108/300], Step [100/384], Loss: 84.3531\n",
      "Epoch [108/300], Step [120/384], Loss: 202.1368\n",
      "Epoch [108/300], Step [140/384], Loss: 44.7888\n",
      "Epoch [108/300], Step [160/384], Loss: 121.3555\n",
      "Epoch [108/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [108/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [108/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [108/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [108/300], Step [260/384], Loss: 0.0046\n",
      "Epoch [108/300], Step [280/384], Loss: 36.2759\n",
      "Epoch [108/300], Step [300/384], Loss: 40.1909\n",
      "Epoch [108/300], Step [320/384], Loss: 7.2399\n",
      "Epoch [108/300], Step [340/384], Loss: 60.7139\n",
      "Epoch [108/300], Step [360/384], Loss: 113.0009\n",
      "Epoch [108/300], Step [380/384], Loss: 23.4850\n",
      "Epoch [109/300], Step [20/384], Loss: 230.5442\n",
      "Epoch [109/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [60/384], Loss: 7.2825\n",
      "Epoch [109/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [100/384], Loss: 86.2124\n",
      "Epoch [109/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [140/384], Loss: 12.7431\n",
      "Epoch [109/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [180/384], Loss: 72.3931\n",
      "Epoch [109/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [110/300], Step [20/384], Loss: 51.5760\n",
      "Epoch [110/300], Step [40/384], Loss: 6.4035\n",
      "Epoch [110/300], Step [60/384], Loss: 65.3054\n",
      "Epoch [110/300], Step [80/384], Loss: 64.3370\n",
      "Epoch [110/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [110/300], Step [120/384], Loss: 103.5014\n",
      "Epoch [110/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [110/300], Step [160/384], Loss: 132.2215\n",
      "Epoch [110/300], Step [180/384], Loss: 74.4772\n",
      "Epoch [110/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [110/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [110/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [110/300], Step [260/384], Loss: 130.7554\n",
      "Epoch [110/300], Step [280/384], Loss: 40.0781\n",
      "Epoch [110/300], Step [300/384], Loss: 30.4938\n",
      "Epoch [110/300], Step [320/384], Loss: 161.0483\n",
      "Epoch [110/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [110/300], Step [360/384], Loss: 208.9572\n",
      "Epoch [110/300], Step [380/384], Loss: 30.1863\n",
      "Epoch [111/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [111/300], Step [40/384], Loss: 21.6380\n",
      "Epoch [111/300], Step [60/384], Loss: 20.4240\n",
      "Epoch [111/300], Step [80/384], Loss: 1.6733\n",
      "Epoch [111/300], Step [100/384], Loss: 58.1576\n",
      "Epoch [111/300], Step [120/384], Loss: 103.3541\n",
      "Epoch [111/300], Step [140/384], Loss: 11.1610\n",
      "Epoch [111/300], Step [160/384], Loss: 38.0438\n",
      "Epoch [111/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [111/300], Step [200/384], Loss: 98.8368\n",
      "Epoch [111/300], Step [220/384], Loss: 32.6681\n",
      "Epoch [111/300], Step [240/384], Loss: 59.2336\n",
      "Epoch [111/300], Step [260/384], Loss: 89.1702\n",
      "Epoch [111/300], Step [280/384], Loss: 6.0721\n",
      "Epoch [111/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [111/300], Step [320/384], Loss: 149.6695\n",
      "Epoch [111/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [111/300], Step [360/384], Loss: 486.0431\n",
      "Epoch [111/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [120/384], Loss: 157.1503\n",
      "Epoch [112/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [200/384], Loss: 30.8615\n",
      "Epoch [112/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [380/384], Loss: 38.0616\n",
      "Epoch [113/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [113/300], Step [40/384], Loss: 71.5154\n",
      "Epoch [113/300], Step [60/384], Loss: 67.1816\n",
      "Epoch [113/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [113/300], Step [100/384], Loss: 0.0007\n",
      "Epoch [113/300], Step [120/384], Loss: 107.2286\n",
      "Epoch [113/300], Step [140/384], Loss: 90.4477\n",
      "Epoch [113/300], Step [160/384], Loss: 0.5625\n",
      "Epoch [113/300], Step [180/384], Loss: 3.9895\n",
      "Epoch [113/300], Step [200/384], Loss: 24.5607\n",
      "Epoch [113/300], Step [220/384], Loss: 33.4879\n",
      "Epoch [113/300], Step [240/384], Loss: 1.4976\n",
      "Epoch [113/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [113/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [113/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [113/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [113/300], Step [340/384], Loss: 18.5611\n",
      "Epoch [113/300], Step [360/384], Loss: 151.8505\n",
      "Epoch [113/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [114/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [114/300], Step [40/384], Loss: 144.5019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [114/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [114/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [114/300], Step [100/384], Loss: 7.8996\n",
      "Epoch [114/300], Step [120/384], Loss: 124.5019\n",
      "Epoch [114/300], Step [140/384], Loss: 53.5201\n",
      "Epoch [114/300], Step [160/384], Loss: 27.2429\n",
      "Epoch [114/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [114/300], Step [200/384], Loss: 6.4828\n",
      "Epoch [114/300], Step [220/384], Loss: 6.8568\n",
      "Epoch [114/300], Step [240/384], Loss: 123.1290\n",
      "Epoch [114/300], Step [260/384], Loss: 36.2354\n",
      "Epoch [114/300], Step [280/384], Loss: 106.1928\n",
      "Epoch [114/300], Step [300/384], Loss: 130.8138\n",
      "Epoch [114/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [114/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [114/300], Step [360/384], Loss: 6.1537\n",
      "Epoch [114/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [60/384], Loss: 51.0452\n",
      "Epoch [115/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [120/384], Loss: 165.5816\n",
      "Epoch [115/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [200/384], Loss: 52.6739\n",
      "Epoch [115/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [300/384], Loss: 10.8816\n",
      "Epoch [115/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [360/384], Loss: 36.5668\n",
      "Epoch [115/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [116/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [116/300], Step [40/384], Loss: 189.4986\n",
      "Epoch [116/300], Step [60/384], Loss: 134.4547\n",
      "Epoch [116/300], Step [80/384], Loss: 66.2621\n",
      "Epoch [116/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [116/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [116/300], Step [140/384], Loss: 21.5803\n",
      "Epoch [116/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [116/300], Step [180/384], Loss: 194.3006\n",
      "Epoch [116/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [116/300], Step [220/384], Loss: 57.6045\n",
      "Epoch [116/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [116/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [116/300], Step [280/384], Loss: 134.2526\n",
      "Epoch [116/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [116/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [116/300], Step [340/384], Loss: 0.0002\n",
      "Epoch [116/300], Step [360/384], Loss: 21.9283\n",
      "Epoch [116/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [117/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [117/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [117/300], Step [60/384], Loss: 246.9236\n",
      "Epoch [117/300], Step [80/384], Loss: 247.8909\n",
      "Epoch [117/300], Step [100/384], Loss: 67.6310\n",
      "Epoch [117/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [117/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [117/300], Step [160/384], Loss: 40.5089\n",
      "Epoch [117/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [117/300], Step [200/384], Loss: 294.9139\n",
      "Epoch [117/300], Step [220/384], Loss: 164.0087\n",
      "Epoch [117/300], Step [240/384], Loss: 2.9649\n",
      "Epoch [117/300], Step [260/384], Loss: 18.5657\n",
      "Epoch [117/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [117/300], Step [300/384], Loss: 140.8464\n",
      "Epoch [117/300], Step [320/384], Loss: 7.7690\n",
      "Epoch [117/300], Step [340/384], Loss: 12.6314\n",
      "Epoch [117/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [117/300], Step [380/384], Loss: 48.9585\n",
      "Epoch [118/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [118/300], Step [40/384], Loss: 2.5719\n",
      "Epoch [118/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [118/300], Step [80/384], Loss: 10.6458\n",
      "Epoch [118/300], Step [100/384], Loss: 130.1549\n",
      "Epoch [118/300], Step [120/384], Loss: 27.1474\n",
      "Epoch [118/300], Step [140/384], Loss: 278.4137\n",
      "Epoch [118/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [118/300], Step [180/384], Loss: 92.9162\n",
      "Epoch [118/300], Step [200/384], Loss: 63.3459\n",
      "Epoch [118/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [118/300], Step [240/384], Loss: 58.4540\n",
      "Epoch [118/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [118/300], Step [280/384], Loss: 45.0230\n",
      "Epoch [118/300], Step [300/384], Loss: 102.4636\n",
      "Epoch [118/300], Step [320/384], Loss: 107.3240\n",
      "Epoch [118/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [118/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [118/300], Step [380/384], Loss: 5.8417\n",
      "Epoch [119/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [119/300], Step [40/384], Loss: 14.9877\n",
      "Epoch [119/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [119/300], Step [80/384], Loss: 72.4324\n",
      "Epoch [119/300], Step [100/384], Loss: 47.9985\n",
      "Epoch [119/300], Step [120/384], Loss: 85.1695\n",
      "Epoch [119/300], Step [140/384], Loss: 76.0405\n",
      "Epoch [119/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [119/300], Step [180/384], Loss: 3.7541\n",
      "Epoch [119/300], Step [200/384], Loss: 59.2529\n",
      "Epoch [119/300], Step [220/384], Loss: 65.0061\n",
      "Epoch [119/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [119/300], Step [260/384], Loss: 25.3572\n",
      "Epoch [119/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [119/300], Step [300/384], Loss: 90.0544\n",
      "Epoch [119/300], Step [320/384], Loss: 151.1009\n",
      "Epoch [119/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [119/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [119/300], Step [380/384], Loss: 70.2504\n",
      "Epoch [120/300], Step [20/384], Loss: 15.0913\n",
      "Epoch [120/300], Step [40/384], Loss: 62.2257\n",
      "Epoch [120/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [80/384], Loss: 32.0639\n",
      "Epoch [120/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [140/384], Loss: 97.4911\n",
      "Epoch [120/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [200/384], Loss: 0.3719\n",
      "Epoch [120/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [380/384], Loss: 33.5153\n",
      "Epoch [121/300], Step [20/384], Loss: 132.2179\n",
      "Epoch [121/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [121/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [121/300], Step [80/384], Loss: 29.9547\n",
      "Epoch [121/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [121/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [121/300], Step [140/384], Loss: 109.4375\n",
      "Epoch [121/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [121/300], Step [180/384], Loss: 66.4779\n",
      "Epoch [121/300], Step [200/384], Loss: 58.6870\n",
      "Epoch [121/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [121/300], Step [240/384], Loss: 31.8073\n",
      "Epoch [121/300], Step [260/384], Loss: 40.6633\n",
      "Epoch [121/300], Step [280/384], Loss: 119.2653\n",
      "Epoch [121/300], Step [300/384], Loss: 0.0026\n",
      "Epoch [121/300], Step [320/384], Loss: 220.5701\n",
      "Epoch [121/300], Step [340/384], Loss: 14.6218\n",
      "Epoch [121/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [121/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [20/384], Loss: 94.0253\n",
      "Epoch [122/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [140/384], Loss: 11.1603\n",
      "Epoch [122/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [180/384], Loss: 31.1244\n",
      "Epoch [122/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [280/384], Loss: 92.3503\n",
      "Epoch [122/300], Step [300/384], Loss: 100.5631\n",
      "Epoch [122/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [340/384], Loss: 104.8171\n",
      "Epoch [122/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [380/384], Loss: 97.4870\n",
      "Epoch [123/300], Step [20/384], Loss: 91.4769\n",
      "Epoch [123/300], Step [40/384], Loss: 144.5107\n",
      "Epoch [123/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [123/300], Step [80/384], Loss: 44.4655\n",
      "Epoch [123/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [123/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [123/300], Step [140/384], Loss: 88.3906\n",
      "Epoch [123/300], Step [160/384], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [123/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [123/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [123/300], Step [220/384], Loss: 88.3662\n",
      "Epoch [123/300], Step [240/384], Loss: 28.3629\n",
      "Epoch [123/300], Step [260/384], Loss: 152.9207\n",
      "Epoch [123/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [123/300], Step [300/384], Loss: 16.3721\n",
      "Epoch [123/300], Step [320/384], Loss: 78.1230\n",
      "Epoch [123/300], Step [340/384], Loss: 13.6447\n",
      "Epoch [123/300], Step [360/384], Loss: 120.4252\n",
      "Epoch [123/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [20/384], Loss: 64.8374\n",
      "Epoch [124/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [60/384], Loss: 115.9414\n",
      "Epoch [124/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [160/384], Loss: 61.7797\n",
      "Epoch [124/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [240/384], Loss: 0.0057\n",
      "Epoch [124/300], Step [260/384], Loss: 3.6459\n",
      "Epoch [124/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [300/384], Loss: 240.3924\n",
      "Epoch [124/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [380/384], Loss: 117.9233\n",
      "Epoch [125/300], Step [20/384], Loss: 236.4170\n",
      "Epoch [125/300], Step [40/384], Loss: 168.2220\n",
      "Epoch [125/300], Step [60/384], Loss: 40.9100\n",
      "Epoch [125/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [125/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [125/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [125/300], Step [140/384], Loss: 254.8585\n",
      "Epoch [125/300], Step [160/384], Loss: 41.3471\n",
      "Epoch [125/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [125/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [125/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [125/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [125/300], Step [260/384], Loss: 5.4270\n",
      "Epoch [125/300], Step [280/384], Loss: 26.9302\n",
      "Epoch [125/300], Step [300/384], Loss: 70.9323\n",
      "Epoch [125/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [125/300], Step [340/384], Loss: 161.2856\n",
      "Epoch [125/300], Step [360/384], Loss: 67.7391\n",
      "Epoch [125/300], Step [380/384], Loss: 25.5786\n",
      "Epoch [126/300], Step [20/384], Loss: 50.8153\n",
      "Epoch [126/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [126/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [126/300], Step [80/384], Loss: 31.2662\n",
      "Epoch [126/300], Step [100/384], Loss: 81.7713\n",
      "Epoch [126/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [126/300], Step [140/384], Loss: 2.5369\n",
      "Epoch [126/300], Step [160/384], Loss: 17.6021\n",
      "Epoch [126/300], Step [180/384], Loss: 14.4618\n",
      "Epoch [126/300], Step [200/384], Loss: 32.4454\n",
      "Epoch [126/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [126/300], Step [240/384], Loss: 13.7615\n",
      "Epoch [126/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [126/300], Step [280/384], Loss: 78.7735\n",
      "Epoch [126/300], Step [300/384], Loss: 191.3715\n",
      "Epoch [126/300], Step [320/384], Loss: 16.8699\n",
      "Epoch [126/300], Step [340/384], Loss: 1.5629\n",
      "Epoch [126/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [126/300], Step [380/384], Loss: 28.2952\n",
      "Epoch [127/300], Step [20/384], Loss: 46.6640\n",
      "Epoch [127/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [127/300], Step [60/384], Loss: 2.4377\n",
      "Epoch [127/300], Step [80/384], Loss: 85.6883\n",
      "Epoch [127/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [127/300], Step [120/384], Loss: 84.8845\n",
      "Epoch [127/300], Step [140/384], Loss: 111.1540\n",
      "Epoch [127/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [127/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [127/300], Step [200/384], Loss: 275.5909\n",
      "Epoch [127/300], Step [220/384], Loss: 152.1347\n",
      "Epoch [127/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [127/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [127/300], Step [280/384], Loss: 92.3588\n",
      "Epoch [127/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [127/300], Step [320/384], Loss: 377.7129\n",
      "Epoch [127/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [127/300], Step [360/384], Loss: 237.5261\n",
      "Epoch [127/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [128/300], Step [20/384], Loss: 68.9289\n",
      "Epoch [128/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [128/300], Step [60/384], Loss: 11.9622\n",
      "Epoch [128/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [128/300], Step [100/384], Loss: 148.4144\n",
      "Epoch [128/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [128/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [128/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [128/300], Step [180/384], Loss: 68.1163\n",
      "Epoch [128/300], Step [200/384], Loss: 91.5340\n",
      "Epoch [128/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [128/300], Step [240/384], Loss: 90.5221\n",
      "Epoch [128/300], Step [260/384], Loss: 126.7219\n",
      "Epoch [128/300], Step [280/384], Loss: 34.8715\n",
      "Epoch [128/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [128/300], Step [320/384], Loss: 134.3930\n",
      "Epoch [128/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [128/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [128/300], Step [380/384], Loss: 34.5776\n",
      "Epoch [129/300], Step [20/384], Loss: 35.2360\n",
      "Epoch [129/300], Step [40/384], Loss: 57.1519\n",
      "Epoch [129/300], Step [60/384], Loss: 130.1262\n",
      "Epoch [129/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [129/300], Step [100/384], Loss: 312.0207\n",
      "Epoch [129/300], Step [120/384], Loss: 40.3370\n",
      "Epoch [129/300], Step [140/384], Loss: 22.8289\n",
      "Epoch [129/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [129/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [129/300], Step [200/384], Loss: 115.9660\n",
      "Epoch [129/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [129/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [129/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [129/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [129/300], Step [300/384], Loss: 1.1724\n",
      "Epoch [129/300], Step [320/384], Loss: 97.7551\n",
      "Epoch [129/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [129/300], Step [360/384], Loss: 48.5521\n",
      "Epoch [129/300], Step [380/384], Loss: 162.4081\n",
      "Epoch [130/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [130/300], Step [40/384], Loss: 92.9364\n",
      "Epoch [130/300], Step [60/384], Loss: 71.2252\n",
      "Epoch [130/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [130/300], Step [100/384], Loss: 78.3253\n",
      "Epoch [130/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [130/300], Step [140/384], Loss: 53.7846\n",
      "Epoch [130/300], Step [160/384], Loss: 31.5455\n",
      "Epoch [130/300], Step [180/384], Loss: 64.2886\n",
      "Epoch [130/300], Step [200/384], Loss: 0.0001\n",
      "Epoch [130/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [130/300], Step [240/384], Loss: 19.2746\n",
      "Epoch [130/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [130/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [130/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [130/300], Step [320/384], Loss: 30.3863\n",
      "Epoch [130/300], Step [340/384], Loss: 18.0411\n",
      "Epoch [130/300], Step [360/384], Loss: 58.7062\n",
      "Epoch [130/300], Step [380/384], Loss: 88.4442\n",
      "Epoch [131/300], Step [20/384], Loss: 125.5162\n",
      "Epoch [131/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [60/384], Loss: 8.6907\n",
      "Epoch [131/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [220/384], Loss: 58.7414\n",
      "Epoch [131/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [260/384], Loss: 93.3607\n",
      "Epoch [131/300], Step [280/384], Loss: 207.5687\n",
      "Epoch [131/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [380/384], Loss: 1.0572\n",
      "Epoch [132/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [132/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [132/300], Step [60/384], Loss: 69.3723\n",
      "Epoch [132/300], Step [80/384], Loss: 87.2521\n",
      "Epoch [132/300], Step [100/384], Loss: 83.5445\n",
      "Epoch [132/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [132/300], Step [140/384], Loss: 37.8810\n",
      "Epoch [132/300], Step [160/384], Loss: 2.1558\n",
      "Epoch [132/300], Step [180/384], Loss: 237.9416\n",
      "Epoch [132/300], Step [200/384], Loss: 56.6283\n",
      "Epoch [132/300], Step [220/384], Loss: 16.1208\n",
      "Epoch [132/300], Step [240/384], Loss: 19.0956\n",
      "Epoch [132/300], Step [260/384], Loss: 103.3876\n",
      "Epoch [132/300], Step [280/384], Loss: 82.7089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [132/300], Step [300/384], Loss: 17.1039\n",
      "Epoch [132/300], Step [320/384], Loss: 92.0448\n",
      "Epoch [132/300], Step [340/384], Loss: 80.7104\n",
      "Epoch [132/300], Step [360/384], Loss: 62.2505\n",
      "Epoch [132/300], Step [380/384], Loss: 6.1980\n",
      "Epoch [133/300], Step [20/384], Loss: 22.1086\n",
      "Epoch [133/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [133/300], Step [60/384], Loss: 1.3628\n",
      "Epoch [133/300], Step [80/384], Loss: 13.2318\n",
      "Epoch [133/300], Step [100/384], Loss: 35.4767\n",
      "Epoch [133/300], Step [120/384], Loss: 43.0004\n",
      "Epoch [133/300], Step [140/384], Loss: 95.8229\n",
      "Epoch [133/300], Step [160/384], Loss: 16.4707\n",
      "Epoch [133/300], Step [180/384], Loss: 9.0379\n",
      "Epoch [133/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [133/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [133/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [133/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [133/300], Step [280/384], Loss: 176.4031\n",
      "Epoch [133/300], Step [300/384], Loss: 26.9045\n",
      "Epoch [133/300], Step [320/384], Loss: 14.1849\n",
      "Epoch [133/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [133/300], Step [360/384], Loss: 12.1197\n",
      "Epoch [133/300], Step [380/384], Loss: 32.8970\n",
      "Epoch [134/300], Step [20/384], Loss: 7.2696\n",
      "Epoch [134/300], Step [40/384], Loss: 0.0861\n",
      "Epoch [134/300], Step [60/384], Loss: 30.0358\n",
      "Epoch [134/300], Step [80/384], Loss: 98.0230\n",
      "Epoch [134/300], Step [100/384], Loss: 63.5146\n",
      "Epoch [134/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [134/300], Step [140/384], Loss: 62.9989\n",
      "Epoch [134/300], Step [160/384], Loss: 92.7377\n",
      "Epoch [134/300], Step [180/384], Loss: 135.9933\n",
      "Epoch [134/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [134/300], Step [220/384], Loss: 52.9033\n",
      "Epoch [134/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [134/300], Step [260/384], Loss: 99.6300\n",
      "Epoch [134/300], Step [280/384], Loss: 39.3594\n",
      "Epoch [134/300], Step [300/384], Loss: 28.5861\n",
      "Epoch [134/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [134/300], Step [340/384], Loss: 2.6210\n",
      "Epoch [134/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [134/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [135/300], Step [20/384], Loss: 46.9901\n",
      "Epoch [135/300], Step [40/384], Loss: 44.8953\n",
      "Epoch [135/300], Step [60/384], Loss: 21.1280\n",
      "Epoch [135/300], Step [80/384], Loss: 66.9142\n",
      "Epoch [135/300], Step [100/384], Loss: 155.6039\n",
      "Epoch [135/300], Step [120/384], Loss: 269.4358\n",
      "Epoch [135/300], Step [140/384], Loss: 81.8875\n",
      "Epoch [135/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [135/300], Step [180/384], Loss: 69.9670\n",
      "Epoch [135/300], Step [200/384], Loss: 102.7989\n",
      "Epoch [135/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [135/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [135/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [135/300], Step [280/384], Loss: 360.6921\n",
      "Epoch [135/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [135/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [135/300], Step [340/384], Loss: 10.5510\n",
      "Epoch [135/300], Step [360/384], Loss: 56.5536\n",
      "Epoch [135/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [40/384], Loss: 76.3566\n",
      "Epoch [136/300], Step [60/384], Loss: 14.8083\n",
      "Epoch [136/300], Step [80/384], Loss: 70.4821\n",
      "Epoch [136/300], Step [100/384], Loss: 100.3015\n",
      "Epoch [136/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [220/384], Loss: 61.0855\n",
      "Epoch [136/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [300/384], Loss: 88.6672\n",
      "Epoch [136/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [360/384], Loss: 130.8455\n",
      "Epoch [136/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [20/384], Loss: 0.7004\n",
      "Epoch [137/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [80/384], Loss: 97.5442\n",
      "Epoch [137/300], Step [100/384], Loss: 42.9693\n",
      "Epoch [137/300], Step [120/384], Loss: 41.3597\n",
      "Epoch [137/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [200/384], Loss: 20.4618\n",
      "Epoch [137/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [240/384], Loss: 95.0204\n",
      "Epoch [137/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [320/384], Loss: 2.4265\n",
      "Epoch [137/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [380/384], Loss: 59.4110\n",
      "Epoch [138/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [138/300], Step [40/384], Loss: 74.7115\n",
      "Epoch [138/300], Step [60/384], Loss: 93.1198\n",
      "Epoch [138/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [138/300], Step [100/384], Loss: 19.6151\n",
      "Epoch [138/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [138/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [138/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [138/300], Step [180/384], Loss: 21.6987\n",
      "Epoch [138/300], Step [200/384], Loss: 95.8628\n",
      "Epoch [138/300], Step [220/384], Loss: 94.7796\n",
      "Epoch [138/300], Step [240/384], Loss: 30.2115\n",
      "Epoch [138/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [138/300], Step [280/384], Loss: 118.7533\n",
      "Epoch [138/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [138/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [138/300], Step [340/384], Loss: 143.2957\n",
      "Epoch [138/300], Step [360/384], Loss: 39.0904\n",
      "Epoch [138/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [139/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [139/300], Step [40/384], Loss: 134.0055\n",
      "Epoch [139/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [139/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [139/300], Step [100/384], Loss: 34.4969\n",
      "Epoch [139/300], Step [120/384], Loss: 130.4041\n",
      "Epoch [139/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [139/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [139/300], Step [180/384], Loss: 127.7957\n",
      "Epoch [139/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [139/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [139/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [139/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [139/300], Step [280/384], Loss: 83.4455\n",
      "Epoch [139/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [139/300], Step [320/384], Loss: 174.4127\n",
      "Epoch [139/300], Step [340/384], Loss: 21.4917\n",
      "Epoch [139/300], Step [360/384], Loss: 19.4958\n",
      "Epoch [139/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [140/300], Step [20/384], Loss: 71.7616\n",
      "Epoch [140/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [140/300], Step [60/384], Loss: 43.3677\n",
      "Epoch [140/300], Step [80/384], Loss: 22.6287\n",
      "Epoch [140/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [140/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [140/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [140/300], Step [160/384], Loss: 103.3448\n",
      "Epoch [140/300], Step [180/384], Loss: 14.3964\n",
      "Epoch [140/300], Step [200/384], Loss: 31.9149\n",
      "Epoch [140/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [140/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [140/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [140/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [140/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [140/300], Step [320/384], Loss: 50.5277\n",
      "Epoch [140/300], Step [340/384], Loss: 68.0798\n",
      "Epoch [140/300], Step [360/384], Loss: 55.2573\n",
      "Epoch [140/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [141/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [141/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [141/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [141/300], Step [80/384], Loss: 103.9211\n",
      "Epoch [141/300], Step [100/384], Loss: 171.6582\n",
      "Epoch [141/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [141/300], Step [140/384], Loss: 49.6269\n",
      "Epoch [141/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [141/300], Step [180/384], Loss: 162.7660\n",
      "Epoch [141/300], Step [200/384], Loss: 19.2747\n",
      "Epoch [141/300], Step [220/384], Loss: 86.4363\n",
      "Epoch [141/300], Step [240/384], Loss: 112.7025\n",
      "Epoch [141/300], Step [260/384], Loss: 86.9685\n",
      "Epoch [141/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [141/300], Step [300/384], Loss: 28.6568\n",
      "Epoch [141/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [141/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [141/300], Step [360/384], Loss: 65.0685\n",
      "Epoch [141/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [20/384], Loss: 64.5537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [142/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [60/384], Loss: 12.5449\n",
      "Epoch [142/300], Step [80/384], Loss: 192.1929\n",
      "Epoch [142/300], Step [100/384], Loss: 31.3975\n",
      "Epoch [142/300], Step [120/384], Loss: 79.5724\n",
      "Epoch [142/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [160/384], Loss: 6.7179\n",
      "Epoch [142/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [320/384], Loss: 0.3007\n",
      "Epoch [142/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [143/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [143/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [143/300], Step [60/384], Loss: 35.1085\n",
      "Epoch [143/300], Step [80/384], Loss: 21.4054\n",
      "Epoch [143/300], Step [100/384], Loss: 87.6762\n",
      "Epoch [143/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [143/300], Step [140/384], Loss: 256.5464\n",
      "Epoch [143/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [143/300], Step [180/384], Loss: 122.4275\n",
      "Epoch [143/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [143/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [143/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [143/300], Step [260/384], Loss: 153.5206\n",
      "Epoch [143/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [143/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [143/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [143/300], Step [340/384], Loss: 98.5045\n",
      "Epoch [143/300], Step [360/384], Loss: 2.1332\n",
      "Epoch [143/300], Step [380/384], Loss: 11.2849\n",
      "Epoch [144/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [144/300], Step [40/384], Loss: 17.2068\n",
      "Epoch [144/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [144/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [144/300], Step [100/384], Loss: 23.0742\n",
      "Epoch [144/300], Step [120/384], Loss: 46.5077\n",
      "Epoch [144/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [144/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [144/300], Step [180/384], Loss: 101.2035\n",
      "Epoch [144/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [144/300], Step [220/384], Loss: 84.7562\n",
      "Epoch [144/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [144/300], Step [260/384], Loss: 134.3900\n",
      "Epoch [144/300], Step [280/384], Loss: 15.7234\n",
      "Epoch [144/300], Step [300/384], Loss: 47.9924\n",
      "Epoch [144/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [144/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [144/300], Step [360/384], Loss: 9.2570\n",
      "Epoch [144/300], Step [380/384], Loss: 146.1013\n",
      "Epoch [145/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [120/384], Loss: 7.4511\n",
      "Epoch [145/300], Step [140/384], Loss: 102.3122\n",
      "Epoch [145/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [180/384], Loss: 82.2172\n",
      "Epoch [145/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [220/384], Loss: 81.7306\n",
      "Epoch [145/300], Step [240/384], Loss: 32.8217\n",
      "Epoch [145/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [280/384], Loss: 97.0748\n",
      "Epoch [145/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [40/384], Loss: 31.5609\n",
      "Epoch [146/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [120/384], Loss: 104.7340\n",
      "Epoch [146/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [200/384], Loss: 78.7272\n",
      "Epoch [146/300], Step [220/384], Loss: 87.6396\n",
      "Epoch [146/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [260/384], Loss: 115.7288\n",
      "Epoch [146/300], Step [280/384], Loss: 13.1013\n",
      "Epoch [146/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [180/384], Loss: 72.7958\n",
      "Epoch [147/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [220/384], Loss: 142.4364\n",
      "Epoch [147/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [280/384], Loss: 23.0325\n",
      "Epoch [147/300], Step [300/384], Loss: 82.2201\n",
      "Epoch [147/300], Step [320/384], Loss: 9.6200\n",
      "Epoch [147/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [360/384], Loss: 13.5532\n",
      "Epoch [147/300], Step [380/384], Loss: 30.9145\n",
      "Epoch [148/300], Step [20/384], Loss: 47.2047\n",
      "Epoch [148/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [260/384], Loss: 4.5318\n",
      "Epoch [148/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [340/384], Loss: 10.6583\n",
      "Epoch [148/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [380/384], Loss: 65.1032\n",
      "Epoch [149/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [149/300], Step [40/384], Loss: 68.5076\n",
      "Epoch [149/300], Step [60/384], Loss: 199.6782\n",
      "Epoch [149/300], Step [80/384], Loss: 335.9071\n",
      "Epoch [149/300], Step [100/384], Loss: 70.7929\n",
      "Epoch [149/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [149/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [149/300], Step [160/384], Loss: 62.3714\n",
      "Epoch [149/300], Step [180/384], Loss: 141.2962\n",
      "Epoch [149/300], Step [200/384], Loss: 58.0084\n",
      "Epoch [149/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [149/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [149/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [149/300], Step [280/384], Loss: 156.7125\n",
      "Epoch [149/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [149/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [149/300], Step [340/384], Loss: 47.2918\n",
      "Epoch [149/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [149/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [150/300], Step [20/384], Loss: 40.2254\n",
      "Epoch [150/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [150/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [150/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [150/300], Step [100/384], Loss: 118.8955\n",
      "Epoch [150/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [150/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [150/300], Step [160/384], Loss: 54.4890\n",
      "Epoch [150/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [150/300], Step [200/384], Loss: 16.0335\n",
      "Epoch [150/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [150/300], Step [240/384], Loss: 94.2004\n",
      "Epoch [150/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [150/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [150/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [150/300], Step [320/384], Loss: 2.1679\n",
      "Epoch [150/300], Step [340/384], Loss: 55.1414\n",
      "Epoch [150/300], Step [360/384], Loss: 40.7761\n",
      "Epoch [150/300], Step [380/384], Loss: 41.9004\n",
      "Epoch [151/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [151/300], Step [40/384], Loss: 277.7157\n",
      "Epoch [151/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [151/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [151/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [151/300], Step [120/384], Loss: 62.2382\n",
      "Epoch [151/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [151/300], Step [160/384], Loss: 9.7044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [151/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [151/300], Step [200/384], Loss: 76.6071\n",
      "Epoch [151/300], Step [220/384], Loss: 14.0842\n",
      "Epoch [151/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [151/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [151/300], Step [280/384], Loss: 68.9109\n",
      "Epoch [151/300], Step [300/384], Loss: 19.7474\n",
      "Epoch [151/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [151/300], Step [340/384], Loss: 212.8303\n",
      "Epoch [151/300], Step [360/384], Loss: 77.5794\n",
      "Epoch [151/300], Step [380/384], Loss: 49.7297\n",
      "Epoch [152/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [152/300], Step [40/384], Loss: 138.0891\n",
      "Epoch [152/300], Step [60/384], Loss: 210.0233\n",
      "Epoch [152/300], Step [80/384], Loss: 48.9976\n",
      "Epoch [152/300], Step [100/384], Loss: 98.3217\n",
      "Epoch [152/300], Step [120/384], Loss: 144.9498\n",
      "Epoch [152/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [152/300], Step [160/384], Loss: 122.2548\n",
      "Epoch [152/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [152/300], Step [200/384], Loss: 1.8128\n",
      "Epoch [152/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [152/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [152/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [152/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [152/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [152/300], Step [320/384], Loss: 99.9719\n",
      "Epoch [152/300], Step [340/384], Loss: 19.2558\n",
      "Epoch [152/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [152/300], Step [380/384], Loss: 85.7670\n",
      "Epoch [153/300], Step [20/384], Loss: 93.2559\n",
      "Epoch [153/300], Step [40/384], Loss: 111.9173\n",
      "Epoch [153/300], Step [60/384], Loss: 10.3465\n",
      "Epoch [153/300], Step [80/384], Loss: 18.3962\n",
      "Epoch [153/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [153/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [153/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [153/300], Step [160/384], Loss: 19.6624\n",
      "Epoch [153/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [153/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [153/300], Step [220/384], Loss: 79.0462\n",
      "Epoch [153/300], Step [240/384], Loss: 124.1085\n",
      "Epoch [153/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [153/300], Step [280/384], Loss: 109.8317\n",
      "Epoch [153/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [153/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [153/300], Step [340/384], Loss: 290.5719\n",
      "Epoch [153/300], Step [360/384], Loss: 10.2883\n",
      "Epoch [153/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [154/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [154/300], Step [40/384], Loss: 39.3566\n",
      "Epoch [154/300], Step [60/384], Loss: 2.7412\n",
      "Epoch [154/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [154/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [154/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [154/300], Step [140/384], Loss: 125.0055\n",
      "Epoch [154/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [154/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [154/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [154/300], Step [220/384], Loss: 208.8940\n",
      "Epoch [154/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [154/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [154/300], Step [280/384], Loss: 148.5391\n",
      "Epoch [154/300], Step [300/384], Loss: 127.3157\n",
      "Epoch [154/300], Step [320/384], Loss: 138.8237\n",
      "Epoch [154/300], Step [340/384], Loss: 39.9758\n",
      "Epoch [154/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [154/300], Step [380/384], Loss: 84.4258\n",
      "Epoch [155/300], Step [20/384], Loss: 10.2348\n",
      "Epoch [155/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [60/384], Loss: 4.1403\n",
      "Epoch [155/300], Step [80/384], Loss: 34.5804\n",
      "Epoch [155/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [120/384], Loss: 145.2737\n",
      "Epoch [155/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [160/384], Loss: 86.5055\n",
      "Epoch [155/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [340/384], Loss: 32.5454\n",
      "Epoch [155/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [156/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [156/300], Step [40/384], Loss: 44.3065\n",
      "Epoch [156/300], Step [60/384], Loss: 89.8918\n",
      "Epoch [156/300], Step [80/384], Loss: 63.2362\n",
      "Epoch [156/300], Step [100/384], Loss: 74.8260\n",
      "Epoch [156/300], Step [120/384], Loss: 300.3027\n",
      "Epoch [156/300], Step [140/384], Loss: 21.3530\n",
      "Epoch [156/300], Step [160/384], Loss: 57.2164\n",
      "Epoch [156/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [156/300], Step [200/384], Loss: 68.5259\n",
      "Epoch [156/300], Step [220/384], Loss: 12.4116\n",
      "Epoch [156/300], Step [240/384], Loss: 145.8539\n",
      "Epoch [156/300], Step [260/384], Loss: 23.8395\n",
      "Epoch [156/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [156/300], Step [300/384], Loss: 93.8936\n",
      "Epoch [156/300], Step [320/384], Loss: 32.4656\n",
      "Epoch [156/300], Step [340/384], Loss: 5.8254\n",
      "Epoch [156/300], Step [360/384], Loss: 44.5769\n",
      "Epoch [156/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [157/300], Step [20/384], Loss: 48.9246\n",
      "Epoch [157/300], Step [40/384], Loss: 0.6296\n",
      "Epoch [157/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [157/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [157/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [157/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [157/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [157/300], Step [160/384], Loss: 117.2006\n",
      "Epoch [157/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [157/300], Step [200/384], Loss: 27.6163\n",
      "Epoch [157/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [157/300], Step [240/384], Loss: 0.2012\n",
      "Epoch [157/300], Step [260/384], Loss: 64.2891\n",
      "Epoch [157/300], Step [280/384], Loss: 49.4550\n",
      "Epoch [157/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [157/300], Step [320/384], Loss: 27.0960\n",
      "Epoch [157/300], Step [340/384], Loss: 16.0205\n",
      "Epoch [157/300], Step [360/384], Loss: 76.6633\n",
      "Epoch [157/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [60/384], Loss: 133.1592\n",
      "Epoch [158/300], Step [80/384], Loss: 112.0204\n",
      "Epoch [158/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [180/384], Loss: 25.5817\n",
      "Epoch [158/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [240/384], Loss: 76.3164\n",
      "Epoch [158/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [300/384], Loss: 78.2935\n",
      "Epoch [158/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [340/384], Loss: 28.1006\n",
      "Epoch [158/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [380/384], Loss: 21.2680\n",
      "Epoch [159/300], Step [20/384], Loss: 32.6972\n",
      "Epoch [159/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [159/300], Step [60/384], Loss: 60.8468\n",
      "Epoch [159/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [159/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [159/300], Step [120/384], Loss: 53.7954\n",
      "Epoch [159/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [159/300], Step [160/384], Loss: 130.6855\n",
      "Epoch [159/300], Step [180/384], Loss: 63.6692\n",
      "Epoch [159/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [159/300], Step [220/384], Loss: 53.0741\n",
      "Epoch [159/300], Step [240/384], Loss: 115.4742\n",
      "Epoch [159/300], Step [260/384], Loss: 229.9058\n",
      "Epoch [159/300], Step [280/384], Loss: 45.2109\n",
      "Epoch [159/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [159/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [159/300], Step [340/384], Loss: 115.4946\n",
      "Epoch [159/300], Step [360/384], Loss: 45.7264\n",
      "Epoch [159/300], Step [380/384], Loss: 51.5060\n",
      "Epoch [160/300], Step [20/384], Loss: 50.1427\n",
      "Epoch [160/300], Step [40/384], Loss: 244.2474\n",
      "Epoch [160/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [160/300], Step [80/384], Loss: 84.5985\n",
      "Epoch [160/300], Step [100/384], Loss: 152.8909\n",
      "Epoch [160/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [160/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [160/300], Step [160/384], Loss: 87.7717\n",
      "Epoch [160/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [160/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [160/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [160/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [160/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [160/300], Step [280/384], Loss: 95.1757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/300], Step [300/384], Loss: 62.6444\n",
      "Epoch [160/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [160/300], Step [340/384], Loss: 176.6499\n",
      "Epoch [160/300], Step [360/384], Loss: 80.1535\n",
      "Epoch [160/300], Step [380/384], Loss: 35.0909\n",
      "Epoch [161/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [161/300], Step [40/384], Loss: 70.8861\n",
      "Epoch [161/300], Step [60/384], Loss: 34.3290\n",
      "Epoch [161/300], Step [80/384], Loss: 69.3589\n",
      "Epoch [161/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [161/300], Step [120/384], Loss: 60.4135\n",
      "Epoch [161/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [161/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [161/300], Step [180/384], Loss: 24.4164\n",
      "Epoch [161/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [161/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [161/300], Step [240/384], Loss: 189.5427\n",
      "Epoch [161/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [161/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [161/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [161/300], Step [320/384], Loss: 7.6498\n",
      "Epoch [161/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [161/300], Step [360/384], Loss: 62.6016\n",
      "Epoch [161/300], Step [380/384], Loss: 104.1482\n",
      "Epoch [162/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [162/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [162/300], Step [60/384], Loss: 14.8711\n",
      "Epoch [162/300], Step [80/384], Loss: 54.8937\n",
      "Epoch [162/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [162/300], Step [120/384], Loss: 31.7532\n",
      "Epoch [162/300], Step [140/384], Loss: 17.2883\n",
      "Epoch [162/300], Step [160/384], Loss: 14.2525\n",
      "Epoch [162/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [162/300], Step [200/384], Loss: 77.3977\n",
      "Epoch [162/300], Step [220/384], Loss: 3.5144\n",
      "Epoch [162/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [162/300], Step [260/384], Loss: 40.3006\n",
      "Epoch [162/300], Step [280/384], Loss: 42.5911\n",
      "Epoch [162/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [162/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [162/300], Step [340/384], Loss: 62.5730\n",
      "Epoch [162/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [162/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [40/384], Loss: 175.8138\n",
      "Epoch [163/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [80/384], Loss: 168.6170\n",
      "Epoch [163/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [160/384], Loss: 81.1200\n",
      "Epoch [163/300], Step [180/384], Loss: 117.4156\n",
      "Epoch [163/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [240/384], Loss: 326.7500\n",
      "Epoch [163/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [300/384], Loss: 34.1677\n",
      "Epoch [163/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [360/384], Loss: 13.9432\n",
      "Epoch [163/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [164/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [164/300], Step [40/384], Loss: 85.7935\n",
      "Epoch [164/300], Step [60/384], Loss: 31.2702\n",
      "Epoch [164/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [164/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [164/300], Step [120/384], Loss: 25.7433\n",
      "Epoch [164/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [164/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [164/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [164/300], Step [200/384], Loss: 74.0078\n",
      "Epoch [164/300], Step [220/384], Loss: 126.7151\n",
      "Epoch [164/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [164/300], Step [260/384], Loss: 38.1629\n",
      "Epoch [164/300], Step [280/384], Loss: 65.7379\n",
      "Epoch [164/300], Step [300/384], Loss: 120.7165\n",
      "Epoch [164/300], Step [320/384], Loss: 3.5030\n",
      "Epoch [164/300], Step [340/384], Loss: 108.5022\n",
      "Epoch [164/300], Step [360/384], Loss: 100.3351\n",
      "Epoch [164/300], Step [380/384], Loss: 83.7333\n",
      "Epoch [165/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [60/384], Loss: 26.3569\n",
      "Epoch [165/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [140/384], Loss: 0.0046\n",
      "Epoch [165/300], Step [160/384], Loss: 62.5051\n",
      "Epoch [165/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [240/384], Loss: 279.3412\n",
      "Epoch [165/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [300/384], Loss: 62.5405\n",
      "Epoch [165/300], Step [320/384], Loss: 106.3454\n",
      "Epoch [165/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [380/384], Loss: 8.7459\n",
      "Epoch [166/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [200/384], Loss: 50.5860\n",
      "Epoch [166/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [280/384], Loss: 33.3041\n",
      "Epoch [166/300], Step [300/384], Loss: 133.5946\n",
      "Epoch [166/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [360/384], Loss: 16.9448\n",
      "Epoch [166/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [167/300], Step [20/384], Loss: 0.0004\n",
      "Epoch [167/300], Step [40/384], Loss: 102.7996\n",
      "Epoch [167/300], Step [60/384], Loss: 30.2087\n",
      "Epoch [167/300], Step [80/384], Loss: 37.0848\n",
      "Epoch [167/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [167/300], Step [120/384], Loss: 46.4092\n",
      "Epoch [167/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [167/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [167/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [167/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [167/300], Step [220/384], Loss: 51.9637\n",
      "Epoch [167/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [167/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [167/300], Step [280/384], Loss: 69.8141\n",
      "Epoch [167/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [167/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [167/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [167/300], Step [360/384], Loss: 178.6585\n",
      "Epoch [167/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [100/384], Loss: 110.3217\n",
      "Epoch [168/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [140/384], Loss: 127.5203\n",
      "Epoch [168/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [220/384], Loss: 202.6714\n",
      "Epoch [168/300], Step [240/384], Loss: 100.2216\n",
      "Epoch [168/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [340/384], Loss: 80.0264\n",
      "Epoch [168/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [169/300], Step [20/384], Loss: 6.4382\n",
      "Epoch [169/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [169/300], Step [60/384], Loss: 21.6040\n",
      "Epoch [169/300], Step [80/384], Loss: 63.3695\n",
      "Epoch [169/300], Step [100/384], Loss: 77.2961\n",
      "Epoch [169/300], Step [120/384], Loss: 51.4139\n",
      "Epoch [169/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [169/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [169/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [169/300], Step [200/384], Loss: 63.6447\n",
      "Epoch [169/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [169/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [169/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [169/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [169/300], Step [300/384], Loss: 80.9715\n",
      "Epoch [169/300], Step [320/384], Loss: 50.0839\n",
      "Epoch [169/300], Step [340/384], Loss: 55.4441\n",
      "Epoch [169/300], Step [360/384], Loss: 75.5243\n",
      "Epoch [169/300], Step [380/384], Loss: 2.0761\n",
      "Epoch [170/300], Step [20/384], Loss: 118.0931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [170/300], Step [60/384], Loss: 21.4192\n",
      "Epoch [170/300], Step [80/384], Loss: 55.0193\n",
      "Epoch [170/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [170/300], Step [120/384], Loss: 20.9745\n",
      "Epoch [170/300], Step [140/384], Loss: 110.0933\n",
      "Epoch [170/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [170/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [170/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [170/300], Step [220/384], Loss: 13.5239\n",
      "Epoch [170/300], Step [240/384], Loss: 58.0659\n",
      "Epoch [170/300], Step [260/384], Loss: 69.5065\n",
      "Epoch [170/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [170/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [170/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [170/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [170/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [170/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [171/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [171/300], Step [40/384], Loss: 61.7528\n",
      "Epoch [171/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [171/300], Step [80/384], Loss: 32.2125\n",
      "Epoch [171/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [171/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [171/300], Step [140/384], Loss: 241.3755\n",
      "Epoch [171/300], Step [160/384], Loss: 210.2234\n",
      "Epoch [171/300], Step [180/384], Loss: 121.1546\n",
      "Epoch [171/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [171/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [171/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [171/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [171/300], Step [280/384], Loss: 40.3313\n",
      "Epoch [171/300], Step [300/384], Loss: 82.6238\n",
      "Epoch [171/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [171/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [171/300], Step [360/384], Loss: 67.7184\n",
      "Epoch [171/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [172/300], Step [20/384], Loss: 61.6753\n",
      "Epoch [172/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [172/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [172/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [172/300], Step [100/384], Loss: 48.2366\n",
      "Epoch [172/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [172/300], Step [140/384], Loss: 116.8444\n",
      "Epoch [172/300], Step [160/384], Loss: 8.1600\n",
      "Epoch [172/300], Step [180/384], Loss: 17.7960\n",
      "Epoch [172/300], Step [200/384], Loss: 31.9575\n",
      "Epoch [172/300], Step [220/384], Loss: 190.0090\n",
      "Epoch [172/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [172/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [172/300], Step [280/384], Loss: 64.2096\n",
      "Epoch [172/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [172/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [172/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [172/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [172/300], Step [380/384], Loss: 126.7630\n",
      "Epoch [173/300], Step [20/384], Loss: 32.6679\n",
      "Epoch [173/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [173/300], Step [60/384], Loss: 44.3184\n",
      "Epoch [173/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [173/300], Step [100/384], Loss: 91.5999\n",
      "Epoch [173/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [173/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [173/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [173/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [173/300], Step [200/384], Loss: 208.9897\n",
      "Epoch [173/300], Step [220/384], Loss: 16.8207\n",
      "Epoch [173/300], Step [240/384], Loss: 63.9033\n",
      "Epoch [173/300], Step [260/384], Loss: 126.5014\n",
      "Epoch [173/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [173/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [173/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [173/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [173/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [173/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [60/384], Loss: 170.2302\n",
      "Epoch [174/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [120/384], Loss: 48.8527\n",
      "Epoch [174/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [200/384], Loss: 83.1549\n",
      "Epoch [174/300], Step [220/384], Loss: 56.4389\n",
      "Epoch [174/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [260/384], Loss: 97.0699\n",
      "Epoch [174/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [300/384], Loss: 194.1529\n",
      "Epoch [174/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [380/384], Loss: 297.0144\n",
      "Epoch [175/300], Step [20/384], Loss: 35.2113\n",
      "Epoch [175/300], Step [40/384], Loss: 140.0403\n",
      "Epoch [175/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [175/300], Step [80/384], Loss: 193.8269\n",
      "Epoch [175/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [175/300], Step [120/384], Loss: 123.6791\n",
      "Epoch [175/300], Step [140/384], Loss: 9.4927\n",
      "Epoch [175/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [175/300], Step [180/384], Loss: 19.0139\n",
      "Epoch [175/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [175/300], Step [220/384], Loss: 48.7050\n",
      "Epoch [175/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [175/300], Step [260/384], Loss: 239.4091\n",
      "Epoch [175/300], Step [280/384], Loss: 79.0222\n",
      "Epoch [175/300], Step [300/384], Loss: 294.3418\n",
      "Epoch [175/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [175/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [175/300], Step [360/384], Loss: 67.9564\n",
      "Epoch [175/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [176/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [176/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [176/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [176/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [176/300], Step [100/384], Loss: 0.0008\n",
      "Epoch [176/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [176/300], Step [140/384], Loss: 49.9360\n",
      "Epoch [176/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [176/300], Step [180/384], Loss: 47.8242\n",
      "Epoch [176/300], Step [200/384], Loss: 187.9336\n",
      "Epoch [176/300], Step [220/384], Loss: 12.8388\n",
      "Epoch [176/300], Step [240/384], Loss: 52.1516\n",
      "Epoch [176/300], Step [260/384], Loss: 9.7622\n",
      "Epoch [176/300], Step [280/384], Loss: 35.5260\n",
      "Epoch [176/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [176/300], Step [320/384], Loss: 78.5253\n",
      "Epoch [176/300], Step [340/384], Loss: 100.4362\n",
      "Epoch [176/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [176/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [60/384], Loss: 131.3769\n",
      "Epoch [177/300], Step [80/384], Loss: 125.3434\n",
      "Epoch [177/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [180/384], Loss: 75.3459\n",
      "Epoch [177/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [300/384], Loss: 24.8569\n",
      "Epoch [177/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [380/384], Loss: 195.1969\n",
      "Epoch [178/300], Step [20/384], Loss: 130.4304\n",
      "Epoch [178/300], Step [40/384], Loss: 15.4773\n",
      "Epoch [178/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [80/384], Loss: 6.9857\n",
      "Epoch [178/300], Step [100/384], Loss: 52.1287\n",
      "Epoch [178/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [280/384], Loss: 73.5894\n",
      "Epoch [178/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [340/384], Loss: 171.2668\n",
      "Epoch [178/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [179/300], Step [20/384], Loss: 72.8179\n",
      "Epoch [179/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [179/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [179/300], Step [80/384], Loss: 12.0149\n",
      "Epoch [179/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [179/300], Step [120/384], Loss: 324.1261\n",
      "Epoch [179/300], Step [140/384], Loss: 27.0471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [179/300], Step [160/384], Loss: 7.4584\n",
      "Epoch [179/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [179/300], Step [200/384], Loss: 117.4469\n",
      "Epoch [179/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [179/300], Step [240/384], Loss: 105.0629\n",
      "Epoch [179/300], Step [260/384], Loss: 39.7745\n",
      "Epoch [179/300], Step [280/384], Loss: 19.2070\n",
      "Epoch [179/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [179/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [179/300], Step [340/384], Loss: 114.7139\n",
      "Epoch [179/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [179/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [40/384], Loss: 154.3595\n",
      "Epoch [180/300], Step [60/384], Loss: 38.2705\n",
      "Epoch [180/300], Step [80/384], Loss: 0.1058\n",
      "Epoch [180/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [140/384], Loss: 23.2117\n",
      "Epoch [180/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [180/384], Loss: 69.9399\n",
      "Epoch [180/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [220/384], Loss: 75.5549\n",
      "Epoch [180/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [340/384], Loss: 29.8522\n",
      "Epoch [180/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [181/300], Step [20/384], Loss: 0.9023\n",
      "Epoch [181/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [181/300], Step [60/384], Loss: 19.1162\n",
      "Epoch [181/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [181/300], Step [100/384], Loss: 49.6537\n",
      "Epoch [181/300], Step [120/384], Loss: 44.3548\n",
      "Epoch [181/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [181/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [181/300], Step [180/384], Loss: 81.7153\n",
      "Epoch [181/300], Step [200/384], Loss: 118.7766\n",
      "Epoch [181/300], Step [220/384], Loss: 23.9243\n",
      "Epoch [181/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [181/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [181/300], Step [280/384], Loss: 188.1190\n",
      "Epoch [181/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [181/300], Step [320/384], Loss: 5.0536\n",
      "Epoch [181/300], Step [340/384], Loss: 90.3514\n",
      "Epoch [181/300], Step [360/384], Loss: 68.9295\n",
      "Epoch [181/300], Step [380/384], Loss: 77.2979\n",
      "Epoch [182/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [40/384], Loss: 33.7381\n",
      "Epoch [182/300], Step [60/384], Loss: 24.1916\n",
      "Epoch [182/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [140/384], Loss: 55.3986\n",
      "Epoch [182/300], Step [160/384], Loss: 29.9794\n",
      "Epoch [182/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [220/384], Loss: 19.2690\n",
      "Epoch [182/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [380/384], Loss: 21.8768\n",
      "Epoch [183/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [183/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [183/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [183/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [183/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [183/300], Step [120/384], Loss: 13.3943\n",
      "Epoch [183/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [183/300], Step [160/384], Loss: 81.2613\n",
      "Epoch [183/300], Step [180/384], Loss: 32.3930\n",
      "Epoch [183/300], Step [200/384], Loss: 62.9044\n",
      "Epoch [183/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [183/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [183/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [183/300], Step [280/384], Loss: 65.5033\n",
      "Epoch [183/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [183/300], Step [320/384], Loss: 106.7370\n",
      "Epoch [183/300], Step [340/384], Loss: 5.6373\n",
      "Epoch [183/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [183/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [120/384], Loss: 47.4525\n",
      "Epoch [184/300], Step [140/384], Loss: 7.5628\n",
      "Epoch [184/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [200/384], Loss: 4.5851\n",
      "Epoch [184/300], Step [220/384], Loss: 89.7015\n",
      "Epoch [184/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [280/384], Loss: 7.9165\n",
      "Epoch [184/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [320/384], Loss: 115.1611\n",
      "Epoch [184/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [380/384], Loss: 67.6461\n",
      "Epoch [185/300], Step [20/384], Loss: 252.8656\n",
      "Epoch [185/300], Step [40/384], Loss: 26.9697\n",
      "Epoch [185/300], Step [60/384], Loss: 0.0010\n",
      "Epoch [185/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [185/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [185/300], Step [120/384], Loss: 27.9600\n",
      "Epoch [185/300], Step [140/384], Loss: 65.5279\n",
      "Epoch [185/300], Step [160/384], Loss: 104.0268\n",
      "Epoch [185/300], Step [180/384], Loss: 44.6305\n",
      "Epoch [185/300], Step [200/384], Loss: 27.5389\n",
      "Epoch [185/300], Step [220/384], Loss: 74.3154\n",
      "Epoch [185/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [185/300], Step [260/384], Loss: 84.8058\n",
      "Epoch [185/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [185/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [185/300], Step [320/384], Loss: 16.0421\n",
      "Epoch [185/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [185/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [185/300], Step [380/384], Loss: 72.0664\n",
      "Epoch [186/300], Step [20/384], Loss: 83.0976\n",
      "Epoch [186/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [80/384], Loss: 22.4439\n",
      "Epoch [186/300], Step [100/384], Loss: 152.7017\n",
      "Epoch [186/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [160/384], Loss: 91.6865\n",
      "Epoch [186/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [360/384], Loss: 0.5054\n",
      "Epoch [186/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [40/384], Loss: 17.8157\n",
      "Epoch [187/300], Step [60/384], Loss: 25.3595\n",
      "Epoch [187/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [120/384], Loss: 8.8017\n",
      "Epoch [187/300], Step [140/384], Loss: 118.8915\n",
      "Epoch [187/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [220/384], Loss: 1.2975\n",
      "Epoch [187/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [280/384], Loss: 53.1639\n",
      "Epoch [187/300], Step [300/384], Loss: 12.8603\n",
      "Epoch [187/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [20/384], Loss: 21.5700\n",
      "Epoch [188/300], Step [40/384], Loss: 47.7890\n",
      "Epoch [188/300], Step [60/384], Loss: 41.6121\n",
      "Epoch [188/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [280/384], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [188/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [320/384], Loss: 50.1470\n",
      "Epoch [188/300], Step [340/384], Loss: 58.2402\n",
      "Epoch [188/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [189/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [189/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [189/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [189/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [189/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [189/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [189/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [189/300], Step [160/384], Loss: 19.2767\n",
      "Epoch [189/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [189/300], Step [200/384], Loss: 56.8644\n",
      "Epoch [189/300], Step [220/384], Loss: 0.8616\n",
      "Epoch [189/300], Step [240/384], Loss: 113.1449\n",
      "Epoch [189/300], Step [260/384], Loss: 137.7244\n",
      "Epoch [189/300], Step [280/384], Loss: 0.0035\n",
      "Epoch [189/300], Step [300/384], Loss: 1.7327\n",
      "Epoch [189/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [189/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [189/300], Step [360/384], Loss: 30.9251\n",
      "Epoch [189/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [80/384], Loss: 43.2862\n",
      "Epoch [190/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [140/384], Loss: 3.7333\n",
      "Epoch [190/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [220/384], Loss: 53.4685\n",
      "Epoch [190/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [360/384], Loss: 4.0996\n",
      "Epoch [190/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [60/384], Loss: 110.6926\n",
      "Epoch [191/300], Step [80/384], Loss: 139.0196\n",
      "Epoch [191/300], Step [100/384], Loss: 51.7465\n",
      "Epoch [191/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [220/384], Loss: 4.8637\n",
      "Epoch [191/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [260/384], Loss: 11.8131\n",
      "Epoch [191/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [320/384], Loss: 67.8462\n",
      "Epoch [191/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [140/384], Loss: 61.7199\n",
      "Epoch [192/300], Step [160/384], Loss: 6.0502\n",
      "Epoch [192/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [200/384], Loss: 5.2964\n",
      "Epoch [192/300], Step [220/384], Loss: 119.2469\n",
      "Epoch [192/300], Step [240/384], Loss: 56.5862\n",
      "Epoch [192/300], Step [260/384], Loss: 11.4100\n",
      "Epoch [192/300], Step [280/384], Loss: 16.8747\n",
      "Epoch [192/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [380/384], Loss: 1.7323\n",
      "Epoch [193/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [193/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [193/300], Step [60/384], Loss: 16.6019\n",
      "Epoch [193/300], Step [80/384], Loss: 73.5092\n",
      "Epoch [193/300], Step [100/384], Loss: 29.9294\n",
      "Epoch [193/300], Step [120/384], Loss: 162.0404\n",
      "Epoch [193/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [193/300], Step [160/384], Loss: 26.6098\n",
      "Epoch [193/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [193/300], Step [200/384], Loss: 59.2473\n",
      "Epoch [193/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [193/300], Step [240/384], Loss: 42.3694\n",
      "Epoch [193/300], Step [260/384], Loss: 210.0524\n",
      "Epoch [193/300], Step [280/384], Loss: 138.1365\n",
      "Epoch [193/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [193/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [193/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [193/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [193/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [60/384], Loss: 31.4421\n",
      "Epoch [194/300], Step [80/384], Loss: 77.6345\n",
      "Epoch [194/300], Step [100/384], Loss: 38.1752\n",
      "Epoch [194/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [140/384], Loss: 80.0974\n",
      "Epoch [194/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [180/384], Loss: 51.5817\n",
      "Epoch [194/300], Step [200/384], Loss: 26.9699\n",
      "Epoch [194/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [320/384], Loss: 28.2718\n",
      "Epoch [194/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [380/384], Loss: 0.0503\n",
      "Epoch [195/300], Step [20/384], Loss: 99.2916\n",
      "Epoch [195/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [195/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [195/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [195/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [195/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [195/300], Step [140/384], Loss: 24.3566\n",
      "Epoch [195/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [195/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [195/300], Step [200/384], Loss: 0.0006\n",
      "Epoch [195/300], Step [220/384], Loss: 138.7916\n",
      "Epoch [195/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [195/300], Step [260/384], Loss: 59.9968\n",
      "Epoch [195/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [195/300], Step [300/384], Loss: 174.3114\n",
      "Epoch [195/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [195/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [195/300], Step [360/384], Loss: 30.3694\n",
      "Epoch [195/300], Step [380/384], Loss: 75.5691\n",
      "Epoch [196/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [100/384], Loss: 15.7302\n",
      "Epoch [196/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [140/384], Loss: 105.3823\n",
      "Epoch [196/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [180/384], Loss: 57.1000\n",
      "Epoch [196/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [320/384], Loss: 24.5765\n",
      "Epoch [196/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [360/384], Loss: 17.1991\n",
      "Epoch [196/300], Step [380/384], Loss: 12.4742\n",
      "Epoch [197/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [197/300], Step [40/384], Loss: 62.7858\n",
      "Epoch [197/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [197/300], Step [80/384], Loss: 82.8132\n",
      "Epoch [197/300], Step [100/384], Loss: 3.1711\n",
      "Epoch [197/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [197/300], Step [140/384], Loss: 26.2115\n",
      "Epoch [197/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [197/300], Step [180/384], Loss: 91.6276\n",
      "Epoch [197/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [197/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [197/300], Step [240/384], Loss: 25.8250\n",
      "Epoch [197/300], Step [260/384], Loss: 40.6980\n",
      "Epoch [197/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [197/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [197/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [197/300], Step [340/384], Loss: 54.5870\n",
      "Epoch [197/300], Step [360/384], Loss: 9.5129\n",
      "Epoch [197/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [198/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [198/300], Step [40/384], Loss: 39.1593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [198/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [198/300], Step [80/384], Loss: 0.6024\n",
      "Epoch [198/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [198/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [198/300], Step [140/384], Loss: 127.7679\n",
      "Epoch [198/300], Step [160/384], Loss: 85.3749\n",
      "Epoch [198/300], Step [180/384], Loss: 50.9202\n",
      "Epoch [198/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [198/300], Step [220/384], Loss: 101.1535\n",
      "Epoch [198/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [198/300], Step [260/384], Loss: 274.3773\n",
      "Epoch [198/300], Step [280/384], Loss: 73.2569\n",
      "Epoch [198/300], Step [300/384], Loss: 40.3435\n",
      "Epoch [198/300], Step [320/384], Loss: 5.5349\n",
      "Epoch [198/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [198/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [198/300], Step [380/384], Loss: 50.5976\n",
      "Epoch [199/300], Step [20/384], Loss: 19.8670\n",
      "Epoch [199/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [199/300], Step [60/384], Loss: 100.7478\n",
      "Epoch [199/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [199/300], Step [100/384], Loss: 3.2660\n",
      "Epoch [199/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [199/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [199/300], Step [160/384], Loss: 3.3356\n",
      "Epoch [199/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [199/300], Step [200/384], Loss: 40.1672\n",
      "Epoch [199/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [199/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [199/300], Step [260/384], Loss: 50.6148\n",
      "Epoch [199/300], Step [280/384], Loss: 22.4106\n",
      "Epoch [199/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [199/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [199/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [199/300], Step [360/384], Loss: 76.5426\n",
      "Epoch [199/300], Step [380/384], Loss: 19.9726\n",
      "Epoch [200/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [200/300], Step [40/384], Loss: 164.7972\n",
      "Epoch [200/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [200/300], Step [80/384], Loss: 0.0004\n",
      "Epoch [200/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [200/300], Step [120/384], Loss: 38.7555\n",
      "Epoch [200/300], Step [140/384], Loss: 16.1026\n",
      "Epoch [200/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [200/300], Step [180/384], Loss: 110.3258\n",
      "Epoch [200/300], Step [200/384], Loss: 246.2244\n",
      "Epoch [200/300], Step [220/384], Loss: 37.7403\n",
      "Epoch [200/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [200/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [200/300], Step [280/384], Loss: 37.3377\n",
      "Epoch [200/300], Step [300/384], Loss: 6.2006\n",
      "Epoch [200/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [200/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [200/300], Step [360/384], Loss: 28.0637\n",
      "Epoch [200/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [60/384], Loss: 18.3805\n",
      "Epoch [201/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [100/384], Loss: 31.8595\n",
      "Epoch [201/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [220/384], Loss: 114.2906\n",
      "Epoch [201/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [260/384], Loss: 534.7391\n",
      "Epoch [201/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [320/384], Loss: 124.9716\n",
      "Epoch [201/300], Step [340/384], Loss: 9.4585\n",
      "Epoch [201/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [380/384], Loss: 64.4773\n",
      "Epoch [202/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [202/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [202/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [202/300], Step [80/384], Loss: 49.1414\n",
      "Epoch [202/300], Step [100/384], Loss: 2.2603\n",
      "Epoch [202/300], Step [120/384], Loss: 15.5655\n",
      "Epoch [202/300], Step [140/384], Loss: 18.4240\n",
      "Epoch [202/300], Step [160/384], Loss: 152.6400\n",
      "Epoch [202/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [202/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [202/300], Step [220/384], Loss: 2.4251\n",
      "Epoch [202/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [202/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [202/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [202/300], Step [300/384], Loss: 34.1646\n",
      "Epoch [202/300], Step [320/384], Loss: 3.4204\n",
      "Epoch [202/300], Step [340/384], Loss: 116.9475\n",
      "Epoch [202/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [202/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [203/300], Step [20/384], Loss: 64.0681\n",
      "Epoch [203/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [203/300], Step [60/384], Loss: 22.6499\n",
      "Epoch [203/300], Step [80/384], Loss: 19.7324\n",
      "Epoch [203/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [203/300], Step [120/384], Loss: 244.1254\n",
      "Epoch [203/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [203/300], Step [160/384], Loss: 40.2007\n",
      "Epoch [203/300], Step [180/384], Loss: 73.9555\n",
      "Epoch [203/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [203/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [203/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [203/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [203/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [203/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [203/300], Step [320/384], Loss: 103.1658\n",
      "Epoch [203/300], Step [340/384], Loss: 95.9578\n",
      "Epoch [203/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [203/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [204/300], Step [20/384], Loss: 14.3674\n",
      "Epoch [204/300], Step [40/384], Loss: 89.5556\n",
      "Epoch [204/300], Step [60/384], Loss: 98.7958\n",
      "Epoch [204/300], Step [80/384], Loss: 0.2548\n",
      "Epoch [204/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [204/300], Step [120/384], Loss: 88.2233\n",
      "Epoch [204/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [204/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [204/300], Step [180/384], Loss: 6.6094\n",
      "Epoch [204/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [204/300], Step [220/384], Loss: 3.3286\n",
      "Epoch [204/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [204/300], Step [260/384], Loss: 60.3443\n",
      "Epoch [204/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [204/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [204/300], Step [320/384], Loss: 301.2232\n",
      "Epoch [204/300], Step [340/384], Loss: 51.3971\n",
      "Epoch [204/300], Step [360/384], Loss: 9.3469\n",
      "Epoch [204/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [205/300], Step [20/384], Loss: 4.0133\n",
      "Epoch [205/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [205/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [205/300], Step [80/384], Loss: 39.2706\n",
      "Epoch [205/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [205/300], Step [120/384], Loss: 56.2265\n",
      "Epoch [205/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [205/300], Step [160/384], Loss: 22.1097\n",
      "Epoch [205/300], Step [180/384], Loss: 20.1969\n",
      "Epoch [205/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [205/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [205/300], Step [240/384], Loss: 15.0172\n",
      "Epoch [205/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [205/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [205/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [205/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [205/300], Step [340/384], Loss: 13.6690\n",
      "Epoch [205/300], Step [360/384], Loss: 61.3548\n",
      "Epoch [205/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [60/384], Loss: 21.9003\n",
      "Epoch [206/300], Step [80/384], Loss: 35.3097\n",
      "Epoch [206/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [220/384], Loss: 42.4454\n",
      "Epoch [206/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [280/384], Loss: 114.3040\n",
      "Epoch [206/300], Step [300/384], Loss: 62.0970\n",
      "Epoch [206/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [340/384], Loss: 28.8759\n",
      "Epoch [206/300], Step [360/384], Loss: 43.3418\n",
      "Epoch [206/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [80/384], Loss: 4.2810\n",
      "Epoch [207/300], Step [100/384], Loss: 20.7713\n",
      "Epoch [207/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [180/384], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [207/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [280/384], Loss: 25.6993\n",
      "Epoch [207/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [340/384], Loss: 57.1590\n",
      "Epoch [207/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [380/384], Loss: 67.8598\n",
      "Epoch [208/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [208/300], Step [40/384], Loss: 34.9832\n",
      "Epoch [208/300], Step [60/384], Loss: 0.3181\n",
      "Epoch [208/300], Step [80/384], Loss: 282.7851\n",
      "Epoch [208/300], Step [100/384], Loss: 4.6896\n",
      "Epoch [208/300], Step [120/384], Loss: 17.7778\n",
      "Epoch [208/300], Step [140/384], Loss: 32.5707\n",
      "Epoch [208/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [208/300], Step [180/384], Loss: 85.4976\n",
      "Epoch [208/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [208/300], Step [220/384], Loss: 160.2449\n",
      "Epoch [208/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [208/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [208/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [208/300], Step [300/384], Loss: 144.4023\n",
      "Epoch [208/300], Step [320/384], Loss: 71.3742\n",
      "Epoch [208/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [208/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [208/300], Step [380/384], Loss: 6.5959\n",
      "Epoch [209/300], Step [20/384], Loss: 66.4972\n",
      "Epoch [209/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [209/300], Step [60/384], Loss: 166.4146\n",
      "Epoch [209/300], Step [80/384], Loss: 129.1721\n",
      "Epoch [209/300], Step [100/384], Loss: 37.1055\n",
      "Epoch [209/300], Step [120/384], Loss: 97.4545\n",
      "Epoch [209/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [209/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [209/300], Step [180/384], Loss: 350.6536\n",
      "Epoch [209/300], Step [200/384], Loss: 88.0962\n",
      "Epoch [209/300], Step [220/384], Loss: 42.4935\n",
      "Epoch [209/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [209/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [209/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [209/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [209/300], Step [320/384], Loss: 24.3143\n",
      "Epoch [209/300], Step [340/384], Loss: 111.5564\n",
      "Epoch [209/300], Step [360/384], Loss: 65.5789\n",
      "Epoch [209/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [40/384], Loss: 46.4993\n",
      "Epoch [210/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [140/384], Loss: 94.9191\n",
      "Epoch [210/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [200/384], Loss: 47.8695\n",
      "Epoch [210/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [240/384], Loss: 174.9922\n",
      "Epoch [210/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [340/384], Loss: 166.8248\n",
      "Epoch [210/300], Step [360/384], Loss: 80.9329\n",
      "Epoch [210/300], Step [380/384], Loss: 103.7986\n",
      "Epoch [211/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [40/384], Loss: 6.5729\n",
      "Epoch [211/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [100/384], Loss: 14.8981\n",
      "Epoch [211/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [160/384], Loss: 43.8721\n",
      "Epoch [211/300], Step [180/384], Loss: 91.7775\n",
      "Epoch [211/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [220/384], Loss: 10.2294\n",
      "Epoch [211/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [360/384], Loss: 12.3595\n",
      "Epoch [211/300], Step [380/384], Loss: 23.6805\n",
      "Epoch [212/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [212/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [212/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [212/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [212/300], Step [100/384], Loss: 85.8250\n",
      "Epoch [212/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [212/300], Step [140/384], Loss: 53.8012\n",
      "Epoch [212/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [212/300], Step [180/384], Loss: 26.2031\n",
      "Epoch [212/300], Step [200/384], Loss: 36.0426\n",
      "Epoch [212/300], Step [220/384], Loss: 0.2587\n",
      "Epoch [212/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [212/300], Step [260/384], Loss: 87.0908\n",
      "Epoch [212/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [212/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [212/300], Step [320/384], Loss: 85.1612\n",
      "Epoch [212/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [212/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [212/300], Step [380/384], Loss: 7.0053\n",
      "Epoch [213/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [200/384], Loss: 31.0549\n",
      "Epoch [213/300], Step [220/384], Loss: 88.9018\n",
      "Epoch [213/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [360/384], Loss: 60.2097\n",
      "Epoch [213/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [20/384], Loss: 42.7527\n",
      "Epoch [214/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [100/384], Loss: 54.2765\n",
      "Epoch [214/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [180/384], Loss: 3.9400\n",
      "Epoch [214/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [260/384], Loss: 36.6497\n",
      "Epoch [214/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [320/384], Loss: 60.4394\n",
      "Epoch [214/300], Step [340/384], Loss: 53.1957\n",
      "Epoch [214/300], Step [360/384], Loss: 52.3536\n",
      "Epoch [214/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [215/300], Step [20/384], Loss: 7.0090\n",
      "Epoch [215/300], Step [40/384], Loss: 95.1631\n",
      "Epoch [215/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [215/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [215/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [215/300], Step [120/384], Loss: 13.3858\n",
      "Epoch [215/300], Step [140/384], Loss: 294.0655\n",
      "Epoch [215/300], Step [160/384], Loss: 0.6231\n",
      "Epoch [215/300], Step [180/384], Loss: 23.0038\n",
      "Epoch [215/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [215/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [215/300], Step [240/384], Loss: 102.3077\n",
      "Epoch [215/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [215/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [215/300], Step [300/384], Loss: 130.5449\n",
      "Epoch [215/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [215/300], Step [340/384], Loss: 47.7113\n",
      "Epoch [215/300], Step [360/384], Loss: 50.1296\n",
      "Epoch [215/300], Step [380/384], Loss: 187.0915\n",
      "Epoch [216/300], Step [20/384], Loss: 100.0838\n",
      "Epoch [216/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [216/300], Step [60/384], Loss: 74.0312\n",
      "Epoch [216/300], Step [80/384], Loss: 18.4271\n",
      "Epoch [216/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [216/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [216/300], Step [140/384], Loss: 19.9291\n",
      "Epoch [216/300], Step [160/384], Loss: 33.6093\n",
      "Epoch [216/300], Step [180/384], Loss: 0.0009\n",
      "Epoch [216/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [216/300], Step [220/384], Loss: 9.7747\n",
      "Epoch [216/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [216/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [216/300], Step [280/384], Loss: 46.6763\n",
      "Epoch [216/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [216/300], Step [320/384], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [216/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [216/300], Step [360/384], Loss: 40.9042\n",
      "Epoch [216/300], Step [380/384], Loss: 27.1488\n",
      "Epoch [217/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [217/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [217/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [217/300], Step [80/384], Loss: 18.6371\n",
      "Epoch [217/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [217/300], Step [120/384], Loss: 16.1868\n",
      "Epoch [217/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [217/300], Step [160/384], Loss: 175.3274\n",
      "Epoch [217/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [217/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [217/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [217/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [217/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [217/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [217/300], Step [300/384], Loss: 193.6335\n",
      "Epoch [217/300], Step [320/384], Loss: 45.9305\n",
      "Epoch [217/300], Step [340/384], Loss: 147.1295\n",
      "Epoch [217/300], Step [360/384], Loss: 59.2187\n",
      "Epoch [217/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [20/384], Loss: 10.5952\n",
      "Epoch [218/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [60/384], Loss: 56.1798\n",
      "Epoch [218/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [100/384], Loss: 185.5063\n",
      "Epoch [218/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [160/384], Loss: 77.0655\n",
      "Epoch [218/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [200/384], Loss: 23.1578\n",
      "Epoch [218/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [219/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [219/300], Step [40/384], Loss: 63.1959\n",
      "Epoch [219/300], Step [60/384], Loss: 79.8691\n",
      "Epoch [219/300], Step [80/384], Loss: 14.0645\n",
      "Epoch [219/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [219/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [219/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [219/300], Step [160/384], Loss: 29.1479\n",
      "Epoch [219/300], Step [180/384], Loss: 137.0475\n",
      "Epoch [219/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [219/300], Step [220/384], Loss: 84.9678\n",
      "Epoch [219/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [219/300], Step [260/384], Loss: 12.9097\n",
      "Epoch [219/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [219/300], Step [300/384], Loss: 60.7972\n",
      "Epoch [219/300], Step [320/384], Loss: 64.1924\n",
      "Epoch [219/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [219/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [219/300], Step [380/384], Loss: 17.3809\n",
      "Epoch [220/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [220/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [220/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [220/300], Step [80/384], Loss: 60.2166\n",
      "Epoch [220/300], Step [100/384], Loss: 111.6759\n",
      "Epoch [220/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [220/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [220/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [220/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [220/300], Step [200/384], Loss: 31.3980\n",
      "Epoch [220/300], Step [220/384], Loss: 1.6800\n",
      "Epoch [220/300], Step [240/384], Loss: 70.3657\n",
      "Epoch [220/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [220/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [220/300], Step [300/384], Loss: 92.5265\n",
      "Epoch [220/300], Step [320/384], Loss: 108.7757\n",
      "Epoch [220/300], Step [340/384], Loss: 96.3097\n",
      "Epoch [220/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [220/300], Step [380/384], Loss: 6.2633\n",
      "Epoch [221/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [221/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [221/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [221/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [221/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [221/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [221/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [221/300], Step [160/384], Loss: 183.8438\n",
      "Epoch [221/300], Step [180/384], Loss: 9.8747\n",
      "Epoch [221/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [221/300], Step [220/384], Loss: 240.9822\n",
      "Epoch [221/300], Step [240/384], Loss: 2.4273\n",
      "Epoch [221/300], Step [260/384], Loss: 61.7557\n",
      "Epoch [221/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [221/300], Step [300/384], Loss: 77.6212\n",
      "Epoch [221/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [221/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [221/300], Step [360/384], Loss: 3.9632\n",
      "Epoch [221/300], Step [380/384], Loss: 276.0280\n",
      "Epoch [222/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [222/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [222/300], Step [60/384], Loss: 3.0268\n",
      "Epoch [222/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [222/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [222/300], Step [120/384], Loss: 14.0506\n",
      "Epoch [222/300], Step [140/384], Loss: 19.6029\n",
      "Epoch [222/300], Step [160/384], Loss: 32.3054\n",
      "Epoch [222/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [222/300], Step [200/384], Loss: 43.9539\n",
      "Epoch [222/300], Step [220/384], Loss: 16.1536\n",
      "Epoch [222/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [222/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [222/300], Step [280/384], Loss: 115.9777\n",
      "Epoch [222/300], Step [300/384], Loss: 8.3621\n",
      "Epoch [222/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [222/300], Step [340/384], Loss: 90.9204\n",
      "Epoch [222/300], Step [360/384], Loss: 87.2382\n",
      "Epoch [222/300], Step [380/384], Loss: 6.0348\n",
      "Epoch [223/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [223/300], Step [40/384], Loss: 214.9855\n",
      "Epoch [223/300], Step [60/384], Loss: 6.6196\n",
      "Epoch [223/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [223/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [223/300], Step [120/384], Loss: 85.0542\n",
      "Epoch [223/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [223/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [223/300], Step [180/384], Loss: 35.4866\n",
      "Epoch [223/300], Step [200/384], Loss: 145.7398\n",
      "Epoch [223/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [223/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [223/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [223/300], Step [280/384], Loss: 162.7617\n",
      "Epoch [223/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [223/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [223/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [223/300], Step [360/384], Loss: 201.6209\n",
      "Epoch [223/300], Step [380/384], Loss: 54.2174\n",
      "Epoch [224/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [224/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [224/300], Step [60/384], Loss: 31.9468\n",
      "Epoch [224/300], Step [80/384], Loss: 90.8912\n",
      "Epoch [224/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [224/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [224/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [224/300], Step [160/384], Loss: 29.9150\n",
      "Epoch [224/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [224/300], Step [200/384], Loss: 212.4821\n",
      "Epoch [224/300], Step [220/384], Loss: 19.1090\n",
      "Epoch [224/300], Step [240/384], Loss: 147.8921\n",
      "Epoch [224/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [224/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [224/300], Step [300/384], Loss: 16.3921\n",
      "Epoch [224/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [224/300], Step [340/384], Loss: 83.5974\n",
      "Epoch [224/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [224/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [225/300], Step [20/384], Loss: 20.2049\n",
      "Epoch [225/300], Step [40/384], Loss: 100.2035\n",
      "Epoch [225/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [225/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [225/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [225/300], Step [120/384], Loss: 41.1533\n",
      "Epoch [225/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [225/300], Step [160/384], Loss: 130.8959\n",
      "Epoch [225/300], Step [180/384], Loss: 105.0794\n",
      "Epoch [225/300], Step [200/384], Loss: 15.2746\n",
      "Epoch [225/300], Step [220/384], Loss: 63.1731\n",
      "Epoch [225/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [225/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [225/300], Step [280/384], Loss: 166.8695\n",
      "Epoch [225/300], Step [300/384], Loss: 32.9297\n",
      "Epoch [225/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [225/300], Step [340/384], Loss: 26.6046\n",
      "Epoch [225/300], Step [360/384], Loss: 10.1845\n",
      "Epoch [225/300], Step [380/384], Loss: 53.4997\n",
      "Epoch [226/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [226/300], Step [40/384], Loss: 41.2101\n",
      "Epoch [226/300], Step [60/384], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [226/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [226/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [226/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [226/300], Step [140/384], Loss: 148.5855\n",
      "Epoch [226/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [226/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [226/300], Step [200/384], Loss: 125.0759\n",
      "Epoch [226/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [226/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [226/300], Step [260/384], Loss: 62.6455\n",
      "Epoch [226/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [226/300], Step [300/384], Loss: 13.0485\n",
      "Epoch [226/300], Step [320/384], Loss: 49.1995\n",
      "Epoch [226/300], Step [340/384], Loss: 121.8418\n",
      "Epoch [226/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [226/300], Step [380/384], Loss: 14.8703\n",
      "Epoch [227/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [60/384], Loss: 39.5175\n",
      "Epoch [227/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [100/384], Loss: 28.4164\n",
      "Epoch [227/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [140/384], Loss: 33.8069\n",
      "Epoch [227/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [220/384], Loss: 0.0017\n",
      "Epoch [227/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [360/384], Loss: 67.8869\n",
      "Epoch [227/300], Step [380/384], Loss: 27.4170\n",
      "Epoch [228/300], Step [20/384], Loss: 51.2353\n",
      "Epoch [228/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [120/384], Loss: 94.2433\n",
      "Epoch [228/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [280/384], Loss: 24.2799\n",
      "Epoch [228/300], Step [300/384], Loss: 81.0017\n",
      "Epoch [228/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [40/384], Loss: 140.3411\n",
      "Epoch [229/300], Step [60/384], Loss: 85.3299\n",
      "Epoch [229/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [240/384], Loss: 69.3603\n",
      "Epoch [229/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [360/384], Loss: 73.6426\n",
      "Epoch [229/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [230/300], Step [20/384], Loss: 117.6490\n",
      "Epoch [230/300], Step [40/384], Loss: 10.3040\n",
      "Epoch [230/300], Step [60/384], Loss: 50.1348\n",
      "Epoch [230/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [230/300], Step [100/384], Loss: 46.7755\n",
      "Epoch [230/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [230/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [230/300], Step [160/384], Loss: 0.0012\n",
      "Epoch [230/300], Step [180/384], Loss: 29.0334\n",
      "Epoch [230/300], Step [200/384], Loss: 34.9143\n",
      "Epoch [230/300], Step [220/384], Loss: 68.6964\n",
      "Epoch [230/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [230/300], Step [260/384], Loss: 42.4580\n",
      "Epoch [230/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [230/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [230/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [230/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [230/300], Step [360/384], Loss: 76.7983\n",
      "Epoch [230/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [231/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [231/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [231/300], Step [60/384], Loss: 170.6795\n",
      "Epoch [231/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [231/300], Step [100/384], Loss: 69.9641\n",
      "Epoch [231/300], Step [120/384], Loss: 19.7393\n",
      "Epoch [231/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [231/300], Step [160/384], Loss: 9.2017\n",
      "Epoch [231/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [231/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [231/300], Step [220/384], Loss: 22.1806\n",
      "Epoch [231/300], Step [240/384], Loss: 198.7946\n",
      "Epoch [231/300], Step [260/384], Loss: 42.5129\n",
      "Epoch [231/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [231/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [231/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [231/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [231/300], Step [360/384], Loss: 32.2571\n",
      "Epoch [231/300], Step [380/384], Loss: 8.4205\n",
      "Epoch [232/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [232/300], Step [40/384], Loss: 153.3779\n",
      "Epoch [232/300], Step [60/384], Loss: 14.6913\n",
      "Epoch [232/300], Step [80/384], Loss: 10.4599\n",
      "Epoch [232/300], Step [100/384], Loss: 72.7118\n",
      "Epoch [232/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [232/300], Step [140/384], Loss: 254.0184\n",
      "Epoch [232/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [232/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [232/300], Step [200/384], Loss: 14.2356\n",
      "Epoch [232/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [232/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [232/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [232/300], Step [280/384], Loss: 51.4536\n",
      "Epoch [232/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [232/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [232/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [232/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [232/300], Step [380/384], Loss: 0.0335\n",
      "Epoch [233/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [60/384], Loss: 83.9390\n",
      "Epoch [233/300], Step [80/384], Loss: 0.0005\n",
      "Epoch [233/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [220/384], Loss: 70.4959\n",
      "Epoch [233/300], Step [240/384], Loss: 43.5826\n",
      "Epoch [233/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [280/384], Loss: 109.2065\n",
      "Epoch [233/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [360/384], Loss: 169.0310\n",
      "Epoch [233/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [234/300], Step [20/384], Loss: 45.4166\n",
      "Epoch [234/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [234/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [234/300], Step [80/384], Loss: 0.0010\n",
      "Epoch [234/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [234/300], Step [120/384], Loss: 187.6063\n",
      "Epoch [234/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [234/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [234/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [234/300], Step [200/384], Loss: 22.1087\n",
      "Epoch [234/300], Step [220/384], Loss: 0.0148\n",
      "Epoch [234/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [234/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [234/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [234/300], Step [300/384], Loss: 367.2193\n",
      "Epoch [234/300], Step [320/384], Loss: 19.2402\n",
      "Epoch [234/300], Step [340/384], Loss: 96.4993\n",
      "Epoch [234/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [234/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [235/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [235/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [235/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [235/300], Step [80/384], Loss: 70.6851\n",
      "Epoch [235/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [235/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [235/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [235/300], Step [160/384], Loss: 30.3994\n",
      "Epoch [235/300], Step [180/384], Loss: 179.6457\n",
      "Epoch [235/300], Step [200/384], Loss: 81.5590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [235/300], Step [220/384], Loss: 115.4939\n",
      "Epoch [235/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [235/300], Step [260/384], Loss: 16.6610\n",
      "Epoch [235/300], Step [280/384], Loss: 7.0410\n",
      "Epoch [235/300], Step [300/384], Loss: 1.9835\n",
      "Epoch [235/300], Step [320/384], Loss: 20.6799\n",
      "Epoch [235/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [235/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [235/300], Step [380/384], Loss: 77.6814\n",
      "Epoch [236/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [40/384], Loss: 1.9507\n",
      "Epoch [236/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [100/384], Loss: 30.8440\n",
      "Epoch [236/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [260/384], Loss: 14.6932\n",
      "Epoch [236/300], Step [280/384], Loss: 51.5278\n",
      "Epoch [236/300], Step [300/384], Loss: 74.8035\n",
      "Epoch [236/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [40/384], Loss: 46.5543\n",
      "Epoch [237/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [140/384], Loss: 79.7721\n",
      "Epoch [237/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [260/384], Loss: 10.7929\n",
      "Epoch [237/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [340/384], Loss: 114.9171\n",
      "Epoch [237/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [238/300], Step [20/384], Loss: 0.4211\n",
      "Epoch [238/300], Step [40/384], Loss: 51.6178\n",
      "Epoch [238/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [238/300], Step [80/384], Loss: 26.5674\n",
      "Epoch [238/300], Step [100/384], Loss: 167.4248\n",
      "Epoch [238/300], Step [120/384], Loss: 63.6161\n",
      "Epoch [238/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [238/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [238/300], Step [180/384], Loss: 79.5417\n",
      "Epoch [238/300], Step [200/384], Loss: 41.8663\n",
      "Epoch [238/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [238/300], Step [240/384], Loss: 1.5159\n",
      "Epoch [238/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [238/300], Step [280/384], Loss: 158.7495\n",
      "Epoch [238/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [238/300], Step [320/384], Loss: 45.6904\n",
      "Epoch [238/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [238/300], Step [360/384], Loss: 193.4023\n",
      "Epoch [238/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [100/384], Loss: 28.7814\n",
      "Epoch [239/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [220/384], Loss: 51.8945\n",
      "Epoch [239/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [260/384], Loss: 3.9275\n",
      "Epoch [239/300], Step [280/384], Loss: 88.3675\n",
      "Epoch [239/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [340/384], Loss: 61.0589\n",
      "Epoch [239/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [20/384], Loss: 70.1249\n",
      "Epoch [240/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [80/384], Loss: 14.3022\n",
      "Epoch [240/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [320/384], Loss: 19.2332\n",
      "Epoch [240/300], Step [340/384], Loss: 111.5833\n",
      "Epoch [240/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [380/384], Loss: 116.5021\n",
      "Epoch [241/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [80/384], Loss: 2.0595\n",
      "Epoch [241/300], Step [100/384], Loss: 81.7012\n",
      "Epoch [241/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [180/384], Loss: 12.0537\n",
      "Epoch [241/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [300/384], Loss: 45.0960\n",
      "Epoch [241/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [340/384], Loss: 11.0417\n",
      "Epoch [241/300], Step [360/384], Loss: 133.6435\n",
      "Epoch [241/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [60/384], Loss: 108.5701\n",
      "Epoch [242/300], Step [80/384], Loss: 89.4757\n",
      "Epoch [242/300], Step [100/384], Loss: 0.0088\n",
      "Epoch [242/300], Step [120/384], Loss: 86.4156\n",
      "Epoch [242/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [180/384], Loss: 126.5634\n",
      "Epoch [242/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [20/384], Loss: 88.7194\n",
      "Epoch [243/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [60/384], Loss: 54.3345\n",
      "Epoch [243/300], Step [80/384], Loss: 90.4842\n",
      "Epoch [243/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [120/384], Loss: 218.6779\n",
      "Epoch [243/300], Step [140/384], Loss: 88.1604\n",
      "Epoch [243/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [360/384], Loss: 55.0380\n",
      "Epoch [243/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [80/384], Loss: 32.6870\n",
      "Epoch [244/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [140/384], Loss: 8.9019\n",
      "Epoch [244/300], Step [160/384], Loss: 22.0198\n",
      "Epoch [244/300], Step [180/384], Loss: 139.9708\n",
      "Epoch [244/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [260/384], Loss: 36.9180\n",
      "Epoch [244/300], Step [280/384], Loss: 20.0575\n",
      "Epoch [244/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [340/384], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [244/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [245/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [245/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [245/300], Step [60/384], Loss: 46.8779\n",
      "Epoch [245/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [245/300], Step [100/384], Loss: 63.9887\n",
      "Epoch [245/300], Step [120/384], Loss: 68.1857\n",
      "Epoch [245/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [245/300], Step [160/384], Loss: 64.1160\n",
      "Epoch [245/300], Step [180/384], Loss: 84.0503\n",
      "Epoch [245/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [245/300], Step [220/384], Loss: 26.0340\n",
      "Epoch [245/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [245/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [245/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [245/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [245/300], Step [320/384], Loss: 50.2728\n",
      "Epoch [245/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [245/300], Step [360/384], Loss: 39.9249\n",
      "Epoch [245/300], Step [380/384], Loss: 21.9484\n",
      "Epoch [246/300], Step [20/384], Loss: 54.2935\n",
      "Epoch [246/300], Step [40/384], Loss: 41.9463\n",
      "Epoch [246/300], Step [60/384], Loss: 40.0187\n",
      "Epoch [246/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [246/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [246/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [246/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [246/300], Step [160/384], Loss: 162.1552\n",
      "Epoch [246/300], Step [180/384], Loss: 15.5498\n",
      "Epoch [246/300], Step [200/384], Loss: 40.4957\n",
      "Epoch [246/300], Step [220/384], Loss: 87.1517\n",
      "Epoch [246/300], Step [240/384], Loss: 2.3812\n",
      "Epoch [246/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [246/300], Step [280/384], Loss: 4.6368\n",
      "Epoch [246/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [246/300], Step [320/384], Loss: 50.3259\n",
      "Epoch [246/300], Step [340/384], Loss: 73.4574\n",
      "Epoch [246/300], Step [360/384], Loss: 153.0007\n",
      "Epoch [246/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [247/300], Step [20/384], Loss: 255.3375\n",
      "Epoch [247/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [247/300], Step [60/384], Loss: 127.3275\n",
      "Epoch [247/300], Step [80/384], Loss: 7.9448\n",
      "Epoch [247/300], Step [100/384], Loss: 131.8011\n",
      "Epoch [247/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [247/300], Step [140/384], Loss: 73.9606\n",
      "Epoch [247/300], Step [160/384], Loss: 78.9276\n",
      "Epoch [247/300], Step [180/384], Loss: 431.1266\n",
      "Epoch [247/300], Step [200/384], Loss: 27.0231\n",
      "Epoch [247/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [247/300], Step [240/384], Loss: 77.3296\n",
      "Epoch [247/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [247/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [247/300], Step [300/384], Loss: 42.2340\n",
      "Epoch [247/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [247/300], Step [340/384], Loss: 53.6270\n",
      "Epoch [247/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [247/300], Step [380/384], Loss: 2.2117\n",
      "Epoch [248/300], Step [20/384], Loss: 65.0558\n",
      "Epoch [248/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [248/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [248/300], Step [80/384], Loss: 59.1805\n",
      "Epoch [248/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [248/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [248/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [248/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [248/300], Step [180/384], Loss: 22.9651\n",
      "Epoch [248/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [248/300], Step [220/384], Loss: 21.5419\n",
      "Epoch [248/300], Step [240/384], Loss: 47.8582\n",
      "Epoch [248/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [248/300], Step [280/384], Loss: 358.5600\n",
      "Epoch [248/300], Step [300/384], Loss: 41.0973\n",
      "Epoch [248/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [248/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [248/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [248/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [40/384], Loss: 73.9561\n",
      "Epoch [249/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [80/384], Loss: 5.0574\n",
      "Epoch [249/300], Step [100/384], Loss: 12.2852\n",
      "Epoch [249/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [260/384], Loss: 39.5810\n",
      "Epoch [249/300], Step [280/384], Loss: 14.2507\n",
      "Epoch [249/300], Step [300/384], Loss: 0.0178\n",
      "Epoch [249/300], Step [320/384], Loss: 89.5519\n",
      "Epoch [249/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [250/300], Step [20/384], Loss: 0.0033\n",
      "Epoch [250/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [250/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [250/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [250/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [250/300], Step [120/384], Loss: 24.1650\n",
      "Epoch [250/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [250/300], Step [160/384], Loss: 97.7811\n",
      "Epoch [250/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [250/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [250/300], Step [220/384], Loss: 62.9160\n",
      "Epoch [250/300], Step [240/384], Loss: 13.6038\n",
      "Epoch [250/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [250/300], Step [280/384], Loss: 38.1483\n",
      "Epoch [250/300], Step [300/384], Loss: 68.4570\n",
      "Epoch [250/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [250/300], Step [340/384], Loss: 27.8180\n",
      "Epoch [250/300], Step [360/384], Loss: 138.6422\n",
      "Epoch [250/300], Step [380/384], Loss: 84.1433\n",
      "Epoch [251/300], Step [20/384], Loss: 33.7140\n",
      "Epoch [251/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [60/384], Loss: 106.0625\n",
      "Epoch [251/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [140/384], Loss: 8.3289\n",
      "Epoch [251/300], Step [160/384], Loss: 31.3547\n",
      "Epoch [251/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [260/384], Loss: 80.5735\n",
      "Epoch [251/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [300/384], Loss: 0.6828\n",
      "Epoch [251/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [380/384], Loss: 61.1711\n",
      "Epoch [252/300], Step [20/384], Loss: 55.8574\n",
      "Epoch [252/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [252/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [252/300], Step [80/384], Loss: 32.5948\n",
      "Epoch [252/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [252/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [252/300], Step [140/384], Loss: 0.0072\n",
      "Epoch [252/300], Step [160/384], Loss: 26.7922\n",
      "Epoch [252/300], Step [180/384], Loss: 82.1434\n",
      "Epoch [252/300], Step [200/384], Loss: 59.9339\n",
      "Epoch [252/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [252/300], Step [240/384], Loss: 18.1696\n",
      "Epoch [252/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [252/300], Step [280/384], Loss: 82.9664\n",
      "Epoch [252/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [252/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [252/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [252/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [252/300], Step [380/384], Loss: 4.8595\n",
      "Epoch [253/300], Step [20/384], Loss: 94.0465\n",
      "Epoch [253/300], Step [40/384], Loss: 61.2078\n",
      "Epoch [253/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [80/384], Loss: 34.6915\n",
      "Epoch [253/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [200/384], Loss: 44.1229\n",
      "Epoch [253/300], Step [220/384], Loss: 108.9709\n",
      "Epoch [253/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [280/384], Loss: 28.2581\n",
      "Epoch [253/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [100/384], Loss: 37.6319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [254/300], Step [120/384], Loss: 33.0976\n",
      "Epoch [254/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [180/384], Loss: 88.9466\n",
      "Epoch [254/300], Step [200/384], Loss: 133.5982\n",
      "Epoch [254/300], Step [220/384], Loss: 55.0255\n",
      "Epoch [254/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [300/384], Loss: 258.6114\n",
      "Epoch [254/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [360/384], Loss: 27.2569\n",
      "Epoch [254/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [255/300], Step [20/384], Loss: 72.4472\n",
      "Epoch [255/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [255/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [255/300], Step [80/384], Loss: 34.7027\n",
      "Epoch [255/300], Step [100/384], Loss: 82.3471\n",
      "Epoch [255/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [255/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [255/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [255/300], Step [180/384], Loss: 114.2612\n",
      "Epoch [255/300], Step [200/384], Loss: 42.5316\n",
      "Epoch [255/300], Step [220/384], Loss: 11.8053\n",
      "Epoch [255/300], Step [240/384], Loss: 205.6674\n",
      "Epoch [255/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [255/300], Step [280/384], Loss: 147.8683\n",
      "Epoch [255/300], Step [300/384], Loss: 5.9961\n",
      "Epoch [255/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [255/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [255/300], Step [360/384], Loss: 39.1509\n",
      "Epoch [255/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [256/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [256/300], Step [40/384], Loss: 106.8118\n",
      "Epoch [256/300], Step [60/384], Loss: 111.8559\n",
      "Epoch [256/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [256/300], Step [100/384], Loss: 127.4570\n",
      "Epoch [256/300], Step [120/384], Loss: 5.2044\n",
      "Epoch [256/300], Step [140/384], Loss: 0.5685\n",
      "Epoch [256/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [256/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [256/300], Step [200/384], Loss: 113.4824\n",
      "Epoch [256/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [256/300], Step [240/384], Loss: 40.6351\n",
      "Epoch [256/300], Step [260/384], Loss: 46.3215\n",
      "Epoch [256/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [256/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [256/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [256/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [256/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [256/300], Step [380/384], Loss: 122.5369\n",
      "Epoch [257/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [257/300], Step [40/384], Loss: 13.1507\n",
      "Epoch [257/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [257/300], Step [80/384], Loss: 268.7841\n",
      "Epoch [257/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [257/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [257/300], Step [140/384], Loss: 16.4703\n",
      "Epoch [257/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [257/300], Step [180/384], Loss: 131.7760\n",
      "Epoch [257/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [257/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [257/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [257/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [257/300], Step [280/384], Loss: 170.1481\n",
      "Epoch [257/300], Step [300/384], Loss: 22.1340\n",
      "Epoch [257/300], Step [320/384], Loss: 60.0998\n",
      "Epoch [257/300], Step [340/384], Loss: 76.2727\n",
      "Epoch [257/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [257/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [100/384], Loss: 0.0048\n",
      "Epoch [258/300], Step [120/384], Loss: 42.7149\n",
      "Epoch [258/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [220/384], Loss: 37.1093\n",
      "Epoch [258/300], Step [240/384], Loss: 54.3055\n",
      "Epoch [258/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [360/384], Loss: 60.0388\n",
      "Epoch [258/300], Step [380/384], Loss: 54.3547\n",
      "Epoch [259/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [80/384], Loss: 26.0553\n",
      "Epoch [259/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [140/384], Loss: 67.5285\n",
      "Epoch [259/300], Step [160/384], Loss: 29.0594\n",
      "Epoch [259/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [240/384], Loss: 106.9022\n",
      "Epoch [259/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [380/384], Loss: 43.5251\n",
      "Epoch [260/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [260/300], Step [40/384], Loss: 76.4121\n",
      "Epoch [260/300], Step [60/384], Loss: 27.7657\n",
      "Epoch [260/300], Step [80/384], Loss: 62.8455\n",
      "Epoch [260/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [260/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [260/300], Step [140/384], Loss: 27.0362\n",
      "Epoch [260/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [260/300], Step [180/384], Loss: 99.1746\n",
      "Epoch [260/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [260/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [260/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [260/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [260/300], Step [280/384], Loss: 171.1010\n",
      "Epoch [260/300], Step [300/384], Loss: 218.4483\n",
      "Epoch [260/300], Step [320/384], Loss: 41.8976\n",
      "Epoch [260/300], Step [340/384], Loss: 47.0188\n",
      "Epoch [260/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [260/300], Step [380/384], Loss: 5.0415\n",
      "Epoch [261/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [100/384], Loss: 15.4007\n",
      "Epoch [261/300], Step [120/384], Loss: 83.9212\n",
      "Epoch [261/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [200/384], Loss: 128.4731\n",
      "Epoch [261/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [260/384], Loss: 6.5086\n",
      "Epoch [261/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [20/384], Loss: 2.9776\n",
      "Epoch [262/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [60/384], Loss: 67.1095\n",
      "Epoch [262/300], Step [80/384], Loss: 31.3910\n",
      "Epoch [262/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [120/384], Loss: 32.5674\n",
      "Epoch [262/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [160/384], Loss: 38.7082\n",
      "Epoch [262/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [240/384], Loss: 88.4768\n",
      "Epoch [262/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [360/384], Loss: 74.3247\n",
      "Epoch [262/300], Step [380/384], Loss: 68.4992\n",
      "Epoch [263/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [60/384], Loss: 11.0299\n",
      "Epoch [263/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [100/384], Loss: 89.2161\n",
      "Epoch [263/300], Step [120/384], Loss: 0.0023\n",
      "Epoch [263/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [240/384], Loss: 101.7329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [263/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [300/384], Loss: 19.3329\n",
      "Epoch [263/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [20/384], Loss: 20.3769\n",
      "Epoch [264/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [120/384], Loss: 5.2632\n",
      "Epoch [264/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [180/384], Loss: 41.9492\n",
      "Epoch [264/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [220/384], Loss: 7.1369\n",
      "Epoch [264/300], Step [240/384], Loss: 11.7757\n",
      "Epoch [264/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [280/384], Loss: 38.9424\n",
      "Epoch [264/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [380/384], Loss: 32.3631\n",
      "Epoch [265/300], Step [20/384], Loss: 42.2380\n",
      "Epoch [265/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [160/384], Loss: 15.0856\n",
      "Epoch [265/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [220/384], Loss: 207.2833\n",
      "Epoch [265/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [380/384], Loss: 171.1775\n",
      "Epoch [266/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [266/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [266/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [266/300], Step [80/384], Loss: 94.8518\n",
      "Epoch [266/300], Step [100/384], Loss: 59.0183\n",
      "Epoch [266/300], Step [120/384], Loss: 77.7776\n",
      "Epoch [266/300], Step [140/384], Loss: 47.9432\n",
      "Epoch [266/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [266/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [266/300], Step [200/384], Loss: 89.1387\n",
      "Epoch [266/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [266/300], Step [240/384], Loss: 38.1400\n",
      "Epoch [266/300], Step [260/384], Loss: 43.9076\n",
      "Epoch [266/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [266/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [266/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [266/300], Step [340/384], Loss: 104.2985\n",
      "Epoch [266/300], Step [360/384], Loss: 9.3322\n",
      "Epoch [266/300], Step [380/384], Loss: 20.1442\n",
      "Epoch [267/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [267/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [267/300], Step [60/384], Loss: 5.2162\n",
      "Epoch [267/300], Step [80/384], Loss: 178.6407\n",
      "Epoch [267/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [267/300], Step [120/384], Loss: 33.5749\n",
      "Epoch [267/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [267/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [267/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [267/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [267/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [267/300], Step [240/384], Loss: 59.4263\n",
      "Epoch [267/300], Step [260/384], Loss: 58.3329\n",
      "Epoch [267/300], Step [280/384], Loss: 131.8516\n",
      "Epoch [267/300], Step [300/384], Loss: 89.9921\n",
      "Epoch [267/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [267/300], Step [340/384], Loss: 161.7238\n",
      "Epoch [267/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [267/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [268/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [268/300], Step [40/384], Loss: 27.7188\n",
      "Epoch [268/300], Step [60/384], Loss: 19.2679\n",
      "Epoch [268/300], Step [80/384], Loss: 9.2011\n",
      "Epoch [268/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [268/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [268/300], Step [140/384], Loss: 141.9817\n",
      "Epoch [268/300], Step [160/384], Loss: 94.1091\n",
      "Epoch [268/300], Step [180/384], Loss: 24.4521\n",
      "Epoch [268/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [268/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [268/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [268/300], Step [260/384], Loss: 47.9012\n",
      "Epoch [268/300], Step [280/384], Loss: 31.1282\n",
      "Epoch [268/300], Step [300/384], Loss: 46.7554\n",
      "Epoch [268/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [268/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [268/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [268/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [60/384], Loss: 23.1658\n",
      "Epoch [269/300], Step [80/384], Loss: 92.4819\n",
      "Epoch [269/300], Step [100/384], Loss: 32.3275\n",
      "Epoch [269/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [140/384], Loss: 33.1224\n",
      "Epoch [269/300], Step [160/384], Loss: 42.5858\n",
      "Epoch [269/300], Step [180/384], Loss: 23.1141\n",
      "Epoch [269/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [240/384], Loss: 114.0403\n",
      "Epoch [269/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [380/384], Loss: 6.2340\n",
      "Epoch [270/300], Step [20/384], Loss: 63.1711\n",
      "Epoch [270/300], Step [40/384], Loss: 59.5585\n",
      "Epoch [270/300], Step [60/384], Loss: 0.2946\n",
      "Epoch [270/300], Step [80/384], Loss: 27.3939\n",
      "Epoch [270/300], Step [100/384], Loss: 83.9197\n",
      "Epoch [270/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [270/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [270/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [270/300], Step [180/384], Loss: 15.9616\n",
      "Epoch [270/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [270/300], Step [220/384], Loss: 0.0033\n",
      "Epoch [270/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [270/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [270/300], Step [280/384], Loss: 35.8239\n",
      "Epoch [270/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [270/300], Step [320/384], Loss: 24.0146\n",
      "Epoch [270/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [270/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [270/300], Step [380/384], Loss: 240.4818\n",
      "Epoch [271/300], Step [20/384], Loss: 36.9740\n",
      "Epoch [271/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [271/300], Step [60/384], Loss: 37.9185\n",
      "Epoch [271/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [271/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [271/300], Step [120/384], Loss: 82.5957\n",
      "Epoch [271/300], Step [140/384], Loss: 208.6537\n",
      "Epoch [271/300], Step [160/384], Loss: 22.1143\n",
      "Epoch [271/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [271/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [271/300], Step [220/384], Loss: 60.2117\n",
      "Epoch [271/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [271/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [271/300], Step [280/384], Loss: 37.6662\n",
      "Epoch [271/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [271/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [271/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [271/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [271/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [100/384], Loss: 52.3472\n",
      "Epoch [272/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [200/384], Loss: 47.6699\n",
      "Epoch [272/300], Step [220/384], Loss: 12.0395\n",
      "Epoch [272/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [340/384], Loss: 43.7836\n",
      "Epoch [272/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [380/384], Loss: 50.3142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [273/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [273/300], Step [40/384], Loss: 0.0129\n",
      "Epoch [273/300], Step [60/384], Loss: 80.5161\n",
      "Epoch [273/300], Step [80/384], Loss: 33.9674\n",
      "Epoch [273/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [273/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [273/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [273/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [273/300], Step [180/384], Loss: 74.3732\n",
      "Epoch [273/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [273/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [273/300], Step [240/384], Loss: 16.8085\n",
      "Epoch [273/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [273/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [273/300], Step [300/384], Loss: 49.3534\n",
      "Epoch [273/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [273/300], Step [340/384], Loss: 63.7593\n",
      "Epoch [273/300], Step [360/384], Loss: 13.5928\n",
      "Epoch [273/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [274/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [274/300], Step [40/384], Loss: 45.6261\n",
      "Epoch [274/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [274/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [274/300], Step [100/384], Loss: 139.1039\n",
      "Epoch [274/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [274/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [274/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [274/300], Step [180/384], Loss: 34.4031\n",
      "Epoch [274/300], Step [200/384], Loss: 231.8695\n",
      "Epoch [274/300], Step [220/384], Loss: 0.0003\n",
      "Epoch [274/300], Step [240/384], Loss: 37.7814\n",
      "Epoch [274/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [274/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [274/300], Step [300/384], Loss: 0.0300\n",
      "Epoch [274/300], Step [320/384], Loss: 18.7115\n",
      "Epoch [274/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [274/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [274/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [275/300], Step [20/384], Loss: 23.3202\n",
      "Epoch [275/300], Step [40/384], Loss: 55.2640\n",
      "Epoch [275/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [275/300], Step [80/384], Loss: 82.7666\n",
      "Epoch [275/300], Step [100/384], Loss: 7.0767\n",
      "Epoch [275/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [275/300], Step [140/384], Loss: 104.7201\n",
      "Epoch [275/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [275/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [275/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [275/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [275/300], Step [240/384], Loss: 28.1269\n",
      "Epoch [275/300], Step [260/384], Loss: 8.3471\n",
      "Epoch [275/300], Step [280/384], Loss: 32.3690\n",
      "Epoch [275/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [275/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [275/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [275/300], Step [360/384], Loss: 118.1242\n",
      "Epoch [275/300], Step [380/384], Loss: 55.0865\n",
      "Epoch [276/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [276/300], Step [40/384], Loss: 0.3253\n",
      "Epoch [276/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [276/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [276/300], Step [100/384], Loss: 180.1876\n",
      "Epoch [276/300], Step [120/384], Loss: 62.9082\n",
      "Epoch [276/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [276/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [276/300], Step [180/384], Loss: 11.0083\n",
      "Epoch [276/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [276/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [276/300], Step [240/384], Loss: 40.2837\n",
      "Epoch [276/300], Step [260/384], Loss: 107.4022\n",
      "Epoch [276/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [276/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [276/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [276/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [276/300], Step [360/384], Loss: 34.4405\n",
      "Epoch [276/300], Step [380/384], Loss: 26.8931\n",
      "Epoch [277/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [277/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [277/300], Step [60/384], Loss: 4.8342\n",
      "Epoch [277/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [277/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [277/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [277/300], Step [140/384], Loss: 51.5035\n",
      "Epoch [277/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [277/300], Step [180/384], Loss: 48.7413\n",
      "Epoch [277/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [277/300], Step [220/384], Loss: 27.2424\n",
      "Epoch [277/300], Step [240/384], Loss: 6.9745\n",
      "Epoch [277/300], Step [260/384], Loss: 59.6449\n",
      "Epoch [277/300], Step [280/384], Loss: 102.8775\n",
      "Epoch [277/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [277/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [277/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [277/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [277/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [278/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [278/300], Step [40/384], Loss: 23.0410\n",
      "Epoch [278/300], Step [60/384], Loss: 58.4922\n",
      "Epoch [278/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [278/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [278/300], Step [120/384], Loss: 45.8273\n",
      "Epoch [278/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [278/300], Step [160/384], Loss: 30.7183\n",
      "Epoch [278/300], Step [180/384], Loss: 26.5316\n",
      "Epoch [278/300], Step [200/384], Loss: 153.0775\n",
      "Epoch [278/300], Step [220/384], Loss: 183.7869\n",
      "Epoch [278/300], Step [240/384], Loss: 76.0514\n",
      "Epoch [278/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [278/300], Step [280/384], Loss: 41.5558\n",
      "Epoch [278/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [278/300], Step [320/384], Loss: 58.3387\n",
      "Epoch [278/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [278/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [278/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [100/384], Loss: 113.0014\n",
      "Epoch [279/300], Step [120/384], Loss: 185.5464\n",
      "Epoch [279/300], Step [140/384], Loss: 55.2765\n",
      "Epoch [279/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [240/384], Loss: 127.1859\n",
      "Epoch [279/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [280/384], Loss: 299.3948\n",
      "Epoch [279/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [100/384], Loss: 10.2822\n",
      "Epoch [280/300], Step [120/384], Loss: 136.2934\n",
      "Epoch [280/300], Step [140/384], Loss: 0.8099\n",
      "Epoch [280/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [260/384], Loss: 73.0617\n",
      "Epoch [280/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [40/384], Loss: 22.2261\n",
      "Epoch [281/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [160/384], Loss: 0.0006\n",
      "Epoch [281/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [260/384], Loss: 35.4527\n",
      "Epoch [281/300], Step [280/384], Loss: 62.6442\n",
      "Epoch [281/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [340/384], Loss: 155.9868\n",
      "Epoch [281/300], Step [360/384], Loss: 9.5032\n",
      "Epoch [281/300], Step [380/384], Loss: 35.6678\n",
      "Epoch [282/300], Step [20/384], Loss: 358.0121\n",
      "Epoch [282/300], Step [40/384], Loss: 44.8021\n",
      "Epoch [282/300], Step [60/384], Loss: 5.7394\n",
      "Epoch [282/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [282/300], Step [100/384], Loss: 90.0691\n",
      "Epoch [282/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [282/300], Step [140/384], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [282/300], Step [160/384], Loss: 93.8688\n",
      "Epoch [282/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [282/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [282/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [282/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [282/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [282/300], Step [280/384], Loss: 47.1006\n",
      "Epoch [282/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [282/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [282/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [282/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [282/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [283/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [283/300], Step [40/384], Loss: 134.5718\n",
      "Epoch [283/300], Step [60/384], Loss: 31.5957\n",
      "Epoch [283/300], Step [80/384], Loss: 47.8037\n",
      "Epoch [283/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [283/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [283/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [283/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [283/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [283/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [283/300], Step [220/384], Loss: 31.5048\n",
      "Epoch [283/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [283/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [283/300], Step [280/384], Loss: 105.2891\n",
      "Epoch [283/300], Step [300/384], Loss: 32.9624\n",
      "Epoch [283/300], Step [320/384], Loss: 65.8782\n",
      "Epoch [283/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [283/300], Step [360/384], Loss: 1.3048\n",
      "Epoch [283/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [20/384], Loss: 13.4299\n",
      "Epoch [284/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [80/384], Loss: 109.2609\n",
      "Epoch [284/300], Step [100/384], Loss: 6.9730\n",
      "Epoch [284/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [160/384], Loss: 166.2358\n",
      "Epoch [284/300], Step [180/384], Loss: 24.2707\n",
      "Epoch [284/300], Step [200/384], Loss: 29.6176\n",
      "Epoch [284/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [380/384], Loss: 127.2257\n",
      "Epoch [285/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [285/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [285/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [285/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [285/300], Step [100/384], Loss: 193.8471\n",
      "Epoch [285/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [285/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [285/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [285/300], Step [180/384], Loss: 55.6398\n",
      "Epoch [285/300], Step [200/384], Loss: 13.8493\n",
      "Epoch [285/300], Step [220/384], Loss: 94.3523\n",
      "Epoch [285/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [285/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [285/300], Step [280/384], Loss: 67.5981\n",
      "Epoch [285/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [285/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [285/300], Step [340/384], Loss: 66.9236\n",
      "Epoch [285/300], Step [360/384], Loss: 48.3750\n",
      "Epoch [285/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [120/384], Loss: 85.9908\n",
      "Epoch [286/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [200/384], Loss: 41.0848\n",
      "Epoch [286/300], Step [220/384], Loss: 27.4672\n",
      "Epoch [286/300], Step [240/384], Loss: 37.9919\n",
      "Epoch [286/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [280/384], Loss: 52.7858\n",
      "Epoch [286/300], Step [300/384], Loss: 13.3011\n",
      "Epoch [286/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [360/384], Loss: 54.6545\n",
      "Epoch [286/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [20/384], Loss: 136.1518\n",
      "Epoch [287/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [80/384], Loss: 188.8861\n",
      "Epoch [287/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [240/384], Loss: 23.7410\n",
      "Epoch [287/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [300/384], Loss: 6.3519\n",
      "Epoch [287/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [340/384], Loss: 148.4376\n",
      "Epoch [287/300], Step [360/384], Loss: 0.1414\n",
      "Epoch [287/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [220/384], Loss: 85.5675\n",
      "Epoch [288/300], Step [240/384], Loss: 1.2047\n",
      "Epoch [288/300], Step [260/384], Loss: 73.3889\n",
      "Epoch [288/300], Step [280/384], Loss: 91.1335\n",
      "Epoch [288/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [320/384], Loss: 45.8490\n",
      "Epoch [288/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [380/384], Loss: 34.3999\n",
      "Epoch [289/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [160/384], Loss: 76.1031\n",
      "Epoch [289/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [200/384], Loss: 36.2174\n",
      "Epoch [289/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [360/384], Loss: 101.0570\n",
      "Epoch [289/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [290/300], Step [20/384], Loss: 5.0094\n",
      "Epoch [290/300], Step [40/384], Loss: 37.1544\n",
      "Epoch [290/300], Step [60/384], Loss: 43.9057\n",
      "Epoch [290/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [290/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [290/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [290/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [290/300], Step [160/384], Loss: 103.5307\n",
      "Epoch [290/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [290/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [290/300], Step [220/384], Loss: 3.8004\n",
      "Epoch [290/300], Step [240/384], Loss: 83.4071\n",
      "Epoch [290/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [290/300], Step [280/384], Loss: 120.5472\n",
      "Epoch [290/300], Step [300/384], Loss: 95.9044\n",
      "Epoch [290/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [290/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [290/300], Step [360/384], Loss: 12.7770\n",
      "Epoch [290/300], Step [380/384], Loss: 9.5616\n",
      "Epoch [291/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [291/300], Step [40/384], Loss: 124.9764\n",
      "Epoch [291/300], Step [60/384], Loss: 3.1092\n",
      "Epoch [291/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [291/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [291/300], Step [120/384], Loss: 85.7068\n",
      "Epoch [291/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [291/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [291/300], Step [180/384], Loss: 19.0413\n",
      "Epoch [291/300], Step [200/384], Loss: 188.1875\n",
      "Epoch [291/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [291/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [291/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [291/300], Step [280/384], Loss: 64.4530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [291/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [291/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [291/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [291/300], Step [360/384], Loss: 74.0528\n",
      "Epoch [291/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [40/384], Loss: 74.9503\n",
      "Epoch [292/300], Step [60/384], Loss: 126.1653\n",
      "Epoch [292/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [100/384], Loss: 85.9443\n",
      "Epoch [292/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [160/384], Loss: 58.9848\n",
      "Epoch [292/300], Step [180/384], Loss: 127.0925\n",
      "Epoch [292/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [260/384], Loss: 35.0548\n",
      "Epoch [292/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [60/384], Loss: 103.2340\n",
      "Epoch [293/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [340/384], Loss: 21.7182\n",
      "Epoch [293/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [60/384], Loss: 110.1307\n",
      "Epoch [294/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [100/384], Loss: 31.2099\n",
      "Epoch [294/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [160/384], Loss: 106.4115\n",
      "Epoch [294/300], Step [180/384], Loss: 48.2011\n",
      "Epoch [294/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [300/384], Loss: 35.1571\n",
      "Epoch [294/300], Step [320/384], Loss: 3.6799\n",
      "Epoch [294/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [380/384], Loss: 123.8508\n",
      "Epoch [295/300], Step [20/384], Loss: 82.8270\n",
      "Epoch [295/300], Step [40/384], Loss: 5.2310\n",
      "Epoch [295/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [100/384], Loss: 16.7740\n",
      "Epoch [295/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [240/384], Loss: 58.6453\n",
      "Epoch [295/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [340/384], Loss: 95.1783\n",
      "Epoch [295/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [80/384], Loss: 54.3144\n",
      "Epoch [296/300], Step [100/384], Loss: 18.5201\n",
      "Epoch [296/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [200/384], Loss: 2.4667\n",
      "Epoch [296/300], Step [220/384], Loss: 19.3458\n",
      "Epoch [296/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [300/384], Loss: 8.2117\n",
      "Epoch [296/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [340/384], Loss: 81.4016\n",
      "Epoch [296/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [380/384], Loss: 2.3676\n",
      "Epoch [297/300], Step [20/384], Loss: 87.3532\n",
      "Epoch [297/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [80/384], Loss: 14.0854\n",
      "Epoch [297/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [160/384], Loss: 75.1780\n",
      "Epoch [297/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [200/384], Loss: 224.2022\n",
      "Epoch [297/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [260/384], Loss: 8.9636\n",
      "Epoch [297/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [340/384], Loss: 16.0414\n",
      "Epoch [297/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [298/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [298/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [298/300], Step [60/384], Loss: 13.4189\n",
      "Epoch [298/300], Step [80/384], Loss: 31.1840\n",
      "Epoch [298/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [298/300], Step [120/384], Loss: 17.5542\n",
      "Epoch [298/300], Step [140/384], Loss: 15.2004\n",
      "Epoch [298/300], Step [160/384], Loss: 0.0278\n",
      "Epoch [298/300], Step [180/384], Loss: 28.5649\n",
      "Epoch [298/300], Step [200/384], Loss: 190.2986\n",
      "Epoch [298/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [298/300], Step [240/384], Loss: 67.0334\n",
      "Epoch [298/300], Step [260/384], Loss: 112.2268\n",
      "Epoch [298/300], Step [280/384], Loss: 11.7679\n",
      "Epoch [298/300], Step [300/384], Loss: 30.9709\n",
      "Epoch [298/300], Step [320/384], Loss: 2.2993\n",
      "Epoch [298/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [298/300], Step [360/384], Loss: 49.0378\n",
      "Epoch [298/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [299/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [299/300], Step [40/384], Loss: 43.8250\n",
      "Epoch [299/300], Step [60/384], Loss: 222.0854\n",
      "Epoch [299/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [299/300], Step [100/384], Loss: 41.5811\n",
      "Epoch [299/300], Step [120/384], Loss: 17.1269\n",
      "Epoch [299/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [299/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [299/300], Step [180/384], Loss: 1.2509\n",
      "Epoch [299/300], Step [200/384], Loss: 87.8938\n",
      "Epoch [299/300], Step [220/384], Loss: 28.4599\n",
      "Epoch [299/300], Step [240/384], Loss: 56.7060\n",
      "Epoch [299/300], Step [260/384], Loss: 23.0064\n",
      "Epoch [299/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [299/300], Step [300/384], Loss: 21.2191\n",
      "Epoch [299/300], Step [320/384], Loss: 238.6537\n",
      "Epoch [299/300], Step [340/384], Loss: 32.8078\n",
      "Epoch [299/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [299/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [120/384], Loss: 5.3629\n",
      "Epoch [300/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [200/384], Loss: 0.2509\n",
      "Epoch [300/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [260/384], Loss: 60.3029\n",
      "Epoch [300/300], Step [280/384], Loss: 32.1194\n",
      "Epoch [300/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [380/384], Loss: 49.7916\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "learning_rate = 0.008\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_list = []\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (window, labels) in enumerate(train_loader):\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(window.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np_loss_list = []\n",
    "loss_total_epoch = 0\n",
    "for iterator, loss in enumerate(loss_list):\n",
    "    loss_total_epoch += loss.detach().numpy()\n",
    "    if iterator%len(train_loader) == 0:\n",
    "        np_loss_list.append(loss_total_epoch)\n",
    "        loss_total_epoch =0 \n",
    "\n",
    "plt.plot(np_loss_list)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 90.36458333333333 %\n",
      "Accuracy of BSD_11: 86.91588785046729 %\n",
      "Accuracy of BSD_21: 96.84210526315789 %\n",
      "Accuracy of BSD_31: 84.46601941747574 %\n",
      "Accuracy of BSD_P1: 94.9367088607595 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    classes = ['BSD_11', 'BSD_21', 'BSD_31', 'BSD_P1']\n",
    "    \n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(4)]\n",
    "    n_class_samples = [0 for i in range(4)]\n",
    "    n_class_samples_out = [0 for i in range(4)]\n",
    "    for window, labels in test_loader:\n",
    "        outputs = model(window.float())\n",
    "        for i in range(batch_size):\n",
    "            if len(labels)==4:\n",
    "                label = labels[i]\n",
    "                output = torch.argmax(outputs[i])\n",
    "                if label == output:\n",
    "                    n_correct+=1\n",
    "                    n_class_correct[label]+=1\n",
    "\n",
    "                n_samples+=1\n",
    "                n_class_samples[label]+=1\n",
    "                n_class_samples_out[output]+=1\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(4):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[96, 117, 88, 83]\n",
      "[107, 95, 103, 79]\n",
      "[93, 92, 87, 75]\n"
     ]
    }
   ],
   "source": [
    "print(n_class_samples_out)\n",
    "print(n_class_samples)\n",
    "print(n_class_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "test_list = ['A', 'B']\n",
    "test_string = \"DES\"\n",
    "\n",
    "res = any(ele in test_string for ele in test_list)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1688, -0.3698,  1.2154, -1.2929,  0.2711],\n",
      "        [-1.4512, -0.5559,  0.1435, -2.4172,  0.3523],\n",
      "        [ 0.8883, -0.0566,  0.8174,  1.0130,  1.3393]], requires_grad=True)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 1, 0])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
