{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(data, window_size, overlap_size):\n",
    "    \n",
    "    #faster implementation but no overlapping function is possible\n",
    "\n",
    "    splits = np.shape(data)[0]//window_size # number of splits\n",
    "    data = data[:splits*window_size] #cut off end of array such that array can be split equaly\n",
    "    data = data.reshape((splits,-1,np.shape(data)[1])) #split array in windows\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from skimage.util.shape import view_as_windows\n",
    "import warnings\n",
    "\n",
    "def load_data(data, window_size, overlap_size):\n",
    "    \"\"\"\n",
    "    Split data in windows of equal size with overlap\n",
    "    \n",
    "    INPUT:\n",
    "    @data: data numpy array of shape [elements per file, features]\n",
    "    @window: number of elements per window\n",
    "    @overlap_size: defines the overlapping elements between consecutive windows\n",
    "    \n",
    "    OUTPUT\n",
    "    @data: data numpy array of shape [number_of_windows, elements per window, features]\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if window_size==overlap_size:\n",
    "        raise Exception(\"Overlap arg must be smaller than length of windows\")\n",
    "    S = window_size - overlap_size\n",
    "    nd0 = ((len(data)-window_size)//S)+1\n",
    "    if nd0*S-S!=len(data)-window_size:\n",
    "        warnings.warn(\"Not all elements were covered\")\n",
    "    return view_as_windows(data, (window_size,data.shape[1]), step=S)[:,0,:,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_nan_element(data_with_nan):\n",
    "    \"\"\"\n",
    "    Delete all elements in the data which have any nan valued feature\n",
    "    \n",
    "    INPUT:\n",
    "    @data_with_nan: data numpy array containing nan_values\n",
    "    \n",
    "    OUTPUT\n",
    "    @data_with_nan: data numpy array inlcuding just elements per window which do have no nan_vaues in any feature\n",
    "    \"\"\"\n",
    "    nan_val = np.isnan(data_with_nan) #mask for all nan_elements as 2d array [elements_per_window, features]\n",
    "    nan_val = np.any(nan_val,axis = 1) #mask for all nan_rows as 1d array [elements_per_window]\n",
    "    return data_with_nan[nan_val==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_folder_dictionary(list_of_train_BSD_states, list_of_test_BSD_states, data_path):\n",
    "    \n",
    "    data_path = data_path\n",
    "    training_folders = {}\n",
    "    testing_folders = {}\n",
    "    \n",
    "    #Sorting the individual folders by findinding the BSD_states in the folder names\n",
    "    for data_path_element in os.listdir(data_path):\n",
    "        if any(element in data_path_element for element in list_of_train_BSD_states): \n",
    "            training_folders[data_path_element]  = os.listdir(os.path.join(data_path,data_path_element))\n",
    "        elif any(element in data_path_element for element in list_of_test_BSD_states): \n",
    "            testing_folders[data_path_element] = os.listdir(os.path.join(data_path,data_path_element))\n",
    "    return training_folders, testing_folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder_dictionary(list_of_train_BSD_states, list_of_test_BSD_states, data_path):\n",
    "    \"\"\"\n",
    "    Create a dictionaty for testing and training containing folder names as keys and files as values\n",
    "    \n",
    "    INPUT:\n",
    "    @list_of_train_BSD_states: list containing the training BSD states as string\n",
    "    @list_of_test_BSD_states: list containing the testing BSD states as string\n",
    "    @data_path: data directory containing folders for each BSD state\n",
    "    \n",
    "    OUTPUT\n",
    "    @training_folders: dictionary folders and keys for training\n",
    "    @testing_folders: dictionary folders and keys for testing\n",
    "    \"\"\"\n",
    "    \n",
    "    data_path = data_path\n",
    "    state_dictionary = {\n",
    "        \"1\":\"NR01_20200317_PGS_31_BSD_31\",\n",
    "        \"2\":\"NR02_20200423_PGS_31_BSD_21\",\n",
    "        \"3\":\"NR03_20200424_PGS_31_BSD_11\",\n",
    "        \"4\":\"NR04_20200424_PGS_31_BSD_P1\",\n",
    "        \"5\":\"NR05_20200930_PGS_31_BSD_22\",\n",
    "        \"6\":\"NR06_20201001_PGS_31_BSD_12\",\n",
    "        \"7\":\"NR07_20201001_PGS_31_BSD_32\",\n",
    "        \"8\":\"NR08_20200918_PGS_31_BSD_33\",\n",
    "        \"9\":\"NR09_20200917_PGS_31_BSD_P2\",\n",
    "        \"10\":\"NR10_20200502_PGS_21_BSD_31\",\n",
    "        \"11\":\"NR11_20200429_PGS_21_BSD_21\",\n",
    "        \"12\":\"NR12_20200429_PGS_21_BSD_11\",\n",
    "        \"13\":\"NR13_20200428_PGS_21_BSD_P1\",\n",
    "        \"14\":\"NR14_20200731_PGS_21_BSD_22\",\n",
    "        \"15\":\"NR15_20200901_PGS_21_BSD_12\",\n",
    "        \"16\":\"NR16_20200908_PGS_21_BSD_32\",\n",
    "        \"17\":\"NR17_20200717_PGS_21_BSD_33\",\n",
    "        \"18\":\"NR18_20200714_PGS_21_BSD_P2\",\n",
    "        \"19\":\"NR19_20200505_PGS_11_BSD_31\",\n",
    "        \"20\":\"NR20_20200507_PGS_11_BSD_21\",\n",
    "        \"21\":\"NR21_20200508_PGS_11_BSD_11\",\n",
    "        \"22\":\"NR22_20200508_PGS_11_BSD_P1\",\n",
    "        \"23\":\"NR23_20200511_PGS_11_BSD_22\",\n",
    "        \"24\":\"NR24_20200512_PGS_11_BSD_12\",\n",
    "        \"25\":\"NR25_20200512_PGS_11_BSD_32\",\n",
    "        \"26\":\"NR26_20200513_PGS_11_BSD_33\",\n",
    "        \"27\":\"NR27_20200513_PGS_11_BSD_P2\",\n",
    "    }\n",
    "    \n",
    "    \n",
    "    training_folders = {}\n",
    "    testing_folders = {}\n",
    "    \n",
    "    for train_element in list_of_train_BSD_states:\n",
    "        training_folders[state_dictionary[train_element]]=os.listdir(os.path.join(data_path,state_dictionary[train_element]))\n",
    "    for test_element in list_of_test_BSD_states:\n",
    "        testing_folders[state_dictionary[test_element]]=os.listdir(os.path.join(data_path,state_dictionary[test_element]))\n",
    "    \n",
    "    return training_folders, testing_folders\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(path):\n",
    "    \"\"\"\n",
    "    Creates a list of all feature names\n",
    "    INPUT:\n",
    "    @path: path to any BSD file since the features are the same for all files\n",
    "    \n",
    "    OUTPUT\n",
    "    @features: list of features:\n",
    "    ['C:s_ist/X', 'C:s_soll/X', 'C:s_diff/X', 'C:v_(n_ist)/X', 'C:v_(n_soll)/X', 'C:P_mech./X', 'C:Pos._Diff./X',\n",
    "    'C:I_ist/X', 'C:I_soll/X', 'C:x_bottom', 'C:y_bottom', 'C:z_bottom', 'C:x_nut', 'C:y_nut', 'C:z_nut',\n",
    "    'C:x_top', 'C:y_top', 'C:z_top', 'D:s_ist/X', 'D:s_soll/X', 'D:s_diff/X', 'D:v_(n_ist)/X', 'D:v_(n_soll)/X',\n",
    "    'D:P_mech./X', 'D:Pos._Diff./X', 'D:I_ist/X', 'D:I_soll/X', 'D:x_bottom', 'D:y_bottom', 'D:z_bottom',\n",
    "    'D:x_nut', 'D:y_nut', 'D:z_nut', 'D:x_top', 'D:y_top', 'D:z_top', 'S:x_bottom', 'S:y_bottom', 'S:z_bottom',\n",
    "    'S:x_nut', 'S:y_nut', 'S:z_nut', 'S:x_top', 'S:y_top', 'S:z_top', 'S:Nominal_rotational_speed[rad/s]',\n",
    "    'S:Actual_rotational_speed[µm/s]', 'S:Actual_position_of_the_position_encoder(dy/dt)[µm/s]',\n",
    "    'S:Actual_position_of_the_motor_encoder(dy/dt)[µm/s]']\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "        csvreader = csv.reader(file)\n",
    "        features = next(csvreader)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_data_from_BSD_state(folders, data_path, features_of_interest, window_size, overlap_size):\n",
    "    \"\"\"\n",
    "    Concatenates all the windowed data from each file to one big torch array\n",
    "    INPUT:\n",
    "    @folders: dictionary containing folders (as keys) and files (as values) to downloaded\n",
    "    @data_path: data directory containing folders for each BSD state\n",
    "    @features_of_interest: list of features which should be included for training\n",
    "    @window_size: number of elements per widow\n",
    "    \n",
    "    OUTPUT:\n",
    "    @n_samples: number of total elements from all included files\n",
    "    @x_data: torch array containing all the data elements \n",
    "    @y_data: torch array containing the labels for all elements\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # arrays to collect data and label\n",
    "    x_data_concatenated = None\n",
    "    y_data_concatenated = None\n",
    "    \n",
    "    \n",
    "    iterator = 0\n",
    "    first = True\n",
    "    \n",
    "    \n",
    "    for BSD_path in folders.keys(): #folder path\n",
    "        for file_path in folders[BSD_path]: #file path \n",
    "            path_BSD_file = os.path.join(data_path, BSD_path, file_path) # concatenate the data_path, folder and file path\n",
    "            #in first iteration get a list if all features\n",
    "            if first == True:\n",
    "                features = get_features(path_BSD_file)\n",
    "            \n",
    "            data_BSD_file = np.genfromtxt(path_BSD_file, dtype = np.dtype('d'), delimiter=',')[1:,:] #write csv in numpy\n",
    "            feature_index_list = np.where(np.isin(features, features_of_interest)) #get index for all features of interest\n",
    "            data_BSD_file = data_BSD_file[:,feature_index_list] #slice numpy array such that just features of interest are included\n",
    "            data_BSD_file = np.squeeze(data_BSD_file, axis = 1) # one unnecessary extra dimension was created while slicing\n",
    "            data_BSD_file = del_nan_element(data_BSD_file) #delete all elements with any nan feature\n",
    "            data_BSD_file = load_data(data_BSD_file, window_size, overlap_size) #window the data\n",
    "            data_BSD_file = np.swapaxes(data_BSD_file,1,2) #swap axes for CNN\n",
    "            \n",
    "            \n",
    "            #rewrite labels as BSD_condition_1 = 0, BSD_condition_2 = 1, BSD_condition_3 = 2, BSD_condition_P1 = 3\n",
    "            label = BSD_path[-2] #take the first number of the BSD state for class label\n",
    "            if label == \"P\":\n",
    "                label = int(3)\n",
    "            else:\n",
    "                label =int(int(label)-1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #concatenate the data from each file in one numpy array\n",
    "            if  first == True: #overwrite variable\n",
    "                x_data_concatenated = np.copy(data_BSD_file)\n",
    "                y_data_concatenated = np.copy(np.asarray([label]*np.shape(data_BSD_file)[0]))\n",
    "                first = False\n",
    "            else: #concatenate data numpy arrays\n",
    "                x_data_concatenated = np.concatenate((x_data_concatenated, data_BSD_file), axis=0)\n",
    "                y_data_concatenated = np.concatenate((y_data_concatenated,np.asarray([label]*np.shape(data_BSD_file)[0])), axis=0)\n",
    "            \n",
    "            iterator +=1\n",
    "            print(f\"{iterator}/{len(folders.keys())*len(folders[list(folders.keys())[0]])} folders downloaded\")\n",
    "            print(f\"downloaded folder: {BSD_path}/{file_path}\")\n",
    "            print(f\"Shape of collected datafram: X_shape: {np.shape(x_data_concatenated)}, Y_shape: {np.shape(y_data_concatenated)}\")\n",
    "    \n",
    "    #generate torch array\n",
    "    n_samples = np.shape(x_data_concatenated)[0]\n",
    "    x_data = torch.from_numpy(x_data_concatenated)\n",
    "    y_data = torch.from_numpy(y_data_concatenated)\n",
    "    \n",
    "    return n_samples, x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesData(Dataset):\n",
    "    \"\"\"\n",
    "    Class for creating dataset using PyTorch data primitive Dataset. An instance of this class can be used in the \n",
    "    PyTorch data primitive Dataloader\n",
    "    \n",
    "    The following patameters can be adjusted:\n",
    "    @windwo_size: Size of window which is used as Input in CNN\n",
    "    @feature_of_interest: List of all features which should be used in the CNN\n",
    "    @list_of_train_BSD_states: List of BSD states which should be used for training. Be careful at least 4 BSD\n",
    "    states representing the 4 different classes should be included for the training\n",
    "    @list_of_test_BSD_states: List of BSD states which should be used for testing\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        window_size = 1024\n",
    "        overlap_size = 300\n",
    "        features_of_interest =     ['S:x_bottom', 'S:y_bottom', 'S:z_bottom',\n",
    "    'S:x_nut', 'S:y_nut', 'S:z_nut', 'S:x_top', 'S:y_top', 'S:z_top', 'S:Nominal_rotational_speed[rad/s]',\n",
    "    'S:Actual_rotational_speed[µm/s]', 'S:Actual_position_of_the_position_encoder(dy/dt)[µm/s]',\n",
    "    'S:Actual_position_of_the_motor_encoder(dy/dt)[µm/s]']\n",
    "        \n",
    "        \n",
    "        \n",
    "    #['C:s_ist/X', 'C:s_soll/X', 'C:s_diff/X', 'C:v_(n_ist)/X', 'C:v_(n_soll)/X', 'C:P_mech./X', 'C:Pos._Diff./X',\n",
    "    #'C:I_ist/X', 'C:I_soll/X', 'C:x_bottom', 'C:y_bottom', 'C:z_bottom', 'C:x_nut', 'C:y_nut', 'C:z_nut',\n",
    "    #'C:x_top', 'C:y_top', 'C:z_top', 'D:s_ist/X', 'D:s_soll/X', 'D:s_diff/X', 'D:v_(n_ist)/X', 'D:v_(n_soll)/X',\n",
    "    #'D:P_mech./X', 'D:Pos._Diff./X', 'D:I_ist/X', 'D:I_soll/X', 'D:x_bottom', 'D:y_bottom', 'D:z_bottom',\n",
    "    #'D:x_nut', 'D:y_nut', 'D:z_nut', 'D:x_top', 'D:y_top', 'D:z_top','S:x_bottom', 'S:y_bottom', 'S:z_bottom',\n",
    "    #'S:x_nut', 'S:y_nut', 'S:z_nut', 'S:x_top', 'S:y_top', 'S:z_top', 'S:Nominal_rotational_speed[rad/s]',\n",
    "    #'S:Actual_rotational_speed[µm/s]', 'S:Actual_position_of_the_position_encoder(dy/dt)[µm/s]',\n",
    "    #'S:Actual_position_of_the_motor_encoder(dy/dt)[µm/s]']\n",
    "        number_of_files_per_BDS_state = 10\n",
    "        \n",
    "        #list_of_train_BSD_states = [\"BSD_31\", \"BSD_21\", \"BSD_11\", \"BSD_P1\"]\n",
    "        #list_of_test_BSD_states = [\"BSD_32\", \"BSD_22\", \"BSD_12\", \"BSD_P2\"]\n",
    "        \n",
    "        list_of_train_BSD_states = [\"1\", \"2\", \"3\", \"4\", \"10\", \"11\", \"12\", \"13\", \"19\", \"20\", \"21\", \"22\"]\n",
    "        list_of_test_BSD_states = [\"5\", \"6\", \"7\", \"9\", \"14\", \"15\", \"16\", \"18\", \"23\", \"24\", \"25\", \"27\"]\n",
    "        \n",
    "        data_path = os.path.join(os.path.abspath(os.path.join(os.getcwd(), os.pardir)), \"data\")\n",
    "        \n",
    "        training_folders, testing_folders = create_folder_dictionary(list_of_train_BSD_states, list_of_test_BSD_states, data_path)\n",
    "        \n",
    "        self.n_samples, self.x_data, self.y_data = concatenate_data_from_BSD_state(training_folders, data_path, features_of_interest, window_size, overlap_size)\n",
    "        \n",
    "                  \n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: Not all elements were covered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/120 folders downloaded\n",
      "downloaded folder: NR01_20200317_PGS_31_BSD_31/020_2020_03_18.csv\n",
      "Shape of collected datafram: X_shape: (22, 13, 1024), Y_shape: (22,)\n",
      "2/120 folders downloaded\n",
      "downloaded folder: NR01_20200317_PGS_31_BSD_31/021_2020_03_18.csv\n",
      "Shape of collected datafram: X_shape: (44, 13, 1024), Y_shape: (44,)\n",
      "3/120 folders downloaded\n",
      "downloaded folder: NR01_20200317_PGS_31_BSD_31/023_2020_03_18.csv\n",
      "Shape of collected datafram: X_shape: (66, 13, 1024), Y_shape: (66,)\n",
      "4/120 folders downloaded\n",
      "downloaded folder: NR01_20200317_PGS_31_BSD_31/022_2020_03_18.csv\n",
      "Shape of collected datafram: X_shape: (88, 13, 1024), Y_shape: (88,)\n",
      "5/120 folders downloaded\n",
      "downloaded folder: NR01_20200317_PGS_31_BSD_31/019_2020_03_18.csv\n",
      "Shape of collected datafram: X_shape: (110, 13, 1024), Y_shape: (110,)\n",
      "6/120 folders downloaded\n",
      "downloaded folder: NR01_20200317_PGS_31_BSD_31/016_2020_03_18.csv\n",
      "Shape of collected datafram: X_shape: (132, 13, 1024), Y_shape: (132,)\n",
      "7/120 folders downloaded\n",
      "downloaded folder: NR01_20200317_PGS_31_BSD_31/017_2020_03_18.csv\n",
      "Shape of collected datafram: X_shape: (154, 13, 1024), Y_shape: (154,)\n",
      "8/120 folders downloaded\n",
      "downloaded folder: NR01_20200317_PGS_31_BSD_31/018_2020_03_18.csv\n",
      "Shape of collected datafram: X_shape: (176, 13, 1024), Y_shape: (176,)\n",
      "9/120 folders downloaded\n",
      "downloaded folder: NR01_20200317_PGS_31_BSD_31/015_2020_03_18.csv\n",
      "Shape of collected datafram: X_shape: (198, 13, 1024), Y_shape: (198,)\n",
      "10/120 folders downloaded\n",
      "downloaded folder: NR01_20200317_PGS_31_BSD_31/014_2020_03_18.csv\n",
      "Shape of collected datafram: X_shape: (220, 13, 1024), Y_shape: (220,)\n",
      "11/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/046_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (242, 13, 1024), Y_shape: (242,)\n",
      "12/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/037_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (264, 13, 1024), Y_shape: (264,)\n",
      "13/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/041_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (286, 13, 1024), Y_shape: (286,)\n",
      "14/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/038_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (308, 13, 1024), Y_shape: (308,)\n",
      "15/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/039_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (330, 13, 1024), Y_shape: (330,)\n",
      "16/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/040_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (352, 13, 1024), Y_shape: (352,)\n",
      "17/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/045_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (374, 13, 1024), Y_shape: (374,)\n",
      "18/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/042_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (396, 13, 1024), Y_shape: (396,)\n",
      "19/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/043_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (418, 13, 1024), Y_shape: (418,)\n",
      "20/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/044_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (440, 13, 1024), Y_shape: (440,)\n",
      "21/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/063_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (462, 13, 1024), Y_shape: (462,)\n",
      "22/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/064_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (484, 13, 1024), Y_shape: (484,)\n",
      "23/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/065_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (506, 13, 1024), Y_shape: (506,)\n",
      "24/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/062_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (528, 13, 1024), Y_shape: (528,)\n",
      "25/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/068_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (550, 13, 1024), Y_shape: (550,)\n",
      "26/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/060_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (572, 13, 1024), Y_shape: (572,)\n",
      "27/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/067_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (594, 13, 1024), Y_shape: (594,)\n",
      "28/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/066_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (616, 13, 1024), Y_shape: (616,)\n",
      "29/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/061_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (638, 13, 1024), Y_shape: (638,)\n",
      "30/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/069_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (660, 13, 1024), Y_shape: (660,)\n",
      "31/120 folders downloaded\n",
      "downloaded folder: NR04_20200424_PGS_31_BSD_P1/089_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (682, 13, 1024), Y_shape: (682,)\n",
      "32/120 folders downloaded\n",
      "downloaded folder: NR04_20200424_PGS_31_BSD_P1/081_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (704, 13, 1024), Y_shape: (704,)\n",
      "33/120 folders downloaded\n",
      "downloaded folder: NR04_20200424_PGS_31_BSD_P1/086_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (726, 13, 1024), Y_shape: (726,)\n",
      "34/120 folders downloaded\n",
      "downloaded folder: NR04_20200424_PGS_31_BSD_P1/087_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (748, 13, 1024), Y_shape: (748,)\n",
      "35/120 folders downloaded\n",
      "downloaded folder: NR04_20200424_PGS_31_BSD_P1/080_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (770, 13, 1024), Y_shape: (770,)\n",
      "36/120 folders downloaded\n",
      "downloaded folder: NR04_20200424_PGS_31_BSD_P1/088_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (792, 13, 1024), Y_shape: (792,)\n",
      "37/120 folders downloaded\n",
      "downloaded folder: NR04_20200424_PGS_31_BSD_P1/082_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (814, 13, 1024), Y_shape: (814,)\n",
      "38/120 folders downloaded\n",
      "downloaded folder: NR04_20200424_PGS_31_BSD_P1/085_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (836, 13, 1024), Y_shape: (836,)\n",
      "39/120 folders downloaded\n",
      "downloaded folder: NR04_20200424_PGS_31_BSD_P1/084_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (858, 13, 1024), Y_shape: (858,)\n",
      "40/120 folders downloaded\n",
      "downloaded folder: NR04_20200424_PGS_31_BSD_P1/083_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (880, 13, 1024), Y_shape: (880,)\n",
      "41/120 folders downloaded\n",
      "downloaded folder: NR10_20200502_PGS_21_BSD_31/177_2020_05_02.csv\n",
      "Shape of collected datafram: X_shape: (902, 13, 1024), Y_shape: (902,)\n",
      "42/120 folders downloaded\n",
      "downloaded folder: NR10_20200502_PGS_21_BSD_31/180_2020_05_02.csv\n",
      "Shape of collected datafram: X_shape: (924, 13, 1024), Y_shape: (924,)\n",
      "43/120 folders downloaded\n",
      "downloaded folder: NR10_20200502_PGS_21_BSD_31/178_2020_05_02.csv\n",
      "Shape of collected datafram: X_shape: (946, 13, 1024), Y_shape: (946,)\n",
      "44/120 folders downloaded\n",
      "downloaded folder: NR10_20200502_PGS_21_BSD_31/179_2020_05_02.csv\n",
      "Shape of collected datafram: X_shape: (968, 13, 1024), Y_shape: (968,)\n",
      "45/120 folders downloaded\n",
      "downloaded folder: NR10_20200502_PGS_21_BSD_31/181_2020_05_02.csv\n",
      "Shape of collected datafram: X_shape: (990, 13, 1024), Y_shape: (990,)\n",
      "46/120 folders downloaded\n",
      "downloaded folder: NR10_20200502_PGS_21_BSD_31/176_2020_05_02.csv\n",
      "Shape of collected datafram: X_shape: (1012, 13, 1024), Y_shape: (1012,)\n",
      "47/120 folders downloaded\n",
      "downloaded folder: NR10_20200502_PGS_21_BSD_31/174_2020_05_02.csv\n",
      "Shape of collected datafram: X_shape: (1034, 13, 1024), Y_shape: (1034,)\n",
      "48/120 folders downloaded\n",
      "downloaded folder: NR10_20200502_PGS_21_BSD_31/173_2020_05_02.csv\n",
      "Shape of collected datafram: X_shape: (1056, 13, 1024), Y_shape: (1056,)\n",
      "49/120 folders downloaded\n",
      "downloaded folder: NR10_20200502_PGS_21_BSD_31/172_2020_05_02.csv\n",
      "Shape of collected datafram: X_shape: (1078, 13, 1024), Y_shape: (1078,)\n",
      "50/120 folders downloaded\n",
      "downloaded folder: NR10_20200502_PGS_21_BSD_31/175_2020_05_02.csv\n",
      "Shape of collected datafram: X_shape: (1100, 13, 1024), Y_shape: (1100,)\n",
      "51/120 folders downloaded\n",
      "downloaded folder: NR11_20200429_PGS_21_BSD_21/149_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1122, 13, 1024), Y_shape: (1122,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/120 folders downloaded\n",
      "downloaded folder: NR11_20200429_PGS_21_BSD_21/154_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1144, 13, 1024), Y_shape: (1144,)\n",
      "53/120 folders downloaded\n",
      "downloaded folder: NR11_20200429_PGS_21_BSD_21/153_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1166, 13, 1024), Y_shape: (1166,)\n",
      "54/120 folders downloaded\n",
      "downloaded folder: NR11_20200429_PGS_21_BSD_21/152_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1188, 13, 1024), Y_shape: (1188,)\n",
      "55/120 folders downloaded\n",
      "downloaded folder: NR11_20200429_PGS_21_BSD_21/155_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1210, 13, 1024), Y_shape: (1210,)\n",
      "56/120 folders downloaded\n",
      "downloaded folder: NR11_20200429_PGS_21_BSD_21/157_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1232, 13, 1024), Y_shape: (1232,)\n",
      "57/120 folders downloaded\n",
      "downloaded folder: NR11_20200429_PGS_21_BSD_21/150_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1254, 13, 1024), Y_shape: (1254,)\n",
      "58/120 folders downloaded\n",
      "downloaded folder: NR11_20200429_PGS_21_BSD_21/158_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1276, 13, 1024), Y_shape: (1276,)\n",
      "59/120 folders downloaded\n",
      "downloaded folder: NR11_20200429_PGS_21_BSD_21/151_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1298, 13, 1024), Y_shape: (1298,)\n",
      "60/120 folders downloaded\n",
      "downloaded folder: NR11_20200429_PGS_21_BSD_21/156_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1320, 13, 1024), Y_shape: (1320,)\n",
      "61/120 folders downloaded\n",
      "downloaded folder: NR12_20200429_PGS_21_BSD_11/130_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1342, 13, 1024), Y_shape: (1342,)\n",
      "62/120 folders downloaded\n",
      "downloaded folder: NR12_20200429_PGS_21_BSD_11/131_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1364, 13, 1024), Y_shape: (1364,)\n",
      "63/120 folders downloaded\n",
      "downloaded folder: NR12_20200429_PGS_21_BSD_11/126_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1386, 13, 1024), Y_shape: (1386,)\n",
      "64/120 folders downloaded\n",
      "downloaded folder: NR12_20200429_PGS_21_BSD_11/133_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1408, 13, 1024), Y_shape: (1408,)\n",
      "65/120 folders downloaded\n",
      "downloaded folder: NR12_20200429_PGS_21_BSD_11/134_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1430, 13, 1024), Y_shape: (1430,)\n",
      "66/120 folders downloaded\n",
      "downloaded folder: NR12_20200429_PGS_21_BSD_11/129_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1452, 13, 1024), Y_shape: (1452,)\n",
      "67/120 folders downloaded\n",
      "downloaded folder: NR12_20200429_PGS_21_BSD_11/128_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1474, 13, 1024), Y_shape: (1474,)\n",
      "68/120 folders downloaded\n",
      "downloaded folder: NR12_20200429_PGS_21_BSD_11/135_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1496, 13, 1024), Y_shape: (1496,)\n",
      "69/120 folders downloaded\n",
      "downloaded folder: NR12_20200429_PGS_21_BSD_11/132_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1518, 13, 1024), Y_shape: (1518,)\n",
      "70/120 folders downloaded\n",
      "downloaded folder: NR12_20200429_PGS_21_BSD_11/127_2020_04_29.csv\n",
      "Shape of collected datafram: X_shape: (1540, 13, 1024), Y_shape: (1540,)\n",
      "71/120 folders downloaded\n",
      "downloaded folder: NR13_20200428_PGS_21_BSD_P1/106_2020_04_28.csv\n",
      "Shape of collected datafram: X_shape: (1562, 13, 1024), Y_shape: (1562,)\n",
      "72/120 folders downloaded\n",
      "downloaded folder: NR13_20200428_PGS_21_BSD_P1/109_2020_04_28.csv\n",
      "Shape of collected datafram: X_shape: (1584, 13, 1024), Y_shape: (1584,)\n",
      "73/120 folders downloaded\n",
      "downloaded folder: NR13_20200428_PGS_21_BSD_P1/108_2020_04_28.csv\n",
      "Shape of collected datafram: X_shape: (1606, 13, 1024), Y_shape: (1606,)\n",
      "74/120 folders downloaded\n",
      "downloaded folder: NR13_20200428_PGS_21_BSD_P1/107_2020_04_28.csv\n",
      "Shape of collected datafram: X_shape: (1628, 13, 1024), Y_shape: (1628,)\n",
      "75/120 folders downloaded\n",
      "downloaded folder: NR13_20200428_PGS_21_BSD_P1/112_2020_04_28.csv\n",
      "Shape of collected datafram: X_shape: (1650, 13, 1024), Y_shape: (1650,)\n",
      "76/120 folders downloaded\n",
      "downloaded folder: NR13_20200428_PGS_21_BSD_P1/105_2020_04_28.csv\n",
      "Shape of collected datafram: X_shape: (1672, 13, 1024), Y_shape: (1672,)\n",
      "77/120 folders downloaded\n",
      "downloaded folder: NR13_20200428_PGS_21_BSD_P1/110_2020_04_28.csv\n",
      "Shape of collected datafram: X_shape: (1694, 13, 1024), Y_shape: (1694,)\n",
      "78/120 folders downloaded\n",
      "downloaded folder: NR13_20200428_PGS_21_BSD_P1/103_2020_04_28.csv\n",
      "Shape of collected datafram: X_shape: (1716, 13, 1024), Y_shape: (1716,)\n",
      "79/120 folders downloaded\n",
      "downloaded folder: NR13_20200428_PGS_21_BSD_P1/111_2020_04_28.csv\n",
      "Shape of collected datafram: X_shape: (1738, 13, 1024), Y_shape: (1738,)\n",
      "80/120 folders downloaded\n",
      "downloaded folder: NR13_20200428_PGS_21_BSD_P1/104_2020_04_28.csv\n",
      "Shape of collected datafram: X_shape: (1760, 13, 1024), Y_shape: (1760,)\n",
      "81/120 folders downloaded\n",
      "downloaded folder: NR19_20200505_PGS_11_BSD_31/195_2020_05_05.csv\n",
      "Shape of collected datafram: X_shape: (1782, 13, 1024), Y_shape: (1782,)\n",
      "82/120 folders downloaded\n",
      "downloaded folder: NR19_20200505_PGS_11_BSD_31/198_2020_05_05.csv\n",
      "Shape of collected datafram: X_shape: (1804, 13, 1024), Y_shape: (1804,)\n",
      "83/120 folders downloaded\n",
      "downloaded folder: NR19_20200505_PGS_11_BSD_31/197_2020_05_05.csv\n",
      "Shape of collected datafram: X_shape: (1826, 13, 1024), Y_shape: (1826,)\n",
      "84/120 folders downloaded\n",
      "downloaded folder: NR19_20200505_PGS_11_BSD_31/196_2020_05_05.csv\n",
      "Shape of collected datafram: X_shape: (1848, 13, 1024), Y_shape: (1848,)\n",
      "85/120 folders downloaded\n",
      "downloaded folder: NR19_20200505_PGS_11_BSD_31/199_2020_05_05.csv\n",
      "Shape of collected datafram: X_shape: (1870, 13, 1024), Y_shape: (1870,)\n",
      "86/120 folders downloaded\n",
      "downloaded folder: NR19_20200505_PGS_11_BSD_31/204_2020_05_05.csv\n",
      "Shape of collected datafram: X_shape: (1892, 13, 1024), Y_shape: (1892,)\n",
      "87/120 folders downloaded\n",
      "downloaded folder: NR19_20200505_PGS_11_BSD_31/203_2020_05_05.csv\n",
      "Shape of collected datafram: X_shape: (1914, 13, 1024), Y_shape: (1914,)\n",
      "88/120 folders downloaded\n",
      "downloaded folder: NR19_20200505_PGS_11_BSD_31/202_2020_05_05.csv\n",
      "Shape of collected datafram: X_shape: (1936, 13, 1024), Y_shape: (1936,)\n",
      "89/120 folders downloaded\n",
      "downloaded folder: NR19_20200505_PGS_11_BSD_31/200_2020_05_05.csv\n",
      "Shape of collected datafram: X_shape: (1958, 13, 1024), Y_shape: (1958,)\n",
      "90/120 folders downloaded\n",
      "downloaded folder: NR19_20200505_PGS_11_BSD_31/201_2020_05_05.csv\n",
      "Shape of collected datafram: X_shape: (1980, 13, 1024), Y_shape: (1980,)\n",
      "91/120 folders downloaded\n",
      "downloaded folder: NR20_20200507_PGS_11_BSD_21/220_2020_05_07.csv\n",
      "Shape of collected datafram: X_shape: (2002, 13, 1024), Y_shape: (2002,)\n",
      "92/120 folders downloaded\n",
      "downloaded folder: NR20_20200507_PGS_11_BSD_21/227_2020_05_07.csv\n",
      "Shape of collected datafram: X_shape: (2024, 13, 1024), Y_shape: (2024,)\n",
      "93/120 folders downloaded\n",
      "downloaded folder: NR20_20200507_PGS_11_BSD_21/226_2020_05_07.csv\n",
      "Shape of collected datafram: X_shape: (2046, 13, 1024), Y_shape: (2046,)\n",
      "94/120 folders downloaded\n",
      "downloaded folder: NR20_20200507_PGS_11_BSD_21/221_2020_05_07.csv\n",
      "Shape of collected datafram: X_shape: (2068, 13, 1024), Y_shape: (2068,)\n",
      "95/120 folders downloaded\n",
      "downloaded folder: NR20_20200507_PGS_11_BSD_21/223_2020_05_07.csv\n",
      "Shape of collected datafram: X_shape: (2090, 13, 1024), Y_shape: (2090,)\n",
      "96/120 folders downloaded\n",
      "downloaded folder: NR20_20200507_PGS_11_BSD_21/224_2020_05_07.csv\n",
      "Shape of collected datafram: X_shape: (2112, 13, 1024), Y_shape: (2112,)\n",
      "97/120 folders downloaded\n",
      "downloaded folder: NR20_20200507_PGS_11_BSD_21/225_2020_05_07.csv\n",
      "Shape of collected datafram: X_shape: (2134, 13, 1024), Y_shape: (2134,)\n",
      "98/120 folders downloaded\n",
      "downloaded folder: NR20_20200507_PGS_11_BSD_21/222_2020_05_07.csv\n",
      "Shape of collected datafram: X_shape: (2156, 13, 1024), Y_shape: (2156,)\n",
      "99/120 folders downloaded\n",
      "downloaded folder: NR20_20200507_PGS_11_BSD_21/219_2020_05_07.csv\n",
      "Shape of collected datafram: X_shape: (2178, 13, 1024), Y_shape: (2178,)\n",
      "100/120 folders downloaded\n",
      "downloaded folder: NR20_20200507_PGS_11_BSD_21/218_2020_05_07.csv\n",
      "Shape of collected datafram: X_shape: (2200, 13, 1024), Y_shape: (2200,)\n",
      "101/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/241_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (2222, 13, 1024), Y_shape: (2222,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/249_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (2244, 13, 1024), Y_shape: (2244,)\n",
      "103/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/246_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (2266, 13, 1024), Y_shape: (2266,)\n",
      "104/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/247_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (2288, 13, 1024), Y_shape: (2288,)\n",
      "105/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/248_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (2310, 13, 1024), Y_shape: (2310,)\n",
      "106/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/240_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (2332, 13, 1024), Y_shape: (2332,)\n",
      "107/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/242_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (2354, 13, 1024), Y_shape: (2354,)\n",
      "108/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/245_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (2376, 13, 1024), Y_shape: (2376,)\n",
      "109/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/244_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (2398, 13, 1024), Y_shape: (2398,)\n",
      "110/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/243_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (2420, 13, 1024), Y_shape: (2420,)\n",
      "111/120 folders downloaded\n",
      "downloaded folder: NR22_20200508_PGS_11_BSD_P1/265_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (2442, 13, 1024), Y_shape: (2442,)\n",
      "112/120 folders downloaded\n",
      "downloaded folder: NR22_20200508_PGS_11_BSD_P1/270_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (2464, 13, 1024), Y_shape: (2464,)\n",
      "113/120 folders downloaded\n",
      "downloaded folder: NR22_20200508_PGS_11_BSD_P1/271_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (2486, 13, 1024), Y_shape: (2486,)\n",
      "114/120 folders downloaded\n",
      "downloaded folder: NR22_20200508_PGS_11_BSD_P1/264_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (2508, 13, 1024), Y_shape: (2508,)\n",
      "115/120 folders downloaded\n",
      "downloaded folder: NR22_20200508_PGS_11_BSD_P1/263_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (2530, 13, 1024), Y_shape: (2530,)\n",
      "116/120 folders downloaded\n",
      "downloaded folder: NR22_20200508_PGS_11_BSD_P1/269_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (2552, 13, 1024), Y_shape: (2552,)\n",
      "117/120 folders downloaded\n",
      "downloaded folder: NR22_20200508_PGS_11_BSD_P1/266_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (2574, 13, 1024), Y_shape: (2574,)\n",
      "118/120 folders downloaded\n",
      "downloaded folder: NR22_20200508_PGS_11_BSD_P1/267_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (2596, 13, 1024), Y_shape: (2596,)\n",
      "119/120 folders downloaded\n",
      "downloaded folder: NR22_20200508_PGS_11_BSD_P1/272_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (2618, 13, 1024), Y_shape: (2618,)\n",
      "120/120 folders downloaded\n",
      "downloaded folder: NR22_20200508_PGS_11_BSD_P1/268_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (2640, 13, 1024), Y_shape: (2640,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset = TimeSeriesData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size,hidden_size,num_layers):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        formula [(W−K+2P)/S]+1.\n",
    "        \"\"\"\n",
    "        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=2, stride=1)#input: 1024\n",
    "        self.conv2 = nn.Conv1d(64,32,kernel_size=1, stride = 1, padding=1)#input: [(1025-2+2*0)/1]+1 = 1023\n",
    "        self.batch1 =nn.BatchNorm1d(32)#input: [(1023-1+2*1)/1]+1 = 1025\n",
    "        self.conv3 = nn.Conv1d(32,32,kernel_size=1, stride = 1, padding=1) #input:1025\n",
    "        self.batch2 =nn.BatchNorm1d(32)#input: [(1025-2+0)/1]+1 = 1027\n",
    "        #self.LSTM = nn.LSTM(input_size=1027, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        #self.fc1 = nn.Linear(32*hidden_size, output_size)\n",
    "        self.fc1 = nn.Linear(32*1027, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.selu(self.conv1(x)) #conv1\n",
    "        x = self.conv2(x) #conv2\n",
    "        x = F.selu(self.batch1(x)) #batch1\n",
    "        x = self.conv3(x) #conv3\n",
    "        x = F.selu(self.batch2(x)) #batch2\n",
    "        #x, h = self.LSTM(x) \n",
    "        x = torch.reshape(x,(x.shape[0],x.shape[1]*x.shape[2])) #flatten\n",
    "        x = self.fc1(x) #linear1\n",
    "        output = x\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv1d(13, 64, kernel_size=(2,), stride=(1,))\n",
      "  (conv2): Conv1d(64, 32, kernel_size=(1,), stride=(1,), padding=(1,))\n",
      "  (batch1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv1d(32, 32, kernel_size=(1,), stride=(1,), padding=(1,))\n",
      "  (batch2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=32864, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 13\n",
    "output_size = 4\n",
    "hidden_size = 1000\n",
    "num_layers = 2\n",
    "\n",
    "model = CNN(input_size, output_size,hidden_size, num_layers)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train/test dimensions\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "#split dataset randomly\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "#define batch size for dataloader\n",
    "batch_size = 4\n",
    "\n",
    "#dataloader\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "\n",
    "data_loader = {}\n",
    "data_loader['train'] = train_loader\n",
    "data_loader['test'] = test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "writer = SummaryWriter('runs/Dataloader2')\n",
    "\n",
    "#define training params\n",
    "num_epochs = 100\n",
    "learning_rate = 0.008\n",
    "\n",
    "#define loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#collect loss for each batch\n",
    "loss_list = []\n",
    "\n",
    "#collect accuracy\n",
    "running_correct = 0\n",
    "\n",
    "#information for plotting\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "\n",
    "############## TENSORBOARD ########################\n",
    "examples = iter(test_loader)\n",
    "example_data, example_targets = examples.next()\n",
    "\n",
    "writer.add_graph(model, example_data.float())\n",
    "###################################################\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (window, labels) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(window.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #get current accuracy\n",
    "        output = outputs.argmax(dim=1)\n",
    "        running_correct += (output == labels).sum().item()\n",
    "        \n",
    "        #plot information during training\n",
    "        if (i+1) % 20 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "            ############## TENSORBOARD ########################\n",
    "            writer.add_scalar('training loss', loss / 20, epoch * n_total_steps + i)\n",
    "            running_accuracy = running_correct / 20 / output.size(0)\n",
    "            writer.add_scalar('accuracy', running_accuracy, epoch * n_total_steps + i)\n",
    "            running_correct = 0\n",
    "            running_loss = 0.0\n",
    "            ###################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np_loss_list = []\n",
    "loss_total_epoch = 0\n",
    "\n",
    "#iterate through collected loss during training\n",
    "for iterator, loss in enumerate(loss_list):\n",
    "    \n",
    "    #remove the computational graph of the torch tensor and transform torch to numpy tensor\n",
    "    loss_total_epoch += loss.detach().numpy()\n",
    "    \n",
    "    #sum up the batch losses from each epoch\n",
    "    if iterator%len(train_loader) == 0:\n",
    "        np_loss_list.append(loss_total_epoch)\n",
    "        loss_total_epoch =0 \n",
    "\n",
    "#plot loss\n",
    "plt.plot(np_loss_list)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 successfull\n",
      "Epoch 2/200 successfull\n",
      "Epoch 3/200 successfull\n",
      "Epoch 4/200 successfull\n",
      "Epoch 5/200 successfull\n",
      "Epoch 6/200 successfull\n",
      "Epoch 7/200 successfull\n",
      "Epoch 8/200 successfull\n",
      "Epoch 9/200 successfull\n",
      "Epoch 10/200 successfull\n",
      "Epoch 11/200 successfull\n",
      "Epoch 12/200 successfull\n",
      "Epoch 13/200 successfull\n"
     ]
    }
   ],
   "source": [
    "writer_graph = SummaryWriter('runs/Dataloader2/graph')\n",
    "writer_train = SummaryWriter('runs/Dataloader2/train')\n",
    "writer_test = SummaryWriter('runs/Dataloader2/test')\n",
    "writer = {}\n",
    "writer[\"train\"] = writer_train\n",
    "writer[\"test\"] = writer_test\n",
    "\n",
    "\n",
    "#define training params\n",
    "num_epochs = 200\n",
    "learning_rate = 0.008\n",
    "\n",
    "#define loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#collect loss for each batch\n",
    "loss_collected = 0\n",
    "loss_list = {}\n",
    "loss_list['train']=[]\n",
    "loss_list['test']=[]\n",
    "\n",
    "#collect accuracy for each batch\n",
    "correct_prediction_collected = 0\n",
    "accuracy_list={}\n",
    "accuracy_list['train']=[]\n",
    "accuracy_list['test']=[]\n",
    "\n",
    "\n",
    "#information for plotting\n",
    "n_total_steps = len(train_loader)\n",
    "len_data_loader={}\n",
    "len_data_loader['train'] = len(train_loader)\n",
    "len_data_loader['test'] = len(test_loader)\n",
    "\n",
    "\n",
    "############## TENSORBOARD ########################\n",
    "examples = iter(test_loader)\n",
    "example_data, example_targets = examples.next()\n",
    "\n",
    "writer_graph.add_graph(model, example_data.float())\n",
    "###################################################\n",
    "\n",
    "# Train and Validate the model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "\n",
    "    for phase in [\"train\", \"test\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train(True)\n",
    "        else:\n",
    "            model.train(False)\n",
    "    \n",
    "\n",
    "        for i, (window, labels) in enumerate(data_loader[phase]):\n",
    "\n",
    "            \n",
    "           ########Forward pass########\n",
    "            outputs = model(window.float())\n",
    "            \n",
    "            #collect loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_collected += loss\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                ########Backward pass########\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            #collect accuracy\n",
    "            output = outputs.argmax(dim=1)\n",
    "            correct_prediction_collected += (output == labels).sum().item()\n",
    "\n",
    "\n",
    "            #plot information during training\n",
    "            #if (i+1) % 20 == 0:\n",
    "            #    print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        ############## TENSORBOARD ########################\n",
    "        running_loss = loss_collected / len_data_loader[phase]\n",
    "        loss_list[phase].append(running_loss)\n",
    "        \n",
    "        \n",
    "        running_accuracy = correct_prediction_collected / len_data_loader[phase] / output.size(0)\n",
    "        accuracy_list[phase].append(running_accuracy)\n",
    "        \n",
    "        \n",
    "        #writer.add_scalar(f'training loss {phase}', epoch)\n",
    "        #writer.add_scalar(f'accuracy {phase}', running_accuracy, epoch)\n",
    "        \n",
    "        writer[phase].add_scalar(f'training loss', running_loss, epoch)\n",
    "        writer[phase].add_scalar(f'accuracy', running_accuracy, epoch)\n",
    "                \n",
    "        correct_prediction_collected = 0\n",
    "        loss_collected = 0.0\n",
    "        ###################################################\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} successfull\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.title('Loss')\n",
    "plt.plot( loss_list['train'], 'bo-', label = 'train', linewidth=1,markersize=0.1)\n",
    "plt.plot(loss_list['test'], 'ro-', label = 'test', linewidth=1,markersize=0.1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig2 = plt.figure\n",
    "\n",
    "plt.plot(accuracy_list['train'], 'bo-', label = 'train', linewidth=1,markersize=0.1)\n",
    "plt.plot(accuracy_list['test'], 'ro-', label = 'test', linewidth=1,markersize=0.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    classes = ['BSD_11', 'BSD_21', 'BSD_31', 'BSD_P1']\n",
    "    \n",
    "    #collect information about labels, predictions\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(4)]\n",
    "    n_class_samples = [0 for i in range(4)]\n",
    "    n_class_samples_out = [0 for i in range(4)]\n",
    "    \n",
    "    #iterate through bateches in test_loader\n",
    "    for window, labels in test_loader:\n",
    "        #make predictions for each batch\n",
    "        outputs = model(window.float())\n",
    "        #for each element in batch check if prediction is correct and collect total and correct predictions and labels\n",
    "        for i in range(batch_size):\n",
    "            if len(labels)==4:\n",
    "                label = labels[i]\n",
    "                output = torch.argmax(outputs[i])\n",
    "                if label == output:\n",
    "                    n_correct+=1\n",
    "                    n_class_correct[label]+=1\n",
    "                \n",
    "                n_samples+=1\n",
    "                n_class_samples[label]+=1\n",
    "                n_class_samples_out[output]+=1\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    #calculate total accuracy\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "    \n",
    "    #calculate class accuracy\n",
    "    for i in range(4):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
