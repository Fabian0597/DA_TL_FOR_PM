{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\ndef load_data(data, window_size):\\n    splits = len(data)//window_size\\n    data = data[:splits*window_size]\\n    data = data.reshape(-1,window_size)\\n    #print(np.shape(data))\\n    return data\\n\\ndef del_nan_element(data_with_nan):\\n    nan_val = np.isnan(data_with_nan)\\n    return data_with_nan[nan_val==False]\\n\\ndef concatenate_data_from_BSD_state(folders, data_path, features_of_interest, window_size):\\n    x_data_concatenated = None\\n    y_data_concatenated = None\\n    \\n    first = True\\n    for BSD_path in folders.keys(): #folder path\\n        for file_path in folders[BSD_path]: #file path \\n            \\n            path_BSD_file = os.path.join(data_path, BSD_path, file_path) # concatenate the folder and file path\\n            \\n            #in first iteration get a list if all features\\n            if first == True:\\n                features = get_features(path_BSD_file)\\n            \\n            #load data from files in shape [window_number, 1, window_size]\\n            data_BSD_file = np.genfromtxt(path_BSD_file, dtype = np.dtype(\\'d\\'), delimiter=\\',\\')[1:,:] #write csv in numpy\\n            data_BSD_file = data_BSD_file[:,features.index(features_of_interest)]\\n            data_BSD_file = np.nan_to_num(data_BSD_file)\\n            #data_BSD_file = del_nan_element(data_BSD_file)\\n            data_BSD_file = load_data(data_BSD_file, window_size)\\n            data_BSD_file = np.expand_dims(data_BSD_file, axis = 1)\\n            \\n            #rewrite labels as BSD_condition_1 = 0, BSD_condition_2 = 1, BSD_condition_3 = 2, BSD_condition_P1 = 3\\n            label = BSD_path[-2]\\n            if label == \"P\":\\n                label = int(3)\\n            else:\\n                label =int(int(label)-1)\\n            \\n            #concatenate the data from each file in one numpy array\\n            if  first == True:\\n                x_data_concatenated = np.copy(data_BSD_file)\\n                y_data_concatenated = np.copy(np.asarray([label]*np.shape(data_BSD_file)[0]))\\n                first = False\\n            else:\\n                x_data_concatenated = np.concatenate((x_data_concatenated, data_BSD_file), axis=0)\\n                y_data_concatenated = np.concatenate((y_data_concatenated,np.asarray([label]*np.shape(data_BSD_file)[0])), axis=0)\\n            print(np.shape(x_data_concatenated), np.shape(y_data_concatenated))\\n    \\n    #generate torch array\\n    n_samples = np.shape(x_data_concatenated)[0]\\n    x_data = torch.from_numpy(x_data_concatenated)\\n    y_data = torch.from_numpy(y_data_concatenated)\\n    \\n    return n_samples, x_data, y_data\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#1 Feature\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_data(data, window_size):\n",
    "    splits = len(data)//window_size\n",
    "    data = data[:splits*window_size]\n",
    "    data = data.reshape(-1,window_size)\n",
    "    #print(np.shape(data))\n",
    "    return data\n",
    "\n",
    "def del_nan_element(data_with_nan):\n",
    "    nan_val = np.isnan(data_with_nan)\n",
    "    return data_with_nan[nan_val==False]\n",
    "\n",
    "def concatenate_data_from_BSD_state(folders, data_path, features_of_interest, window_size):\n",
    "    x_data_concatenated = None\n",
    "    y_data_concatenated = None\n",
    "    \n",
    "    first = True\n",
    "    for BSD_path in folders.keys(): #folder path\n",
    "        for file_path in folders[BSD_path]: #file path \n",
    "            \n",
    "            path_BSD_file = os.path.join(data_path, BSD_path, file_path) # concatenate the folder and file path\n",
    "            \n",
    "            #in first iteration get a list if all features\n",
    "            if first == True:\n",
    "                features = get_features(path_BSD_file)\n",
    "            \n",
    "            #load data from files in shape [window_number, 1, window_size]\n",
    "            data_BSD_file = np.genfromtxt(path_BSD_file, dtype = np.dtype('d'), delimiter=',')[1:,:] #write csv in numpy\n",
    "            data_BSD_file = data_BSD_file[:,features.index(features_of_interest)]\n",
    "            data_BSD_file = np.nan_to_num(data_BSD_file)\n",
    "            #data_BSD_file = del_nan_element(data_BSD_file)\n",
    "            data_BSD_file = load_data(data_BSD_file, window_size)\n",
    "            data_BSD_file = np.expand_dims(data_BSD_file, axis = 1)\n",
    "            \n",
    "            #rewrite labels as BSD_condition_1 = 0, BSD_condition_2 = 1, BSD_condition_3 = 2, BSD_condition_P1 = 3\n",
    "            label = BSD_path[-2]\n",
    "            if label == \"P\":\n",
    "                label = int(3)\n",
    "            else:\n",
    "                label =int(int(label)-1)\n",
    "            \n",
    "            #concatenate the data from each file in one numpy array\n",
    "            if  first == True:\n",
    "                x_data_concatenated = np.copy(data_BSD_file)\n",
    "                y_data_concatenated = np.copy(np.asarray([label]*np.shape(data_BSD_file)[0]))\n",
    "                first = False\n",
    "            else:\n",
    "                x_data_concatenated = np.concatenate((x_data_concatenated, data_BSD_file), axis=0)\n",
    "                y_data_concatenated = np.concatenate((y_data_concatenated,np.asarray([label]*np.shape(data_BSD_file)[0])), axis=0)\n",
    "            print(np.shape(x_data_concatenated), np.shape(y_data_concatenated))\n",
    "    \n",
    "    #generate torch array\n",
    "    n_samples = np.shape(x_data_concatenated)[0]\n",
    "    x_data = torch.from_numpy(x_data_concatenated)\n",
    "    y_data = torch.from_numpy(y_data_concatenated)\n",
    "    \n",
    "    return n_samples, x_data, y_data\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TimeSeriesData_Single_Feature(Dataset):\n",
    "    def __init__(self):\n",
    "        window_size = 1024\n",
    "        feature = 'D:y_bottom'\n",
    "        \n",
    "        data_path = os.path.join(os.path.abspath(os.path.join(os.getcwd(), os.pardir)), \"data\")\n",
    "        training_folders = {}\n",
    "        testing_folders = {}\n",
    "        for element in os.listdir(data_path):\n",
    "            if \"BSD_31\" in element or \"BSD_21\"  in element or \"BSD_11\" in element  or \"BSD_P1\" in element: \n",
    "                training_folders[element]  = os.listdir(os.path.join(data_path,element))\n",
    "            elif \"csv\" in element:\n",
    "                pass\n",
    "            else:\n",
    "                testing_folders[element] = os.listdir(os.path.join(data_path,element))\n",
    "                \n",
    "\n",
    "\n",
    "        #data_BSD11_collected = np.empty((0,1, window_size))\n",
    "        #data_BSD21_collected = np.empty((0,1, window_size))\n",
    "        #data_BSD31_collected = np.empty((0,1, window_size))\n",
    "        \n",
    "        \n",
    "        data_BSD11_collected = None\n",
    "        data_BSD21_collected = None\n",
    "        data_BSD31_collected = None\n",
    "\n",
    "        for i in range(len(training_folders['NR03_20200424_PGS_31_BSD_11'])):                \n",
    "            \n",
    "            path_BSD11 = os.path.join(data_path, 'NR03_20200424_PGS_31_BSD_11', training_folders['NR03_20200424_PGS_31_BSD_11'][i])\n",
    "            path_BSD21 = os.path.join(data_path, 'NR02_20200423_PGS_31_BSD_21', training_folders['NR02_20200423_PGS_31_BSD_21'][i])\n",
    "            path_BSD31 = os.path.join(data_path, 'NR01_20200317_PGS_31_BSD_31', training_folders['NR01_20200317_PGS_31_BSD_31'][i])\n",
    "\n",
    "            if i == 0:\n",
    "                with open(path_BSD11, 'r') as file:\n",
    "                    csvreader = csv.reader(file)\n",
    "                    features = next(csvreader)\n",
    "            \n",
    "            data_BSD11 = np.genfromtxt(path_BSD11, dtype = np.dtype('d'), delimiter=',')[1:,:] #write csv in numpy\n",
    "            data_BSD21 = np.genfromtxt(path_BSD21, dtype = np.dtype('d'), delimiter=',')[1:,:] #write csv in numpy\n",
    "            data_BSD31 = np.genfromtxt(path_BSD31, dtype = np.dtype('d'), delimiter=',')[1:,:] #write csv in numpy\n",
    "            \n",
    "\n",
    "            data_BSD11_loaded = load_data(del_nan_element(data_BSD11[:,features.index(feature)]), window_size)\n",
    "            data_BSD11_loaded = np.expand_dims(data_BSD11_loaded, axis = 1)\n",
    "            if i == 0:\n",
    "                data_BSD11_collected = np.copy(data_BSD11_loaded)\n",
    "            else:\n",
    "                data_BSD11_collected = np.concatenate((data_BSD11_collected, data_BSD11_loaded), axis=0)\n",
    "                \n",
    "\n",
    "            data_BSD21_loaded = load_data(del_nan_element(data_BSD21[:,features.index(feature)]), window_size)\n",
    "            data_BSD21_loaded = np.expand_dims(data_BSD21_loaded, axis = 1)\n",
    "            if i == 0:\n",
    "                data_BSD21_collected = np.copy(data_BSD21_loaded)\n",
    "            else:\n",
    "                data_BSD21_collected = np.concatenate((data_BSD21_collected, data_BSD21_loaded), axis=0)\n",
    "            \n",
    "            \n",
    "            data_BSD31_loaded = load_data(del_nan_element(data_BSD31[:,features.index(feature)]), window_size)\n",
    "            data_BSD31_loaded = np.expand_dims(data_BSD31_loaded, axis = 1)\n",
    "            if i == 0:\n",
    "                data_BSD31_collected = np.copy(data_BSD31_loaded)\n",
    "            else:\n",
    "                data_BSD31_collected = np.concatenate((data_BSD31_collected, data_BSD31_loaded), axis=0)\n",
    "            \n",
    "            \n",
    "            print(f\"number of loaded train files: {i}/ {len(training_folders['NR03_20200424_PGS_31_BSD_11'])-1}\")\n",
    "        \n",
    "        y_BSD11 = np.asarray([0]*np.shape(data_BSD11_collected)[0])\n",
    "        y_BSD21 = np.asarray([1]*np.shape(data_BSD21_collected)[0])\n",
    "        y_BSD31 = np.asarray([2]*np.shape(data_BSD31_collected)[0])\n",
    "                  \n",
    "                  \n",
    "        data_x = np.concatenate((data_BSD11_collected, data_BSD21_collected, data_BSD31_collected), axis=0)\n",
    "        data_y = np.concatenate((y_BSD11, y_BSD21, y_BSD31), axis=0)\n",
    "        \n",
    "        self.x_data = torch.from_numpy(data_x)\n",
    "        self.y_data = torch.from_numpy(data_y)\n",
    "        self.n_samples = np.shape(data_x)[0]\n",
    "        \n",
    "                  \n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data, window_size):\n",
    "    \"\"\"\n",
    "    Split data in windows of equal size\n",
    "    \n",
    "    INPUT:\n",
    "    @data: data numpy array of shape [elements per file, features]\n",
    "    @window: number of elements per window\n",
    "    \n",
    "    OUTPUT\n",
    "    @data: data numpy array of shape [number_of_windows, elements per window, features]\n",
    "    \"\"\"\n",
    "    splits = np.shape(data)[0]//window_size # number of splits\n",
    "    data = data[:splits*window_size] #cut off end of array such that array can be split equaly\n",
    "    data = data.reshape((splits,-1,np.shape(data)[1])) #split array in windows\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def load_data(data, window_size, overlap_size):\n",
    "    \"\"\"\n",
    "    Split data in windows of equal size\n",
    "    \n",
    "    INPUT:\n",
    "    @data: data numpy array of shape [elements per file, features]\n",
    "    @window: number of elements per window\n",
    "    \n",
    "    OUTPUT\n",
    "    @data: data numpy array of shape [number_of_windows, elements per window, features]\n",
    "    \"\"\"\n",
    "    num_windows = (data.shape[0] - window_size) // (window_size - overlap_size) + 1\n",
    "    overhang = data.shape[0] - (num_windows*window_size - (num_windows-1)*overlap_size)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_nan_element(data_with_nan):\n",
    "    \"\"\"\n",
    "    Delete all elements in the data which have any nan valued feature\n",
    "    \n",
    "    INPUT:\n",
    "    @data_with_nan: data numpy array containing nan_values\n",
    "    \n",
    "    OUTPUT\n",
    "    @data_with_nan: data numpy array inlcuding just elements per window which do have no nan_vaues in any feature\n",
    "    \"\"\"\n",
    "    nan_val = np.isnan(data_with_nan) #mask for all nan_elements as 2d array [elements_per_window, features]\n",
    "    nan_val = np.any(nan_val,axis = 1) #mask for all nan_rows as 1d array [elements_per_window]\n",
    "    return data_with_nan[nan_val==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder_dictionary(list_of_train_BSD_states, list_of_test_BSD_states, data_path):\n",
    "    \"\"\"\n",
    "    Create a dictionaty for testing and training containing folder names as keys and files as values\n",
    "    \n",
    "    INPUT:\n",
    "    @list_of_train_BSD_states: list containing the training BSD states as string\n",
    "    @list_of_test_BSD_states: list containing the testing BSD states as string\n",
    "    @data_path: data directory containing folders for each BSD state\n",
    "    \n",
    "    OUTPUT\n",
    "    @training_folders: dictionary folders and keys for training\n",
    "    @testing_folders: dictionary folders and keys for testing\n",
    "    \"\"\"\n",
    "    \n",
    "    data_path = data_path\n",
    "    training_folders = {}\n",
    "    testing_folders = {}\n",
    "    \n",
    "    #Sorting the individual folders by findinding the BSD_states in the folder names\n",
    "    for data_path_element in os.listdir(data_path):\n",
    "        if any(element in data_path_element for element in list_of_train_BSD_states): \n",
    "            training_folders[data_path_element]  = os.listdir(os.path.join(data_path,data_path_element))\n",
    "        elif any(element in data_path_element for element in list_of_test_BSD_states): \n",
    "            testing_folders[data_path_element] = os.listdir(os.path.join(data_path,data_path_element))\n",
    "    return training_folders, testing_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(path):\n",
    "    \"\"\"\n",
    "    Creates a list of all feature names\n",
    "    INPUT:\n",
    "    @path: path to any BSD file since the features are the same for all files\n",
    "    \n",
    "    OUTPUT\n",
    "    @features: list of features:\n",
    "    ['C:s_ist/X', 'C:s_soll/X', 'C:s_diff/X', 'C:v_(n_ist)/X', 'C:v_(n_soll)/X', 'C:P_mech./X', 'C:Pos._Diff./X',\n",
    "    'C:I_ist/X', 'C:I_soll/X', 'C:x_bottom', 'C:y_bottom', 'C:z_bottom', 'C:x_nut', 'C:y_nut', 'C:z_nut',\n",
    "    'C:x_top', 'C:y_top', 'C:z_top', 'D:s_ist/X', 'D:s_soll/X', 'D:s_diff/X', 'D:v_(n_ist)/X', 'D:v_(n_soll)/X',\n",
    "    'D:P_mech./X', 'D:Pos._Diff./X', 'D:I_ist/X', 'D:I_soll/X', 'D:x_bottom', 'D:y_bottom', 'D:z_bottom',\n",
    "    'D:x_nut', 'D:y_nut', 'D:z_nut', 'D:x_top', 'D:y_top', 'D:z_top', 'S:x_bottom', 'S:y_bottom', 'S:z_bottom',\n",
    "    'S:x_nut', 'S:y_nut', 'S:z_nut', 'S:x_top', 'S:y_top', 'S:z_top', 'S:Nominal_rotational_speed[rad/s]',\n",
    "    'S:Actual_rotational_speed[µm/s]', 'S:Actual_position_of_the_position_encoder(dy/dt)[µm/s]',\n",
    "    'S:Actual_position_of_the_motor_encoder(dy/dt)[µm/s]']\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "        csvreader = csv.reader(file)\n",
    "        features = next(csvreader)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_data_from_BSD_state(folders, data_path, features_of_interest, window_size):\n",
    "    \"\"\"\n",
    "    Concatenates all the windowed data from each file to one big torch array\n",
    "    INPUT:\n",
    "    @folders: dictionary containing folders (as keys) and files (as values) to downloaded\n",
    "    @data_path: data directory containing folders for each BSD state\n",
    "    @features_of_interest: list of features which should be included for training\n",
    "    @window_size: number of elements per widow\n",
    "    \n",
    "    OUTPUT:\n",
    "    @n_samples: number of total elements from all included files\n",
    "    @x_data: torch array containing all the data elements \n",
    "    @y_data: torch array containing the labels for all elements\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # arrays to collect data and label\n",
    "    x_data_concatenated = None\n",
    "    y_data_concatenated = None\n",
    "    \n",
    "    \n",
    "    iterator = 0\n",
    "    first = True\n",
    "    \n",
    "    \n",
    "    for BSD_path in folders.keys(): #folder path\n",
    "        for file_path in folders[BSD_path]: #file path \n",
    "            path_BSD_file = os.path.join(data_path, BSD_path, file_path) # concatenate the data_path, folder and file path\n",
    "            #in first iteration get a list if all features\n",
    "            if first == True:\n",
    "                features = get_features(path_BSD_file)\n",
    "            \n",
    "            data_BSD_file = np.genfromtxt(path_BSD_file, dtype = np.dtype('d'), delimiter=',')[1:,:] #write csv in numpy\n",
    "            feature_index_list = np.where(np.isin(features, features_of_interest)) #get index for all features of interest\n",
    "            data_BSD_file = data_BSD_file[:,feature_index_list] #slice numpy array such that just features of interest are included\n",
    "            data_BSD_file = np.squeeze(data_BSD_file, axis = 1) # one unnecessary extra dimension was created while slicing\n",
    "            data_BSD_file = del_nan_element(data_BSD_file) #delete all elements with any nan feature\n",
    "            data_BSD_file = load_data(data_BSD_file, window_size) #window the data\n",
    "            data_BSD_file = np.swapaxes(data_BSD_file,1,2) #swap axes for CNN\n",
    "            \n",
    "            \n",
    "            #rewrite labels as BSD_condition_1 = 0, BSD_condition_2 = 1, BSD_condition_3 = 2, BSD_condition_P1 = 3\n",
    "            label = BSD_path[-2] #take the first number of the BSD state for class label\n",
    "            if label == \"P\":\n",
    "                label = int(3)\n",
    "            else:\n",
    "                label =int(int(label)-1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #concatenate the data from each file in one numpy array\n",
    "            if  first == True: #overwrite variable\n",
    "                x_data_concatenated = np.copy(data_BSD_file)\n",
    "                y_data_concatenated = np.copy(np.asarray([label]*np.shape(data_BSD_file)[0]))\n",
    "                first = False\n",
    "            else: #concatenate data numpy arrays\n",
    "                x_data_concatenated = np.concatenate((x_data_concatenated, data_BSD_file), axis=0)\n",
    "                y_data_concatenated = np.concatenate((y_data_concatenated,np.asarray([label]*np.shape(data_BSD_file)[0])), axis=0)\n",
    "            \n",
    "            \n",
    "            iterator +=1\n",
    "            print(f\"{iterator}/{len(folders.keys())*len(folders[list(folders.keys())[0]])} folders downloaded\")\n",
    "            print(f\"downloaded folder: {BSD_path}/{file_path}\")\n",
    "            print(f\"Shape of collected datafram: X_shape: {np.shape(x_data_concatenated)}, Y_shape: {np.shape(y_data_concatenated)}\")\n",
    "    \n",
    "    #generate torch array\n",
    "    n_samples = np.shape(x_data_concatenated)[0]\n",
    "    x_data = torch.from_numpy(x_data_concatenated)\n",
    "    y_data = torch.from_numpy(y_data_concatenated)\n",
    "    \n",
    "    return n_samples, x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesData(Dataset):\n",
    "    \"\"\"\n",
    "    Class for creating dataset using PyTorch data primitive Dataset. An instance of this class can be used in the \n",
    "    PyTorch data primitive Dataloader\n",
    "    \n",
    "    The following patameters can be adjusted:\n",
    "    @windwo_size: Size of window which is used as Input in CNN\n",
    "    @feature_of_interest: List of all features which should be used in the CNN\n",
    "    @list_of_train_BSD_states: List of BSD states which should be used for training. Be careful at least 4 BSD\n",
    "    states representing the 4 different classes should be included for the training\n",
    "    @list_of_test_BSD_states: List of BSD states which should be used for testing\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        window_size = 1024\n",
    "        features_of_interest = ['C:s_ist/X', 'C:s_soll/X', 'C:s_diff/X', 'C:v_(n_ist)/X', 'C:v_(n_soll)/X', 'C:P_mech./X', 'C:Pos._Diff./X',\n",
    "    'C:I_ist/X', 'C:I_soll/X', 'C:x_bottom', 'C:y_bottom', 'C:z_bottom', 'C:x_nut', 'C:y_nut', 'C:z_nut',\n",
    "    'C:x_top', 'C:y_top', 'C:z_top', 'D:s_ist/X', 'D:s_soll/X', 'D:s_diff/X', 'D:v_(n_ist)/X', 'D:v_(n_soll)/X',\n",
    "    'D:P_mech./X', 'D:Pos._Diff./X', 'D:I_ist/X', 'D:I_soll/X', 'D:x_bottom', 'D:y_bottom', 'D:z_bottom',\n",
    "    'D:x_nut', 'D:y_nut', 'D:z_nut', 'D:x_top', 'D:y_top', 'D:z_top', 'S:x_bottom', 'S:y_bottom', 'S:z_bottom',\n",
    "    'S:x_nut', 'S:y_nut', 'S:z_nut', 'S:x_top', 'S:y_top', 'S:z_top', 'S:Nominal_rotational_speed[rad/s]',\n",
    "    'S:Actual_rotational_speed[µm/s]', 'S:Actual_position_of_the_position_encoder(dy/dt)[µm/s]',\n",
    "    'S:Actual_position_of_the_motor_encoder(dy/dt)[µm/s]']\n",
    "        number_of_files_per_BDS_state = 10\n",
    "        list_of_train_BSD_states = [\"BSD_31\", \"BSD_21\", \"BSD_11\", \"BSD_P1\"]\n",
    "        list_of_test_BSD_states = [\"BSD_32\", \"BSD_22\", \"BSD_12\", \"BSD_P2\"]\n",
    "        data_path = os.path.join(os.path.abspath(os.path.join(os.getcwd(), os.pardir)), \"data\")\n",
    "        \n",
    "        training_folders, testing_folders = create_folder_dictionary(list_of_train_BSD_states, list_of_test_BSD_states, data_path)\n",
    "        \n",
    "        self.n_samples, self.x_data, self.y_data = concatenate_data_from_BSD_state(training_folders, data_path, features_of_interest, window_size)\n",
    "        \n",
    "                  \n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR03_20200424_PGS_31_BSD_11/063_2020_04_24.csv\n",
      "[[[-7.12132900e+02 -7.12132900e+02 -7.12132900e+02 ... -7.00613000e+02\n",
      "   -7.00613000e+02 -7.00613000e+02]\n",
      "  [-7.00001000e+02 -7.00001000e+02 -7.00001000e+02 ... -7.04808600e+02\n",
      "   -7.04808600e+02 -7.04808600e+02]\n",
      "  [ 4.92090000e+00  4.92090000e+00  4.92090000e+00 ...  6.13100000e-01\n",
      "    6.13100000e-01  6.13100000e-01]\n",
      "  ...\n",
      "  [ 2.18460000e+00  2.49670000e+00  2.74640000e+00 ...  4.51281000e+01\n",
      "    4.11958000e+01  3.54533000e+01]\n",
      "  [ 2.00195300e+02  2.00195300e+02  2.34374900e+02 ...  7.44140420e+03\n",
      "    7.24609170e+03  6.65038880e+03]\n",
      "  [ 3.33786000e+02  3.33786000e+02  3.89099100e+02 ...  7.16018680e+03\n",
      "    6.80541990e+03  6.16645810e+03]]\n",
      "\n",
      " [[-7.00613000e+02 -7.00613000e+02 -7.00613000e+02 ... -7.01703700e+02\n",
      "   -7.01703700e+02 -7.01703700e+02]\n",
      "  [-7.04808600e+02 -7.04808600e+02 -7.04808600e+02 ... -7.32027500e+02\n",
      "   -7.32027500e+02 -7.32027500e+02]\n",
      "  [ 6.13100000e-01  6.13100000e-01  6.13100000e-01 ... -1.11530000e+00\n",
      "   -1.11530000e+00 -1.11530000e+00]\n",
      "  ...\n",
      "  [ 2.82753000e+01  1.94119000e+01  9.86200000e+00 ... -2.62150000e+00\n",
      "   -1.21715000e+01 -1.97865000e+01]\n",
      "  [ 5.87890460e+03  4.81445180e+03  3.59863180e+03 ...  1.08398407e+04\n",
      "    8.05663840e+03  2.94433510e+03]\n",
      "  [ 5.22232060e+03  4.04548650e+03  2.66838070e+03 ...  1.13296510e+03\n",
      "   -7.15255700e+02 -2.12287900e+03]]\n",
      "\n",
      " [[-7.01703700e+02 -7.01703700e+02 -7.01703700e+02 ... -7.20079400e+02\n",
      "   -7.20079400e+02 -7.20079400e+02]\n",
      "  [-7.32027500e+02 -7.32027500e+02 -7.32027500e+02 ... -7.71813400e+02\n",
      "   -7.71813400e+02 -7.71813400e+02]\n",
      "  [-1.11530000e+00 -1.11530000e+00 -1.11530000e+00 ... -5.90000000e+00\n",
      "   -5.90000000e+00 -5.90000000e+00]\n",
      "  ...\n",
      "  [-2.30946000e+01 -2.55913000e+01 -2.45302000e+01 ... -4.53778000e+01\n",
      "   -4.26314000e+01 -2.35315000e+01]\n",
      "  [-2.95898350e+03 -7.69531030e+03 -1.02587862e+04 ...  4.83398300e+02\n",
      "    2.28027280e+03  2.38281180e+03]\n",
      "  [-3.14331050e+03 -3.65638730e+03 -3.92341610e+03 ... -5.52177430e+03\n",
      "   -6.92367550e+03 -6.01959230e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.12149260e+03 -1.12149260e+03 -1.12149260e+03 ... -1.16167970e+03\n",
      "   -1.16167970e+03 -1.16167970e+03]\n",
      "  [-1.17605740e+03 -1.17605740e+03 -1.17605740e+03 ... -1.21624540e+03\n",
      "   -1.21624540e+03 -1.21624540e+03]\n",
      "  [-7.87650000e+00 -7.87650000e+00 -7.87650000e+00 ... -7.87620000e+00\n",
      "   -7.87620000e+00 -7.87620000e+00]\n",
      "  ...\n",
      "  [ 4.61890000e+00 -1.32950000e+01  1.78515000e+01 ... -1.29205000e+01\n",
      "    2.93360000e+00  7.73980000e+00]\n",
      "  [ 1.51367150e+03 -1.50390580e+03  9.76562200e+02 ...  1.00097630e+03\n",
      "   -1.02050750e+03  9.17968500e+02]\n",
      "  [ 1.54495240e+03 -5.20706200e+02 -6.73294100e+02 ...  2.97546390e+03\n",
      "   -2.58636470e+03  1.83486940e+03]]\n",
      "\n",
      " [[-1.16167970e+03 -1.16167970e+03 -1.16167970e+03 ... -1.20186810e+03\n",
      "   -1.20186810e+03 -1.20186810e+03]\n",
      "  [-1.21624540e+03 -1.21624540e+03 -1.21624540e+03 ... -1.25641930e+03\n",
      "   -1.25641930e+03 -1.25641930e+03]\n",
      "  [-7.87620000e+00 -7.87620000e+00 -7.87620000e+00 ... -7.87580000e+00\n",
      "   -7.87580000e+00 -7.87580000e+00]\n",
      "  ...\n",
      "  [-1.68528000e+01  2.32819000e+01 -2.69021000e+01 ... -5.24310000e+00\n",
      "    8.73850000e+00 -1.17970000e+01]\n",
      "  [-6.78710700e+02  3.32031200e+02  4.88281000e+01 ... -2.05078100e+02\n",
      "    1.85546800e+02 -1.61132800e+02]\n",
      "  [-8.08715800e+02 -3.54766800e+02  1.45912170e+03 ... -9.82284500e+02\n",
      "    6.35147100e+02 -2.53677400e+02]]\n",
      "\n",
      " [[-1.20186810e+03 -1.20186810e+03 -1.20186810e+03 ... -1.24205570e+03\n",
      "   -1.24205570e+03 -1.24205570e+03]\n",
      "  [-1.25641930e+03 -1.25641930e+03 -1.25641930e+03 ... -1.28989080e+03\n",
      "   -1.28989080e+03 -1.28989080e+03]\n",
      "  [-7.87580000e+00 -7.87580000e+00 -7.87580000e+00 ... -7.87800000e+00\n",
      "   -7.87800000e+00 -7.87800000e+00]\n",
      "  ...\n",
      "  [ 1.41688000e+01 -1.64783000e+01  1.81636000e+01 ...  1.37943000e+01\n",
      "   -1.34198000e+01  1.37319000e+01]\n",
      "  [ 1.36718700e+02 -8.30078000e+01  4.39453000e+01 ...  5.37109000e+01\n",
      "   -4.88281000e+01  5.85937000e+01]\n",
      "  [-1.43051100e+02  5.13076800e+02 -8.90731800e+02 ... -8.50677500e+02\n",
      "    8.44955400e+02 -8.37326000e+02]]]\n",
      "1/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/063_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (16, 49, 1024), Y_shape: (16,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR03_20200424_PGS_31_BSD_11/064_2020_04_24.csv\n",
      "[[[-7.12132300e+02 -7.12132300e+02 -7.12132300e+02 ... -7.00613500e+02\n",
      "   -7.00613500e+02 -7.00613500e+02]\n",
      "  [-7.00001000e+02 -7.00001000e+02 -7.00001000e+02 ... -7.04808600e+02\n",
      "   -7.04808600e+02 -7.04808600e+02]\n",
      "  [ 4.92160000e+00  4.92160000e+00  4.92160000e+00 ...  6.12800000e-01\n",
      "    6.12800000e-01  6.12800000e-01]\n",
      "  ...\n",
      "  [ 2.18460000e+00  2.49670000e+00  2.68400000e+00 ...  4.51905000e+01\n",
      "    4.13206000e+01  3.58278000e+01]\n",
      "  [ 1.90429600e+02  1.90429600e+02  2.24609300e+02 ...  7.44140420e+03\n",
      "    7.29980260e+03  6.70409970e+03]\n",
      "  [ 3.31878700e+02  3.31878700e+02  3.75747700e+02 ...  7.16209410e+03\n",
      "    6.81304930e+03  6.14929200e+03]]\n",
      "\n",
      " [[-7.00613500e+02 -7.00613500e+02 -7.00613500e+02 ... -7.01703700e+02\n",
      "   -7.01703700e+02 -7.01703700e+02]\n",
      "  [-7.04808600e+02 -7.04808600e+02 -7.04808600e+02 ... -7.32027500e+02\n",
      "   -7.32027500e+02 -7.32027500e+02]\n",
      "  [ 6.12800000e-01  6.12800000e-01  6.12800000e-01 ... -1.11520000e+00\n",
      "   -1.11520000e+00 -1.11520000e+00]\n",
      "  ...\n",
      "  [ 2.79632000e+01  1.92247000e+01  9.67480000e+00 ... -2.87120000e+00\n",
      "   -1.29205000e+01 -1.97240000e+01]\n",
      "  [ 5.91308430e+03  4.87304550e+03  3.61328020e+03 ...  1.06738251e+04\n",
      "    8.35937270e+03  3.48632720e+03]\n",
      "  [ 5.22804260e+03  4.03022770e+03  2.66838070e+03 ...  1.11198430e+03\n",
      "   -7.51495400e+02 -2.20680240e+03]]\n",
      "\n",
      " [[-7.01703700e+02 -7.01703700e+02 -7.01703700e+02 ... -7.20079700e+02\n",
      "   -7.20079700e+02 -7.20079700e+02]\n",
      "  [-7.32027500e+02 -7.32027500e+02 -7.32027500e+02 ... -7.71813400e+02\n",
      "   -7.71813400e+02 -7.71813400e+02]\n",
      "  [-1.11520000e+00 -1.11520000e+00 -1.11520000e+00 ... -5.90030000e+00\n",
      "   -5.90030000e+00 -5.90030000e+00]\n",
      "  ...\n",
      "  [-2.32194000e+01 -2.54665000e+01 -2.42805000e+01 ... -4.52529000e+01\n",
      "   -4.30059000e+01 -2.41557000e+01]\n",
      "  [-2.54394460e+03 -7.61230260e+03 -1.02246065e+04 ...  4.54101400e+02\n",
      "    2.37792900e+03  2.50976490e+03]\n",
      "  [-3.14331050e+03 -3.62586980e+03 -3.86428830e+03 ... -5.49316410e+03\n",
      "   -6.92939760e+03 -6.08062740e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.12149240e+03 -1.12149240e+03 -1.12149240e+03 ... -1.16167990e+03\n",
      "   -1.16167990e+03 -1.16167990e+03]\n",
      "  [-1.17605740e+03 -1.17605740e+03 -1.17605740e+03 ... -1.21624540e+03\n",
      "   -1.21624540e+03 -1.21624540e+03]\n",
      "  [-7.87610000e+00 -7.87610000e+00 -7.87610000e+00 ... -7.87690000e+00\n",
      "   -7.87690000e+00 -7.87690000e+00]\n",
      "  ...\n",
      "  [ 5.43040000e+00 -1.46682000e+01  1.92871000e+01 ... -1.26708000e+01\n",
      "    2.30950000e+00  8.36400000e+00]\n",
      "  [ 1.54296830e+03 -1.49414020e+03  9.42382500e+02 ...  9.61913800e+02\n",
      "   -9.81445000e+02  8.74023200e+02]\n",
      "  [ 1.67846680e+03 -5.49316400e+02 -7.59124800e+02 ...  2.99263000e+03\n",
      "   -2.58064270e+03  1.80053710e+03]]\n",
      "\n",
      " [[-1.16167990e+03 -1.16167990e+03 -1.16167990e+03 ... -1.20186870e+03\n",
      "   -1.20186870e+03 -1.20186870e+03]\n",
      "  [-1.21624540e+03 -1.21624540e+03 -1.21624540e+03 ... -1.25641930e+03\n",
      "   -1.25641930e+03 -1.25641930e+03]\n",
      "  [-7.87690000e+00 -7.87690000e+00 -7.87690000e+00 ... -7.87660000e+00\n",
      "   -7.87660000e+00 -7.87660000e+00]\n",
      "  ...\n",
      "  [-1.74770000e+01  2.38436000e+01 -2.69021000e+01 ... -5.49280000e+00\n",
      "    9.05060000e+00 -1.21715000e+01]\n",
      "  [-6.34765400e+02  2.92968700e+02  7.81250000e+01 ... -2.09960900e+02\n",
      "    1.61132800e+02 -1.36718700e+02]\n",
      "  [-7.45773300e+02 -4.15802000e+02  1.51634220e+03 ... -9.47952300e+02\n",
      "    5.98907500e+02 -2.15530400e+02]]\n",
      "\n",
      " [[-1.20186870e+03 -1.20186870e+03 -1.20186870e+03 ... -1.24205490e+03\n",
      "   -1.24205490e+03 -1.24205490e+03]\n",
      "  [-1.25641930e+03 -1.25641930e+03 -1.25641930e+03 ... -1.28989080e+03\n",
      "   -1.28989080e+03 -1.28989080e+03]\n",
      "  [-7.87660000e+00 -7.87660000e+00 -7.87660000e+00 ... -7.87760000e+00\n",
      "   -7.87760000e+00 -7.87760000e+00]\n",
      "  ...\n",
      "  [ 1.44809000e+01 -1.67904000e+01  1.79763000e+01 ...  1.34823000e+01\n",
      "   -1.32950000e+01  1.36071000e+01]\n",
      "  [ 1.07421800e+02 -6.83594000e+01  3.41797000e+01 ...  4.88281000e+01\n",
      "   -4.39453000e+01  5.85937000e+01]\n",
      "  [-1.71661400e+02  5.43594400e+02 -9.09805300e+02 ... -8.56399500e+02\n",
      "    8.41140700e+02 -8.33511400e+02]]]\n",
      "2/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/064_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (32, 49, 1024), Y_shape: (32,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR03_20200424_PGS_31_BSD_11/065_2020_04_24.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-7.12132700e+02 -7.12132700e+02 -7.12132700e+02 ... -7.00613100e+02\n",
      "   -7.00613100e+02 -7.00613100e+02]\n",
      "  [-7.00001000e+02 -7.00001000e+02 -7.00001000e+02 ... -7.04808600e+02\n",
      "   -7.04808600e+02 -7.04808600e+02]\n",
      "  [ 4.92100000e+00  4.92100000e+00  4.92100000e+00 ...  6.12900000e-01\n",
      "    6.12900000e-01  6.12900000e-01]\n",
      "  ...\n",
      "  [ 1.81010000e+00  2.24700000e+00  2.55910000e+00 ...  4.50657000e+01\n",
      "    4.11958000e+01  3.57030000e+01]\n",
      "  [ 1.70898400e+02  1.70898400e+02  2.05078100e+02 ...  7.39745890e+03\n",
      "    7.24120890e+03  6.66992000e+03]\n",
      "  [ 2.97546400e+02  2.97546400e+02  3.50952100e+02 ...  7.14111330e+03\n",
      "    6.78253170e+03  6.14547730e+03]]\n",
      "\n",
      " [[-7.00613100e+02 -7.00613100e+02 -7.00613100e+02 ... -7.01703900e+02\n",
      "   -7.01703900e+02 -7.01703900e+02]\n",
      "  [-7.04808600e+02 -7.04808600e+02 -7.04808600e+02 ... -7.32027500e+02\n",
      "   -7.32027500e+02 -7.32027500e+02]\n",
      "  [ 6.12900000e-01  6.12900000e-01  6.12900000e-01 ... -1.11500000e+00\n",
      "   -1.11500000e+00 -1.11500000e+00]\n",
      "  ...\n",
      "  [ 2.81504000e+01  1.94119000e+01  9.73720000e+00 ... -2.87120000e+00\n",
      "   -1.26708000e+01 -1.95368000e+01]\n",
      "  [ 5.88378740e+03  4.87304550e+03  3.60351460e+03 ...  1.09082001e+04\n",
      "    8.40331800e+03  3.38378810e+03]\n",
      "  [ 5.22232060e+03  4.02641300e+03  2.65693660e+03 ...  1.09672550e+03\n",
      "   -7.62939500e+02 -2.15911870e+03]]\n",
      "\n",
      " [[-7.01703900e+02 -7.01703900e+02 -7.01703900e+02 ... -7.20079400e+02\n",
      "   -7.20079400e+02 -7.20079400e+02]\n",
      "  [-7.32027500e+02 -7.32027500e+02 -7.32027500e+02 ... -7.71813400e+02\n",
      "   -7.71813400e+02 -7.71813400e+02]\n",
      "  [-1.11500000e+00 -1.11500000e+00 -1.11500000e+00 ... -5.90020000e+00\n",
      "   -5.90020000e+00 -5.90020000e+00]\n",
      "  ...\n",
      "  [-2.30946000e+01 -2.53416000e+01 -2.44678000e+01 ... -4.55650000e+01\n",
      "   -4.28811000e+01 -2.42181000e+01]\n",
      "  [-2.70996020e+03 -7.78808380e+03 -1.03320284e+04 ...  4.39453000e+02\n",
      "    2.38281180e+03  2.52441340e+03]\n",
      "  [-3.12042240e+03 -3.59725950e+03 -3.84712220e+03 ... -5.50079350e+03\n",
      "   -6.93321230e+03 -6.07109070e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.12149300e+03 -1.12149300e+03 -1.12149300e+03 ... -1.16167990e+03\n",
      "   -1.16167990e+03 -1.16167990e+03]\n",
      "  [-1.17605740e+03 -1.17605740e+03 -1.17605740e+03 ... -1.21624540e+03\n",
      "   -1.21624540e+03 -1.21624540e+03]\n",
      "  [-7.87620000e+00 -7.87620000e+00 -7.87620000e+00 ... -7.87640000e+00\n",
      "   -7.87640000e+00 -7.87640000e+00]\n",
      "  ...\n",
      "  [ 4.61890000e+00 -1.36695000e+01  1.79763000e+01 ... -1.29205000e+01\n",
      "    2.43430000e+00  8.05190000e+00]\n",
      "  [ 1.48437460e+03 -1.50390580e+03  9.86327800e+02 ...  9.86327800e+02\n",
      "   -1.02050750e+03  9.17968500e+02]\n",
      "  [ 1.58882140e+03 -5.41687000e+02 -6.73294100e+02 ...  2.98309330e+03\n",
      "   -2.58827210e+03  1.81579590e+03]]\n",
      "\n",
      " [[-1.16167990e+03 -1.16167990e+03 -1.16167990e+03 ... -1.20186840e+03\n",
      "   -1.20186840e+03 -1.20186840e+03]\n",
      "  [-1.21624540e+03 -1.21624540e+03 -1.21624540e+03 ... -1.25641930e+03\n",
      "   -1.25641930e+03 -1.25641930e+03]\n",
      "  [-7.87640000e+00 -7.87640000e+00 -7.87640000e+00 ... -7.87600000e+00\n",
      "   -7.87600000e+00 -7.87600000e+00]\n",
      "  ...\n",
      "  [-1.71025000e+01  2.35939000e+01 -2.70269000e+01 ... -5.49280000e+00\n",
      "    9.05060000e+00 -1.22963000e+01]\n",
      "  [-6.64062300e+02  3.02734300e+02  5.85937000e+01 ... -1.95312400e+02\n",
      "    1.85546800e+02 -1.51367100e+02]\n",
      "  [-7.64846800e+02 -3.91006500e+02  1.49345400e+03 ... -9.47952300e+02\n",
      "    6.06536900e+02 -2.19345100e+02]]\n",
      "\n",
      " [[-1.20186840e+03 -1.20186840e+03 -1.20186840e+03 ... -1.24205570e+03\n",
      "   -1.24205570e+03 -1.24205570e+03]\n",
      "  [-1.25641930e+03 -1.25641930e+03 -1.25641930e+03 ... -1.28989080e+03\n",
      "   -1.28989080e+03 -1.28989080e+03]\n",
      "  [-7.87600000e+00 -7.87600000e+00 -7.87600000e+00 ... -7.87790000e+00\n",
      "   -7.87790000e+00 -7.87790000e+00]\n",
      "  ...\n",
      "  [ 1.44185000e+01 -1.69777000e+01  1.81012000e+01 ...  1.36071000e+01\n",
      "   -1.34198000e+01  1.35447000e+01]\n",
      "  [ 1.12304700e+02 -8.30078000e+01  2.92969000e+01 ...  5.85937000e+01\n",
      "   -4.39453000e+01  3.90625000e+01]\n",
      "  [-1.77383400e+02  5.43594400e+02 -9.21249400e+02 ... -8.64028900e+02\n",
      "    8.52584800e+02 -8.54492200e+02]]]\n",
      "3/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/065_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (48, 49, 1024), Y_shape: (48,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR03_20200424_PGS_31_BSD_11/062_2020_04_24.csv\n",
      "[[[-7.12132300e+02 -7.12132300e+02 -7.12132300e+02 ... -7.00612800e+02\n",
      "   -7.00612800e+02 -7.00612800e+02]\n",
      "  [-7.00001000e+02 -7.00001000e+02 -7.00001000e+02 ... -7.04808600e+02\n",
      "   -7.04808600e+02 -7.04808600e+02]\n",
      "  [ 4.92140000e+00  4.92140000e+00  4.92140000e+00 ...  6.13100000e-01\n",
      "    6.13100000e-01  6.13100000e-01]\n",
      "  ...\n",
      "  [ 2.12220000e+00  2.49670000e+00  2.74640000e+00 ...  4.52529000e+01\n",
      "    4.13830000e+01  3.58278000e+01]\n",
      "  [ 1.95312400e+02  1.95312400e+02  2.49023400e+02 ...  7.47558380e+03\n",
      "    7.27050580e+03  6.68945130e+03]\n",
      "  [ 3.39508100e+02  3.39508100e+02  3.91006500e+02 ...  7.18307500e+03\n",
      "    6.84165950e+03  6.18553160e+03]]\n",
      "\n",
      " [[-7.00612800e+02 -7.00612800e+02 -7.00612800e+02 ... -7.01703900e+02\n",
      "   -7.01703900e+02 -7.01703900e+02]\n",
      "  [-7.04808600e+02 -7.04808600e+02 -7.04808600e+02 ... -7.32027500e+02\n",
      "   -7.32027500e+02 -7.32027500e+02]\n",
      "  [ 6.13100000e-01  6.13100000e-01  6.13100000e-01 ... -1.11530000e+00\n",
      "   -1.11530000e+00 -1.11530000e+00]\n",
      "  ...\n",
      "  [ 2.83377000e+01  1.94119000e+01  9.86200000e+00 ... -2.55910000e+00\n",
      "   -1.21091000e+01 -1.97240000e+01]\n",
      "  [ 5.89355300e+03  4.86816270e+03  3.60839740e+03 ...  1.08105439e+04\n",
      "    8.02734150e+03  2.91992110e+03]\n",
      "  [ 5.25093080e+03  4.05120850e+03  2.67601010e+03 ...  1.13677980e+03\n",
      "   -6.88552900e+02 -2.11334230e+03]]\n",
      "\n",
      " [[-7.01703900e+02 -7.01703900e+02 -7.01703900e+02 ... -7.20079600e+02\n",
      "   -7.20079600e+02 -7.20079600e+02]\n",
      "  [-7.32027500e+02 -7.32027500e+02 -7.32027500e+02 ... -7.71813400e+02\n",
      "   -7.71813400e+02 -7.71813400e+02]\n",
      "  [-1.11530000e+00 -1.11530000e+00 -1.11530000e+00 ... -5.90050000e+00\n",
      "   -5.90050000e+00 -5.90050000e+00]\n",
      "  ...\n",
      "  [-2.32194000e+01 -2.55289000e+01 -2.44678000e+01 ... -4.53153000e+01\n",
      "   -4.27562000e+01 -2.35939000e+01]\n",
      "  [-2.92480390e+03 -7.72460720e+03 -1.03271456e+04 ...  4.88281100e+02\n",
      "    2.28027280e+03  2.37304620e+03]\n",
      "  [-3.13377380e+03 -3.64685060e+03 -3.92532350e+03 ... -5.52749630e+03\n",
      "   -6.90841670e+03 -6.03294370e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.12149240e+03 -1.12149240e+03 -1.12149240e+03 ... -1.16167970e+03\n",
      "   -1.16167970e+03 -1.16167970e+03]\n",
      "  [-1.17605740e+03 -1.17605740e+03 -1.17605740e+03 ... -1.21624540e+03\n",
      "   -1.21624540e+03 -1.21624540e+03]\n",
      "  [-7.87580000e+00 -7.87580000e+00 -7.87580000e+00 ... -7.87630000e+00\n",
      "   -7.87630000e+00 -7.87630000e+00]\n",
      "  ...\n",
      "  [ 4.55650000e+00 -1.29829000e+01  1.74146000e+01 ... -1.29829000e+01\n",
      "    2.93360000e+00  7.67740000e+00]\n",
      "  [ 1.50390580e+03 -1.47460900e+03  9.76562200e+02 ...  9.81445000e+02\n",
      "   -1.00585910e+03  9.03320100e+02]\n",
      "  [ 1.51443480e+03 -5.09262100e+02 -6.54220600e+02 ...  2.96592710e+03\n",
      "   -2.59208680e+03  1.83296200e+03]]\n",
      "\n",
      " [[-1.16167970e+03 -1.16167970e+03 -1.16167970e+03 ... -1.20186850e+03\n",
      "   -1.20186850e+03 -1.20186850e+03]\n",
      "  [-1.21624540e+03 -1.21624540e+03 -1.21624540e+03 ... -1.25641930e+03\n",
      "   -1.25641930e+03 -1.25641930e+03]\n",
      "  [-7.87630000e+00 -7.87630000e+00 -7.87630000e+00 ... -7.87650000e+00\n",
      "   -7.87650000e+00 -7.87650000e+00]\n",
      "  ...\n",
      "  [-1.69152000e+01  2.33443000e+01 -2.67148000e+01 ... -5.30550000e+00\n",
      "    8.80090000e+00 -1.17970000e+01]\n",
      "  [-6.83593600e+02  3.32031200e+02  2.92969000e+01 ... -2.05078100e+02\n",
      "    1.90429600e+02 -1.66015600e+02]\n",
      "  [-8.18252600e+02 -3.39508100e+02  1.44958500e+03 ... -9.74655200e+02\n",
      "    6.35147100e+02 -2.53677400e+02]]\n",
      "\n",
      " [[-1.20186850e+03 -1.20186850e+03 -1.20186850e+03 ... -1.24205530e+03\n",
      "   -1.24205530e+03 -1.24205530e+03]\n",
      "  [-1.25641930e+03 -1.25641930e+03 -1.25641930e+03 ... -1.28989080e+03\n",
      "   -1.28989080e+03 -1.28989080e+03]\n",
      "  [-7.87650000e+00 -7.87650000e+00 -7.87650000e+00 ... -7.87750000e+00\n",
      "   -7.87750000e+00 -7.87750000e+00]\n",
      "  ...\n",
      "  [ 1.42937000e+01 -1.64783000e+01  1.81012000e+01 ...  1.37319000e+01\n",
      "   -1.32950000e+01  1.37319000e+01]\n",
      "  [ 1.31835900e+02 -8.78906000e+01  4.39453000e+01 ...  5.37109000e+01\n",
      "   -4.88281000e+01  6.34765000e+01]\n",
      "  [-1.43051100e+02  5.14984100e+02 -8.96453900e+02 ... -8.44955400e+02\n",
      "    8.39233400e+02 -8.39233400e+02]]]\n",
      "4/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/062_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (64, 49, 1024), Y_shape: (64,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR03_20200424_PGS_31_BSD_11/068_2020_04_24.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-7.12132900e+02 -7.12132900e+02 -7.12132900e+02 ... -7.00613000e+02\n",
      "   -7.00613000e+02 -7.00613000e+02]\n",
      "  [-7.00001000e+02 -7.00001000e+02 -7.00001000e+02 ... -7.04808600e+02\n",
      "   -7.04808600e+02 -7.04808600e+02]\n",
      "  [ 4.92110000e+00  4.92110000e+00  4.92110000e+00 ...  6.13200000e-01\n",
      "    6.13200000e-01  6.13200000e-01]\n",
      "  ...\n",
      "  [ 2.05980000e+00  2.37190000e+00  2.74640000e+00 ...  4.50033000e+01\n",
      "    4.13830000e+01  3.57654000e+01]\n",
      "  [ 1.85546800e+02  1.85546800e+02  2.24609300e+02 ...  7.42675570e+03\n",
      "    7.27538860e+03  6.67968560e+03]\n",
      "  [ 3.26156600e+02  3.26156600e+02  3.75747700e+02 ...  7.13920590e+03\n",
      "    6.78062440e+03  6.14929200e+03]]\n",
      "\n",
      " [[-7.00613000e+02 -7.00613000e+02 -7.00613000e+02 ... -7.01703700e+02\n",
      "   -7.01703700e+02 -7.01703700e+02]\n",
      "  [-7.04808600e+02 -7.04808600e+02 -7.04808600e+02 ... -7.32027500e+02\n",
      "   -7.32027500e+02 -7.32027500e+02]\n",
      "  [ 6.13200000e-01  6.13200000e-01  6.13200000e-01 ... -1.11510000e+00\n",
      "   -1.11510000e+00 -1.11510000e+00]\n",
      "  ...\n",
      "  [ 2.80256000e+01  1.95368000e+01  9.61230000e+00 ... -2.87120000e+00\n",
      "   -1.30453000e+01 -1.98489000e+01]\n",
      "  [ 5.90820150e+03  4.84863150e+03  3.59374900e+03 ...  1.06201142e+04\n",
      "    8.19823990e+03  3.35449130e+03]\n",
      "  [ 5.21659850e+03  4.03213500e+03  2.67601010e+03 ...  1.11961360e+03\n",
      "   -7.45773300e+02 -2.19917300e+03]]\n",
      "\n",
      " [[-7.01703700e+02 -7.01703700e+02 -7.01703700e+02 ... -7.20079600e+02\n",
      "   -7.20079600e+02 -7.20079600e+02]\n",
      "  [-7.32027500e+02 -7.32027500e+02 -7.32027500e+02 ... -7.71813400e+02\n",
      "   -7.71813400e+02 -7.71813400e+02]\n",
      "  [-1.11510000e+00 -1.11510000e+00 -1.11510000e+00 ... -5.90000000e+00\n",
      "   -5.90000000e+00 -5.90000000e+00]\n",
      "  ...\n",
      "  [-2.34691000e+01 -2.56537000e+01 -2.41557000e+01 ... -4.51281000e+01\n",
      "   -4.26314000e+01 -2.42805000e+01]\n",
      "  [-2.57324150e+03 -7.51464630e+03 -1.01074191e+04 ...  4.24804600e+02\n",
      "    2.19726500e+03  2.39257750e+03]\n",
      "  [-3.15475460e+03 -3.64685060e+03 -3.88526920e+03 ... -5.50460820e+03\n",
      "   -6.91604610e+03 -6.05392460e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.12149150e+03 -1.12149150e+03 -1.12149150e+03 ... -1.16167950e+03\n",
      "   -1.16167950e+03 -1.16167950e+03]\n",
      "  [-1.17605740e+03 -1.17605740e+03 -1.17605740e+03 ... -1.21624540e+03\n",
      "   -1.21624540e+03 -1.21624540e+03]\n",
      "  [-7.87620000e+00 -7.87620000e+00 -7.87620000e+00 ... -7.87680000e+00\n",
      "   -7.87680000e+00 -7.87680000e+00]\n",
      "  ...\n",
      "  [ 4.86860000e+00 -1.38568000e+01  1.81636000e+01 ... -1.28581000e+01\n",
      "    2.30950000e+00  8.30160000e+00]\n",
      "  [ 1.48437460e+03 -1.48437460e+03  9.81445000e+02 ...  9.81445000e+02\n",
      "   -1.01562470e+03  9.03320100e+02]\n",
      "  [ 1.61361690e+03 -5.56945800e+02 -6.86645500e+02 ...  3.00979610e+03\n",
      "   -2.59971620e+03  1.81388850e+03]]\n",
      "\n",
      " [[-1.16167950e+03 -1.16167950e+03 -1.16167950e+03 ... -1.20186830e+03\n",
      "   -1.20186830e+03 -1.20186830e+03]\n",
      "  [-1.21624540e+03 -1.21624540e+03 -1.21624540e+03 ... -1.25641930e+03\n",
      "   -1.25641930e+03 -1.25641930e+03]\n",
      "  [-7.87680000e+00 -7.87680000e+00 -7.87680000e+00 ... -7.87670000e+00\n",
      "   -7.87670000e+00 -7.87670000e+00]\n",
      "  ...\n",
      "  [-1.74770000e+01  2.39685000e+01 -2.69645000e+01 ... -5.30550000e+00\n",
      "    8.92580000e+00 -1.17970000e+01]\n",
      "  [-6.68945100e+02  3.17382700e+02  6.34765000e+01 ... -2.09960900e+02\n",
      "    1.95312400e+02 -1.51367100e+02]\n",
      "  [-7.64846800e+02 -4.06265300e+02  1.51062010e+03 ... -9.51767000e+02\n",
      "    6.04629500e+02 -2.28881800e+02]]\n",
      "\n",
      " [[-1.20186830e+03 -1.20186830e+03 -1.20186830e+03 ... -1.24205520e+03\n",
      "   -1.24205520e+03 -1.24205520e+03]\n",
      "  [-1.25641930e+03 -1.25641930e+03 -1.25641930e+03 ... -1.28989080e+03\n",
      "   -1.28989080e+03 -1.28989080e+03]\n",
      "  [-7.87670000e+00 -7.87670000e+00 -7.87670000e+00 ... -7.87730000e+00\n",
      "   -7.87730000e+00 -7.87730000e+00]\n",
      "  ...\n",
      "  [ 1.41688000e+01 -1.64159000e+01  1.78515000e+01 ...  1.31702000e+01\n",
      "   -1.29205000e+01  1.31702000e+01]\n",
      "  [ 1.26953100e+02 -9.76562000e+01  5.37109000e+01 ...  5.85937000e+01\n",
      "   -5.85937000e+01  4.88281000e+01]\n",
      "  [-1.58309900e+02  5.26428200e+02 -8.92639200e+02 ... -8.35418700e+02\n",
      "    8.27789300e+02 -8.18252600e+02]]]\n",
      "5/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/068_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (80, 49, 1024), Y_shape: (80,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR03_20200424_PGS_31_BSD_11/060_2020_04_24.csv\n",
      "[[[-7.12132500e+02 -7.12132500e+02 -7.12132500e+02 ... -7.00613400e+02\n",
      "   -7.00613400e+02 -7.00613400e+02]\n",
      "  [-7.00001000e+02 -7.00001000e+02 -7.00001000e+02 ... -7.04808600e+02\n",
      "   -7.04808600e+02 -7.04808600e+02]\n",
      "  [ 4.92090000e+00  4.92090000e+00  4.92090000e+00 ...  6.12900000e-01\n",
      "    6.12900000e-01  6.12900000e-01]\n",
      "  ...\n",
      "  [ 2.12220000e+00  2.43430000e+00  2.80880000e+00 ...  4.55650000e+01\n",
      "    4.16951000e+01  3.57654000e+01]\n",
      "  [ 2.14843700e+02  2.14843700e+02  2.29492100e+02 ...  7.53906040e+03\n",
      "    7.32421670e+03  6.70409970e+03]\n",
      "  [ 3.37600700e+02  3.37600700e+02  3.94821200e+02 ...  7.19070430e+03\n",
      "    6.84165950e+03  6.20269780e+03]]\n",
      "\n",
      " [[-7.00613400e+02 -7.00613400e+02 -7.00613400e+02 ... -7.01703900e+02\n",
      "   -7.01703900e+02 -7.01703900e+02]\n",
      "  [-7.04808600e+02 -7.04808600e+02 -7.04808600e+02 ... -7.32027500e+02\n",
      "   -7.32027500e+02 -7.32027500e+02]\n",
      "  [ 6.12900000e-01  6.12900000e-01  6.12900000e-01 ... -1.11550000e+00\n",
      "   -1.11550000e+00 -1.11550000e+00]\n",
      "  ...\n",
      "  [ 2.85250000e+01  1.95368000e+01  9.86200000e+00 ... -2.49670000e+00\n",
      "   -1.18594000e+01 -1.90374000e+01]\n",
      "  [ 5.94238120e+03  4.91210800e+03  3.67675680e+03 ...  1.12939422e+04\n",
      "    8.74511470e+03  3.49609280e+03]\n",
      "  [ 5.26618960e+03  4.06074520e+03  2.69508360e+03 ...  1.13868710e+03\n",
      "   -6.79016100e+02 -2.04849240e+03]]\n",
      "\n",
      " [[-7.01703900e+02 -7.01703900e+02 -7.01703900e+02 ... -7.20079300e+02\n",
      "   -7.20079300e+02 -7.20079300e+02]\n",
      "  [-7.32027500e+02 -7.32027500e+02 -7.32027500e+02 ... -7.71813400e+02\n",
      "   -7.71813400e+02 -7.71813400e+02]\n",
      "  [-1.11550000e+00 -1.11550000e+00 -1.11550000e+00 ... -5.90000000e+00\n",
      "   -5.90000000e+00 -5.90000000e+00]\n",
      "  ...\n",
      "  [-2.24080000e+01 -2.50296000e+01 -2.45302000e+01 ... -4.53778000e+01\n",
      "   -4.28811000e+01 -2.40309000e+01]\n",
      "  [-2.94433510e+03 -8.17382580e+03 -1.07421845e+04 ...  4.10156100e+02\n",
      "    2.40722590e+03  2.54394460e+03]\n",
      "  [-3.02124020e+03 -3.53050230e+03 -3.85856630e+03 ... -5.52368160e+03\n",
      "   -6.94847110e+03 -6.07681270e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.12149330e+03 -1.12149330e+03 -1.12149330e+03 ... -1.16167980e+03\n",
      "   -1.16167980e+03 -1.16167980e+03]\n",
      "  [-1.17605740e+03 -1.17605740e+03 -1.17605740e+03 ... -1.21624540e+03\n",
      "   -1.21624540e+03 -1.21624540e+03]\n",
      "  [-7.87580000e+00 -7.87580000e+00 -7.87580000e+00 ... -7.87640000e+00\n",
      "   -7.87640000e+00 -7.87640000e+00]\n",
      "  ...\n",
      "  [ 8.55120000e+00 -1.84757000e+01  2.28449000e+01 ... -1.17970000e+01\n",
      "    1.81010000e+00  8.42640000e+00]\n",
      "  [ 1.57226520e+03 -1.41113240e+03  7.32421700e+02 ...  9.32616900e+02\n",
      "   -9.52148200e+02  8.39843500e+02]\n",
      "  [ 1.70898440e+03 -3.20434600e+02 -1.16348270e+03 ...  2.87628170e+03\n",
      "   -2.47001650e+03  1.69563290e+03]]\n",
      "\n",
      " [[-1.16167980e+03 -1.16167980e+03 -1.16167980e+03 ... -1.20186830e+03\n",
      "   -1.20186830e+03 -1.20186830e+03]\n",
      "  [-1.21624540e+03 -1.21624540e+03 -1.21624540e+03 ... -1.25641930e+03\n",
      "   -1.25641930e+03 -1.25641930e+03]\n",
      "  [-7.87640000e+00 -7.87640000e+00 -7.87640000e+00 ... -7.87570000e+00\n",
      "   -7.87570000e+00 -7.87570000e+00]\n",
      "  ...\n",
      "  [-1.71649000e+01  2.34067000e+01 -2.59034000e+01 ... -5.30550000e+00\n",
      "    8.92580000e+00 -1.16721000e+01]\n",
      "  [-6.05468600e+02  2.83203000e+02  5.37109000e+01 ... -1.90429600e+02\n",
      "    1.66015600e+02 -1.41601500e+02]\n",
      "  [-6.90460200e+02 -4.44412200e+02  1.50680540e+03 ... -9.34600800e+02\n",
      "    5.97000100e+02 -2.15530400e+02]]\n",
      "\n",
      " [[-1.20186830e+03 -1.20186830e+03 -1.20186830e+03 ... -1.24205590e+03\n",
      "   -1.24205590e+03 -1.24205590e+03]\n",
      "  [-1.25641930e+03 -1.25641930e+03 -1.25641930e+03 ... -1.28989080e+03\n",
      "   -1.28989080e+03 -1.28989080e+03]\n",
      "  [-7.87570000e+00 -7.87570000e+00 -7.87570000e+00 ... -7.87760000e+00\n",
      "   -7.87760000e+00 -7.87760000e+00]\n",
      "  ...\n",
      "  [ 1.41688000e+01 -1.62286000e+01  1.77891000e+01 ...  1.35447000e+01\n",
      "   -1.31702000e+01  1.36695000e+01]\n",
      "  [ 1.17187500e+02 -6.83594000e+01  3.41797000e+01 ...  5.85937000e+01\n",
      "   -4.88281000e+01  6.34765000e+01]\n",
      "  [-1.60217300e+02  5.26428200e+02 -8.88824500e+02 ... -8.41140700e+02\n",
      "    8.35418700e+02 -8.29696700e+02]]]\n",
      "6/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/060_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (96, 49, 1024), Y_shape: (96,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR03_20200424_PGS_31_BSD_11/067_2020_04_24.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-7.12132900e+02 -7.12132900e+02 -7.12132900e+02 ... -7.00613200e+02\n",
      "   -7.00613200e+02 -7.00613200e+02]\n",
      "  [-7.00001000e+02 -7.00001000e+02 -7.00001000e+02 ... -7.04808600e+02\n",
      "   -7.04808600e+02 -7.04808600e+02]\n",
      "  [ 4.92110000e+00  4.92110000e+00  4.92110000e+00 ...  6.13300000e-01\n",
      "    6.13300000e-01  6.13300000e-01]\n",
      "  ...\n",
      "  [ 2.12220000e+00  2.43430000e+00  2.68400000e+00 ...  4.53778000e+01\n",
      "    4.11958000e+01  3.54533000e+01]\n",
      "  [ 1.85546800e+02  1.85546800e+02  2.24609300e+02 ...  7.39745890e+03\n",
      "    7.20702920e+03  6.61620910e+03]\n",
      "  [ 3.18527200e+02  3.18527200e+02  3.77655000e+02 ...  7.14492800e+03\n",
      "    6.79016110e+03  6.14166260e+03]]\n",
      "\n",
      " [[-7.00613200e+02 -7.00613200e+02 -7.00613200e+02 ... -7.01703800e+02\n",
      "   -7.01703800e+02 -7.01703800e+02]\n",
      "  [-7.04808600e+02 -7.04808600e+02 -7.04808600e+02 ... -7.32027500e+02\n",
      "   -7.32027500e+02 -7.32027500e+02]\n",
      "  [ 6.13300000e-01  6.13300000e-01  6.13300000e-01 ... -1.11520000e+00\n",
      "   -1.11520000e+00 -1.11520000e+00]\n",
      "  ...\n",
      "  [ 2.80256000e+01  1.91623000e+01  9.61230000e+00 ... -2.62150000e+00\n",
      "   -1.21715000e+01 -1.96616000e+01]\n",
      "  [ 5.84960770e+03  4.80956900e+03  3.59374900e+03 ...  1.07519501e+04\n",
      "    8.06152120e+03  3.01757730e+03]\n",
      "  [ 5.20706180e+03  4.01687620e+03  2.65312190e+03 ...  1.13105770e+03\n",
      "   -7.22885100e+02 -2.12287900e+03]]\n",
      "\n",
      " [[-7.01703800e+02 -7.01703800e+02 -7.01703800e+02 ... -7.20079400e+02\n",
      "   -7.20079400e+02 -7.20079400e+02]\n",
      "  [-7.32027500e+02 -7.32027500e+02 -7.32027500e+02 ... -7.71813400e+02\n",
      "   -7.71813400e+02 -7.71813400e+02]\n",
      "  [-1.11520000e+00 -1.11520000e+00 -1.11520000e+00 ... -5.90020000e+00\n",
      "   -5.90020000e+00 -5.90020000e+00]\n",
      "  ...\n",
      "  [-2.31570000e+01 -2.56537000e+01 -2.43430000e+01 ... -4.54402000e+01\n",
      "   -4.25065000e+01 -2.36564000e+01]\n",
      "  [-2.88574140e+03 -7.72949000e+03 -1.03027315e+04 ...  3.46679600e+02\n",
      "    2.21191340e+03  2.48046810e+03]\n",
      "  [-3.12805180e+03 -3.64303590e+03 -3.91387940e+03 ... -5.52368160e+03\n",
      "   -6.92176820e+03 -6.04629520e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.12149290e+03 -1.12149290e+03 -1.12149290e+03 ... -1.16168010e+03\n",
      "   -1.16168010e+03 -1.16168010e+03]\n",
      "  [-1.17605740e+03 -1.17605740e+03 -1.17605740e+03 ... -1.21624540e+03\n",
      "   -1.21624540e+03 -1.21624540e+03]\n",
      "  [-7.87580000e+00 -7.87580000e+00 -7.87580000e+00 ... -7.87660000e+00\n",
      "   -7.87660000e+00 -7.87660000e+00]\n",
      "  ...\n",
      "  [ 4.30680000e+00 -1.29829000e+01  1.76018000e+01 ... -1.31702000e+01\n",
      "    2.87120000e+00  7.86460000e+00]\n",
      "  [ 1.45996050e+03 -1.51855430e+03  1.01562470e+03 ...  9.86327800e+02\n",
      "   -1.02539030e+03  9.13085700e+02]\n",
      "  [ 1.56974790e+03 -5.66482500e+02 -6.29425000e+02 ...  3.01361080e+03\n",
      "   -2.61878970e+03  1.84440610e+03]]\n",
      "\n",
      " [[-1.16168010e+03 -1.16168010e+03 -1.16168010e+03 ... -1.20186810e+03\n",
      "   -1.20186810e+03 -1.20186810e+03]\n",
      "  [-1.21624540e+03 -1.21624540e+03 -1.21624540e+03 ... -1.25641930e+03\n",
      "   -1.25641930e+03 -1.25641930e+03]\n",
      "  [-7.87660000e+00 -7.87660000e+00 -7.87660000e+00 ... -7.87610000e+00\n",
      "   -7.87610000e+00 -7.87610000e+00]\n",
      "  ...\n",
      "  [-1.70401000e+01  2.37188000e+01 -2.70893000e+01 ... -5.55520000e+00\n",
      "    9.17540000e+00 -1.20466000e+01]\n",
      "  [-6.73827900e+02  3.32031200e+02  4.88281000e+01 ... -1.95312400e+02\n",
      "    1.66015600e+02 -1.46484300e+02]\n",
      "  [-8.14437900e+02 -3.71933000e+02  1.48773190e+03 ... -9.53674300e+02\n",
      "    6.00814800e+02 -2.19345100e+02]]\n",
      "\n",
      " [[-1.20186810e+03 -1.20186810e+03 -1.20186810e+03 ... -1.24205530e+03\n",
      "   -1.24205530e+03 -1.24205530e+03]\n",
      "  [-1.25641930e+03 -1.25641930e+03 -1.25641930e+03 ... -1.28989080e+03\n",
      "   -1.28989080e+03 -1.28989080e+03]\n",
      "  [-7.87610000e+00 -7.87610000e+00 -7.87610000e+00 ... -7.87770000e+00\n",
      "   -7.87770000e+00 -7.87770000e+00]\n",
      "  ...\n",
      "  [ 1.46058000e+01 -1.66656000e+01  1.83508000e+01 ...  1.37943000e+01\n",
      "   -1.33574000e+01  1.37943000e+01]\n",
      "  [ 1.22070300e+02 -6.83594000e+01  1.95312000e+01 ...  5.85937000e+01\n",
      "   -3.90625000e+01  4.88281000e+01]\n",
      "  [-1.88827500e+02  5.49316400e+02 -9.30786100e+02 ... -8.65936300e+02\n",
      "    8.60214200e+02 -8.62121600e+02]]]\n",
      "7/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/067_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (112, 49, 1024), Y_shape: (112,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR03_20200424_PGS_31_BSD_11/066_2020_04_24.csv\n",
      "[[[-7.12132800e+02 -7.12132800e+02 -7.12132800e+02 ... -7.00613100e+02\n",
      "   -7.00613100e+02 -7.00613100e+02]\n",
      "  [-7.00001000e+02 -7.00001000e+02 -7.00001000e+02 ... -7.04808600e+02\n",
      "   -7.04808600e+02 -7.04808600e+02]\n",
      "  [ 4.92130000e+00  4.92130000e+00  4.92130000e+00 ...  6.13500000e-01\n",
      "    6.13500000e-01  6.13500000e-01]\n",
      "  ...\n",
      "  [ 1.93500000e+00  2.37190000e+00  2.74640000e+00 ...  4.50033000e+01\n",
      "    4.14454000e+01  3.55782000e+01]\n",
      "  [ 1.80664000e+02  1.80664000e+02  2.24609300e+02 ...  7.40234170e+03\n",
      "    7.24120890e+03  6.66503720e+03]\n",
      "  [ 3.14712500e+02  3.14712500e+02  3.73840300e+02 ...  7.14492800e+03\n",
      "    6.79397580e+03  6.14547730e+03]]\n",
      "\n",
      " [[-7.00613100e+02 -7.00613100e+02 -7.00613100e+02 ... -7.01703800e+02\n",
      "   -7.01703800e+02 -7.01703800e+02]\n",
      "  [-7.04808600e+02 -7.04808600e+02 -7.04808600e+02 ... -7.32027500e+02\n",
      "   -7.32027500e+02 -7.32027500e+02]\n",
      "  [ 6.13500000e-01  6.13500000e-01  6.13500000e-01 ... -1.11510000e+00\n",
      "   -1.11510000e+00 -1.11510000e+00]\n",
      "  ...\n",
      "  [ 2.77759000e+01  1.90999000e+01  9.61230000e+00 ... -2.80880000e+00\n",
      "   -1.28581000e+01 -1.98489000e+01]\n",
      "  [ 5.89843590e+03  4.87792830e+03  3.60351460e+03 ...  1.07080048e+04\n",
      "    8.18359150e+03  3.21777250e+03]\n",
      "  [ 5.20133970e+03  4.01306150e+03  2.64739990e+03 ...  1.10054020e+03\n",
      "   -7.57217400e+02 -2.18772890e+03]]\n",
      "\n",
      " [[-7.01703800e+02 -7.01703800e+02 -7.01703800e+02 ... -7.20079600e+02\n",
      "   -7.20079600e+02 -7.20079600e+02]\n",
      "  [-7.32027500e+02 -7.32027500e+02 -7.32027500e+02 ... -7.71813400e+02\n",
      "   -7.71813400e+02 -7.71813400e+02]\n",
      "  [-1.11510000e+00 -1.11510000e+00 -1.11510000e+00 ... -5.89990000e+00\n",
      "   -5.89990000e+00 -5.89990000e+00]\n",
      "  ...\n",
      "  [-2.34067000e+01 -2.55289000e+01 -2.41557000e+01 ... -4.51281000e+01\n",
      "   -4.28186000e+01 -2.38436000e+01]\n",
      "  [-2.70996020e+03 -7.55859160e+03 -1.01025362e+04 ...  5.41992000e+02\n",
      "    2.26562440e+03  2.36328060e+03]\n",
      "  [-3.16047670e+03 -3.64494320e+03 -3.89289860e+03 ... -5.50460820e+03\n",
      "   -6.91413880e+03 -6.04248050e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.12149250e+03 -1.12149250e+03 -1.12149250e+03 ... -1.16168010e+03\n",
      "   -1.16168010e+03 -1.16168010e+03]\n",
      "  [-1.17605740e+03 -1.17605740e+03 -1.17605740e+03 ... -1.21624540e+03\n",
      "   -1.21624540e+03 -1.21624540e+03]\n",
      "  [-7.87620000e+00 -7.87620000e+00 -7.87620000e+00 ... -7.87690000e+00\n",
      "   -7.87690000e+00 -7.87690000e+00]\n",
      "  ...\n",
      "  [ 4.86860000e+00 -1.35447000e+01  1.80388000e+01 ... -1.31077000e+01\n",
      "    2.87120000e+00  7.92710000e+00]\n",
      "  [ 1.47460900e+03 -1.48437460e+03  9.66796600e+02 ...  9.91210700e+02\n",
      "   -1.02050750e+03  9.13085700e+02]\n",
      "  [ 1.56784060e+03 -5.32150300e+02 -6.86645500e+02 ...  2.99263000e+03\n",
      "   -2.61116030e+03  1.83105470e+03]]\n",
      "\n",
      " [[-1.16168010e+03 -1.16168010e+03 -1.16168010e+03 ... -1.20186820e+03\n",
      "   -1.20186820e+03 -1.20186820e+03]\n",
      "  [-1.21624540e+03 -1.21624540e+03 -1.21624540e+03 ... -1.25641930e+03\n",
      "   -1.25641930e+03 -1.25641930e+03]\n",
      "  [-7.87690000e+00 -7.87690000e+00 -7.87690000e+00 ... -7.87590000e+00\n",
      "   -7.87590000e+00 -7.87590000e+00]\n",
      "  ...\n",
      "  [-1.72273000e+01  2.37188000e+01 -2.68397000e+01 ... -5.55520000e+00\n",
      "    9.11300000e+00 -1.21091000e+01]\n",
      "  [-6.88476400e+02  3.12499900e+02  4.88281000e+01 ... -2.14843700e+02\n",
      "    1.85546800e+02 -1.56250000e+02]\n",
      "  [-7.97271700e+02 -3.68118300e+02  1.47247310e+03 ... -9.51767000e+02\n",
      "    6.00814800e+02 -2.21252400e+02]]\n",
      "\n",
      " [[-1.20186820e+03 -1.20186820e+03 -1.20186820e+03 ... -1.24205520e+03\n",
      "   -1.24205520e+03 -1.24205520e+03]\n",
      "  [-1.25641930e+03 -1.25641930e+03 -1.25641930e+03 ... -1.28989080e+03\n",
      "   -1.28989080e+03 -1.28989080e+03]\n",
      "  [-7.87590000e+00 -7.87590000e+00 -7.87590000e+00 ... -7.87750000e+00\n",
      "   -7.87750000e+00 -7.87750000e+00]\n",
      "  ...\n",
      "  [ 1.44185000e+01 -1.66656000e+01  1.81636000e+01 ...  1.34823000e+01\n",
      "   -1.31702000e+01  1.35447000e+01]\n",
      "  [ 1.07421800e+02 -7.81250000e+01  4.39453000e+01 ...  4.88281000e+01\n",
      "   -4.39453000e+01  6.34765000e+01]\n",
      "  [-1.71661400e+02  5.37872300e+02 -9.15527300e+02 ... -8.52584800e+02\n",
      "    8.43048100e+02 -8.41140700e+02]]]\n",
      "8/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/066_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (128, 49, 1024), Y_shape: (128,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR03_20200424_PGS_31_BSD_11/061_2020_04_24.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-7.12132600e+02 -7.12132600e+02 -7.12132600e+02 ... -7.00613400e+02\n",
      "   -7.00613400e+02 -7.00613400e+02]\n",
      "  [-7.00001000e+02 -7.00001000e+02 -7.00001000e+02 ... -7.04808600e+02\n",
      "   -7.04808600e+02 -7.04808600e+02]\n",
      "  [ 4.92100000e+00  4.92100000e+00  4.92100000e+00 ...  6.13400000e-01\n",
      "    6.13400000e-01  6.13400000e-01]\n",
      "  ...\n",
      "  [ 2.12220000e+00  2.55910000e+00  2.80880000e+00 ...  4.53778000e+01\n",
      "    4.15703000e+01  3.57654000e+01]\n",
      "  [ 1.95312400e+02  1.95312400e+02  2.44140600e+02 ...  7.50976350e+03\n",
      "    7.29003700e+03  6.69921690e+03]\n",
      "  [ 3.37600700e+02  3.37600700e+02  3.91006500e+02 ...  7.19261170e+03\n",
      "    6.83212280e+03  6.19697570e+03]]\n",
      "\n",
      " [[-7.00613400e+02 -7.00613400e+02 -7.00613400e+02 ... -7.01703800e+02\n",
      "   -7.01703800e+02 -7.01703800e+02]\n",
      "  [-7.04808600e+02 -7.04808600e+02 -7.04808600e+02 ... -7.32027500e+02\n",
      "   -7.32027500e+02 -7.32027500e+02]\n",
      "  [ 6.13400000e-01  6.13400000e-01  6.13400000e-01 ... -1.11570000e+00\n",
      "   -1.11570000e+00 -1.11570000e+00]\n",
      "  ...\n",
      "  [ 2.82753000e+01  1.95992000e+01  9.79960000e+00 ... -2.55910000e+00\n",
      "   -1.23587000e+01 -1.94119000e+01]\n",
      "  [ 5.91308430e+03  4.85839710e+03  3.59863180e+03 ...  1.09570282e+04\n",
      "    8.30077890e+03  3.18359290e+03]\n",
      "  [ 5.25856020e+03  4.05502320e+03  2.68363950e+03 ...  1.13487240e+03\n",
      "   -7.11441000e+02 -2.12860110e+03]]\n",
      "\n",
      " [[-7.01703800e+02 -7.01703800e+02 -7.01703800e+02 ... -7.20079400e+02\n",
      "   -7.20079400e+02 -7.20079400e+02]\n",
      "  [-7.32027500e+02 -7.32027500e+02 -7.32027500e+02 ... -7.71813400e+02\n",
      "   -7.71813400e+02 -7.71813400e+02]\n",
      "  [-1.11570000e+00 -1.11570000e+00 -1.11570000e+00 ... -5.90000000e+00\n",
      "   -5.90000000e+00 -5.90000000e+00]\n",
      "  ...\n",
      "  [-2.32194000e+01 -2.55913000e+01 -2.45926000e+01 ... -4.55026000e+01\n",
      "   -4.25690000e+01 -2.37188000e+01]\n",
      "  [-2.85644450e+03 -7.84667750e+03 -1.03515596e+04 ...  5.02929500e+02\n",
      "    2.32910090e+03  2.47070240e+03]\n",
      "  [-3.11470030e+03 -3.62777710e+03 -3.90052800e+03 ... -5.52558900e+03\n",
      "   -6.92749020e+03 -6.03485110e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.12149220e+03 -1.12149220e+03 -1.12149220e+03 ... -1.16167970e+03\n",
      "   -1.16167970e+03 -1.16167970e+03]\n",
      "  [-1.17605740e+03 -1.17605740e+03 -1.17605740e+03 ... -1.21624540e+03\n",
      "   -1.21624540e+03 -1.21624540e+03]\n",
      "  [-7.87600000e+00 -7.87600000e+00 -7.87600000e+00 ... -7.87690000e+00\n",
      "   -7.87690000e+00 -7.87690000e+00]\n",
      "  ...\n",
      "  [ 5.49280000e+00 -1.43561000e+01  1.89126000e+01 ... -1.27957000e+01\n",
      "    2.55910000e+00  8.11430000e+00]\n",
      "  [ 1.49902300e+03 -1.44042930e+03  9.03320100e+02 ...  9.76562200e+02\n",
      "   -1.01562470e+03  9.03320100e+02]\n",
      "  [ 1.57547000e+03 -4.74929800e+02 -7.76290900e+02 ...  2.96020510e+03\n",
      "   -2.56919860e+03  1.80625920e+03]]\n",
      "\n",
      " [[-1.16167970e+03 -1.16167970e+03 -1.16167970e+03 ... -1.20186840e+03\n",
      "   -1.20186840e+03 -1.20186840e+03]\n",
      "  [-1.21624540e+03 -1.21624540e+03 -1.21624540e+03 ... -1.25641930e+03\n",
      "   -1.25641930e+03 -1.25641930e+03]\n",
      "  [-7.87690000e+00 -7.87690000e+00 -7.87690000e+00 ... -7.87630000e+00\n",
      "   -7.87630000e+00 -7.87630000e+00]\n",
      "  ...\n",
      "  [-1.69777000e+01  2.33443000e+01 -2.68397000e+01 ... -5.36790000e+00\n",
      "    8.67610000e+00 -1.18594000e+01]\n",
      "  [-6.78710700e+02  3.32031200e+02  4.88281000e+01 ... -2.05078100e+02\n",
      "    1.90429600e+02 -1.70898400e+02]\n",
      "  [-7.72476200e+02 -3.83377100e+02  1.47819520e+03 ... -9.80377200e+02\n",
      "    6.33239700e+02 -2.47955300e+02]]\n",
      "\n",
      " [[-1.20186840e+03 -1.20186840e+03 -1.20186840e+03 ... -1.24205540e+03\n",
      "   -1.24205540e+03 -1.24205540e+03]\n",
      "  [-1.25641930e+03 -1.25641930e+03 -1.25641930e+03 ... -1.28989080e+03\n",
      "   -1.28989080e+03 -1.28989080e+03]\n",
      "  [-7.87630000e+00 -7.87630000e+00 -7.87630000e+00 ... -7.87710000e+00\n",
      "   -7.87710000e+00 -7.87710000e+00]\n",
      "  ...\n",
      "  [ 1.41688000e+01 -1.66656000e+01  1.79139000e+01 ...  1.37319000e+01\n",
      "   -1.34823000e+01  1.37319000e+01]\n",
      "  [ 1.22070300e+02 -8.78906000e+01  4.39453000e+01 ...  5.37109000e+01\n",
      "   -5.37109000e+01  5.37109000e+01]\n",
      "  [-1.46865800e+02  5.20706200e+02 -8.92639200e+02 ... -8.48770100e+02\n",
      "    8.41140700e+02 -8.37326000e+02]]]\n",
      "9/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/061_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (144, 49, 1024), Y_shape: (144,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR03_20200424_PGS_31_BSD_11/069_2020_04_24.csv\n",
      "[[[-7.12132800e+02 -7.12132800e+02 -7.12132800e+02 ... -7.00613200e+02\n",
      "   -7.00613200e+02 -7.00613200e+02]\n",
      "  [-7.00001000e+02 -7.00001000e+02 -7.00001000e+02 ... -7.04808600e+02\n",
      "   -7.04808600e+02 -7.04808600e+02]\n",
      "  [ 4.92130000e+00  4.92130000e+00  4.92130000e+00 ...  6.13100000e-01\n",
      "    6.13100000e-01  6.13100000e-01]\n",
      "  ...\n",
      "  [ 1.99740000e+00  2.30950000e+00  2.68400000e+00 ...  4.49408000e+01\n",
      "    4.12582000e+01  3.57030000e+01]\n",
      "  [ 1.80664000e+02  1.80664000e+02  2.05078100e+02 ...  7.36327920e+03\n",
      "    7.21679490e+03  6.60644350e+03]\n",
      "  [ 3.14712500e+02  3.14712500e+02  3.66210900e+02 ...  7.12966920e+03\n",
      "    6.76345830e+03  6.12640380e+03]]\n",
      "\n",
      " [[-7.00613200e+02 -7.00613200e+02 -7.00613200e+02 ... -7.01703700e+02\n",
      "   -7.01703700e+02 -7.01703700e+02]\n",
      "  [-7.04808600e+02 -7.04808600e+02 -7.04808600e+02 ... -7.32027500e+02\n",
      "   -7.32027500e+02 -7.32027500e+02]\n",
      "  [ 6.13100000e-01  6.13100000e-01  6.13100000e-01 ... -1.11520000e+00\n",
      "   -1.11520000e+00 -1.11520000e+00]\n",
      "  ...\n",
      "  [ 2.75887000e+01  1.91623000e+01  9.48750000e+00 ... -2.62150000e+00\n",
      "   -1.26708000e+01 -2.00361000e+01]\n",
      "  [ 5.87402180e+03  4.86816270e+03  3.64257710e+03 ...  1.07128876e+04\n",
      "    8.09570090e+03  3.10058510e+03]\n",
      "  [ 5.19943240e+03  3.99208070e+03  2.64930730e+03 ...  1.12724300e+03\n",
      "   -7.26699800e+02 -2.16484070e+03]]\n",
      "\n",
      " [[-7.01703700e+02 -7.01703700e+02 -7.01703700e+02 ... -7.20079800e+02\n",
      "   -7.20079800e+02 -7.20079800e+02]\n",
      "  [-7.32027500e+02 -7.32027500e+02 -7.32027500e+02 ... -7.71813400e+02\n",
      "   -7.71813400e+02 -7.71813400e+02]\n",
      "  [-1.11520000e+00 -1.11520000e+00 -1.11520000e+00 ... -5.90020000e+00\n",
      "   -5.90020000e+00 -5.90020000e+00]\n",
      "  ...\n",
      "  [-2.33443000e+01 -2.57786000e+01 -2.41557000e+01 ... -4.50033000e+01\n",
      "   -4.28186000e+01 -2.39685000e+01]\n",
      "  [-2.74902270e+03 -7.55370880e+03 -1.00683566e+04 ...  5.46874800e+02\n",
      "    2.29492120e+03  2.41210870e+03]\n",
      "  [-3.16429140e+03 -3.64494320e+03 -3.91197200e+03 ... -5.52558900e+03\n",
      "   -6.91604610e+03 -6.04629520e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.12149250e+03 -1.12149250e+03 -1.12149250e+03 ... -1.16167980e+03\n",
      "   -1.16167980e+03 -1.16167980e+03]\n",
      "  [-1.17605740e+03 -1.17605740e+03 -1.17605740e+03 ... -1.21624540e+03\n",
      "   -1.21624540e+03 -1.21624540e+03]\n",
      "  [-7.87670000e+00 -7.87670000e+00 -7.87670000e+00 ... -7.87680000e+00\n",
      "   -7.87680000e+00 -7.87680000e+00]\n",
      "  ...\n",
      "  [ 4.49410000e+00 -1.30453000e+01  1.75394000e+01 ... -1.27957000e+01\n",
      "    2.37190000e+00  8.30160000e+00]\n",
      "  [ 1.42089800e+03 -1.44042930e+03  9.76562200e+02 ...  9.66796600e+02\n",
      "   -9.96093500e+02  9.08202900e+02]\n",
      "  [ 1.56784060e+03 -5.51223800e+02 -6.23703000e+02 ...  3.01361080e+03\n",
      "   -2.60925290e+03  1.81579590e+03]]\n",
      "\n",
      " [[-1.16167980e+03 -1.16167980e+03 -1.16167980e+03 ... -1.20186830e+03\n",
      "   -1.20186830e+03 -1.20186830e+03]\n",
      "  [-1.21624540e+03 -1.21624540e+03 -1.21624540e+03 ... -1.25641930e+03\n",
      "   -1.25641930e+03 -1.25641930e+03]\n",
      "  [-7.87680000e+00 -7.87680000e+00 -7.87680000e+00 ... -7.87630000e+00\n",
      "   -7.87630000e+00 -7.87630000e+00]\n",
      "  ...\n",
      "  [-1.73522000e+01  2.40933000e+01 -2.70893000e+01 ... -5.92970000e+00\n",
      "    9.48750000e+00 -1.22963000e+01]\n",
      "  [-6.78710700e+02  3.17382700e+02  5.85937000e+01 ... -2.00195300e+02\n",
      "    1.90429600e+02 -1.51367100e+02]\n",
      "  [-7.70568800e+02 -4.00543200e+02  1.52015690e+03 ... -9.23156700e+02\n",
      "    5.62667800e+02 -1.83105500e+02]]\n",
      "\n",
      " [[-1.20186830e+03 -1.20186830e+03 -1.20186830e+03 ... -1.24205550e+03\n",
      "   -1.24205550e+03 -1.24205550e+03]\n",
      "  [-1.25641930e+03 -1.25641930e+03 -1.25641930e+03 ... -1.28989080e+03\n",
      "   -1.28989080e+03 -1.28989080e+03]\n",
      "  [-7.87630000e+00 -7.87630000e+00 -7.87630000e+00 ... -7.87820000e+00\n",
      "   -7.87820000e+00 -7.87820000e+00]\n",
      "  ...\n",
      "  [ 1.47930000e+01 -1.67280000e+01  1.82884000e+01 ...  1.32326000e+01\n",
      "   -1.29205000e+01  1.34198000e+01]\n",
      "  [ 1.17187500e+02 -8.30078000e+01  3.41797000e+01 ...  4.39453000e+01\n",
      "   -5.37109000e+01  4.39453000e+01]\n",
      "  [-2.07901000e+02  5.83648700e+02 -9.42230200e+02 ... -8.52584800e+02\n",
      "    8.43048100e+02 -8.44955400e+02]]]\n",
      "10/120 folders downloaded\n",
      "downloaded folder: NR03_20200424_PGS_31_BSD_11/069_2020_04_24.csv\n",
      "Shape of collected datafram: X_shape: (160, 49, 1024), Y_shape: (160,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR21_20200508_PGS_11_BSD_11/241_2020_05_08.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-7.1216810e+02 -7.1216810e+02 -7.1216810e+02 ... -7.0061590e+02\n",
      "   -7.0061590e+02 -7.0061590e+02]\n",
      "  [-7.0000060e+02 -7.0000060e+02 -7.0000060e+02 ... -7.0480820e+02\n",
      "   -7.0480820e+02 -7.0480820e+02]\n",
      "  [ 4.9438000e+00  4.9438000e+00  4.9438000e+00 ...  6.1420000e-01\n",
      "    6.1420000e-01  6.1420000e-01]\n",
      "  ...\n",
      "  [ 2.1846000e+00  2.5591000e+00  3.0585000e+00 ...  4.6501300e+01\n",
      "    4.2569000e+01  3.6826500e+01]\n",
      "  [ 2.3925770e+02  2.3925770e+02  2.7832020e+02 ...  7.9882790e+03\n",
      "    7.6318338e+03  6.9628887e+03]\n",
      "  [ 3.4713750e+02  3.4713750e+02  4.1008000e+02 ...  7.3261261e+03\n",
      "    6.9980621e+03  6.3610077e+03]]\n",
      "\n",
      " [[-7.0061590e+02 -7.0061590e+02 -7.0061590e+02 ... -7.0170500e+02\n",
      "   -7.0170500e+02 -7.0170500e+02]\n",
      "  [-7.0480820e+02 -7.0480820e+02 -7.0480820e+02 ... -7.3217490e+02\n",
      "   -7.3217490e+02 -7.3217490e+02]\n",
      "  [ 6.1420000e-01  6.1420000e-01  6.1420000e-01 ... -1.1146000e+00\n",
      "   -1.1146000e+00 -1.1146000e+00]\n",
      "  ...\n",
      "  [ 2.9211500e+01  2.0098500e+01  1.0174100e+01 ... -3.8075000e+00\n",
      "   -1.4980300e+01 -2.2782500e+01]\n",
      "  [ 5.9130843e+03  4.6533190e+03  3.2714835e+03 ...  8.8330053e+03\n",
      "    6.9921855e+03  3.1249991e+03]\n",
      "  [ 5.4187775e+03  4.1751862e+03  2.7656555e+03 ...  1.1024475e+03\n",
      "   -9.7274780e+02 -2.5749207e+03]]\n",
      "\n",
      " [[-7.0170500e+02 -7.0170500e+02 -7.0170500e+02 ... -7.2012960e+02\n",
      "   -7.2012960e+02 -7.2012960e+02]\n",
      "  [-7.3217490e+02 -7.3217490e+02 -7.3217490e+02 ... -7.7252030e+02\n",
      "   -7.7252030e+02 -7.7252030e+02]\n",
      "  [-1.1146000e+00 -1.1146000e+00 -1.1146000e+00 ... -5.9383000e+00\n",
      "   -5.9383000e+00 -5.9383000e+00]\n",
      "  ...\n",
      "  [-2.6527600e+01 -2.7651100e+01 -2.5528900e+01 ... -4.4379100e+01\n",
      "   -4.3630100e+01 -2.5903400e+01]\n",
      "  [-1.8798823e+03 -6.8310528e+03 -9.9365207e+03 ... -2.9296900e+01\n",
      "    6.8847640e+02  1.4208980e+03]\n",
      "  [-3.5877228e+03 -4.0855408e+03 -4.1828156e+03 ... -5.3062439e+03\n",
      "   -6.8302155e+03 -6.2160492e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.1275241e+03 -1.1275241e+03 -1.1275241e+03 ... -1.1683239e+03\n",
      "   -1.1683239e+03 -1.1683239e+03]\n",
      "  [-1.1829203e+03 -1.1829203e+03 -1.1829203e+03 ... -1.2237203e+03\n",
      "   -1.2237203e+03 -1.2237203e+03]\n",
      "  [-7.9965000e+00 -7.9965000e+00 -7.9965000e+00 ... -7.9962000e+00\n",
      "   -7.9962000e+00 -7.9962000e+00]\n",
      "  ...\n",
      "  [ 1.2920500e+01 -2.2595300e+01  2.5653700e+01 ... -1.0423800e+01\n",
      "    8.1140000e-01  9.0506000e+00]\n",
      "  [ 1.2548825e+03 -1.2890621e+03  8.7402320e+02 ...  7.7148420e+02\n",
      "   -7.5683570e+02  6.4453110e+02]\n",
      "  [ 1.5792847e+03  5.9127800e+01 -1.6975403e+03 ...  2.7275085e+03\n",
      "   -2.3078918e+03  1.5430450e+03]]\n",
      "\n",
      " [[-1.1683239e+03 -1.1683239e+03 -1.1683239e+03 ... -1.2091238e+03\n",
      "   -1.2091238e+03 -1.2091238e+03]\n",
      "  [-1.2237203e+03 -1.2237203e+03 -1.2237203e+03 ... -1.2642933e+03\n",
      "   -1.2642933e+03 -1.2642933e+03]\n",
      "  [-7.9962000e+00 -7.9962000e+00 -7.9962000e+00 ... -7.9968000e+00\n",
      "   -7.9968000e+00 -7.9968000e+00]\n",
      "  ...\n",
      "  [-1.6728000e+01  2.2720100e+01 -2.5466500e+01 ... -5.0558000e+00\n",
      "    8.3640000e+00 -1.0985500e+01]\n",
      "  [-4.3945300e+02  1.7089840e+02  1.0253900e+02 ... -1.6601560e+02\n",
      "    1.6113280e+02 -1.5136710e+02]\n",
      "  [-5.5313110e+02 -5.3405760e+02  1.5411377e+03 ... -9.0026860e+02\n",
      "    5.7983400e+02 -2.1743770e+02]]\n",
      "\n",
      " [[-1.2091238e+03 -1.2091238e+03 -1.2091238e+03 ... -1.2499133e+03\n",
      "   -1.2499133e+03 -1.2499133e+03]\n",
      "  [-1.2642933e+03 -1.2642933e+03 -1.2642933e+03 ... -1.2938568e+03\n",
      "   -1.2938568e+03 -1.2938568e+03]\n",
      "  [-7.9968000e+00 -7.9968000e+00 -7.9968000e+00 ... -7.9740000e+00\n",
      "   -7.9740000e+00 -7.9740000e+00]\n",
      "  ...\n",
      "  [ 1.3170200e+01 -1.5479600e+01  1.6603100e+01 ...  1.2670800e+01\n",
      "   -1.2421100e+01  1.2608400e+01]\n",
      "  [ 1.2207030e+02 -8.7890600e+01  4.3945300e+01 ...  7.3242200e+01\n",
      "   -7.3242200e+01  7.3242200e+01]\n",
      "  [-1.4114380e+02  4.9209590e+02 -8.5067750e+02 ... -7.7247620e+02\n",
      "    7.6484680e+02 -7.6103210e+02]]]\n",
      "11/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/241_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (176, 49, 1024), Y_shape: (176,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR21_20200508_PGS_11_BSD_11/249_2020_05_08.csv\n",
      "[[[-7.1216800e+02 -7.1216800e+02 -7.1216800e+02 ... -7.0061620e+02\n",
      "   -7.0061620e+02 -7.0061620e+02]\n",
      "  [-7.0000060e+02 -7.0000060e+02 -7.0000060e+02 ... -7.0480820e+02\n",
      "   -7.0480820e+02 -7.0480820e+02]\n",
      "  [ 4.9443000e+00  4.9443000e+00  4.9443000e+00 ...  6.1410000e-01\n",
      "    6.1410000e-01  6.1410000e-01]\n",
      "  ...\n",
      "  [ 2.2470000e+00  2.5591000e+00  2.9961000e+00 ...  4.7125500e+01\n",
      "    4.3193100e+01  3.7325900e+01]\n",
      "  [ 2.3925770e+02  2.3925770e+02  2.7832020e+02 ...  8.1884743e+03\n",
      "    7.8173806e+03  7.1582011e+03]\n",
      "  [ 3.5285950e+02  3.5285950e+02  4.1008000e+02 ...  7.3833466e+03\n",
      "    7.0743561e+03  6.4506531e+03]]\n",
      "\n",
      " [[-7.0061620e+02 -7.0061620e+02 -7.0061620e+02 ... -7.0170450e+02\n",
      "   -7.0170450e+02 -7.0170450e+02]\n",
      "  [-7.0480820e+02 -7.0480820e+02 -7.0480820e+02 ... -7.3217490e+02\n",
      "   -7.3217490e+02 -7.3217490e+02]\n",
      "  [ 6.1410000e-01  6.1410000e-01  6.1410000e-01 ... -1.1150000e+00\n",
      "   -1.1150000e+00 -1.1150000e+00]\n",
      "  ...\n",
      "  [ 2.9960600e+01  2.0722700e+01  1.0611000e+01 ... -3.6827000e+00\n",
      "   -1.4917900e+01 -2.2470400e+01]\n",
      "  [ 6.0937483e+03  4.8291002e+03  3.4374990e+03 ...  8.7646460e+03\n",
      "    6.9873027e+03  3.2617178e+03]\n",
      "  [ 5.5046082e+03  4.2972565e+03  2.8629303e+03 ...  1.1482239e+03\n",
      "   -9.3078610e+02 -2.5482178e+03]]\n",
      "\n",
      " [[-7.0170450e+02 -7.0170450e+02 -7.0170450e+02 ... -7.2012980e+02\n",
      "   -7.2012980e+02 -7.2012980e+02]\n",
      "  [-7.3217490e+02 -7.3217490e+02 -7.3217490e+02 ... -7.7252030e+02\n",
      "   -7.7252030e+02 -7.7252030e+02]\n",
      "  [-1.1150000e+00 -1.1150000e+00 -1.1150000e+00 ... -5.9386000e+00\n",
      "   -5.9386000e+00 -5.9386000e+00]\n",
      "  ...\n",
      "  [-2.6153100e+01 -2.7214200e+01 -2.5092000e+01 ... -4.4316700e+01\n",
      "   -4.3255600e+01 -2.6215500e+01]\n",
      "  [-1.6357417e+03 -6.5478497e+03 -9.8535129e+03 ...  1.4648400e+01\n",
      "    5.7617170e+02  1.2011715e+03]\n",
      "  [-3.5591125e+03 -4.0378571e+03 -4.1294098e+03 ... -5.2909851e+03\n",
      "   -6.8092346e+03 -6.1893463e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.1275238e+03 -1.1275238e+03 -1.1275238e+03 ... -1.1683230e+03\n",
      "   -1.1683230e+03 -1.1683230e+03]\n",
      "  [-1.1829203e+03 -1.1829203e+03 -1.1829203e+03 ... -1.2237203e+03\n",
      "   -1.2237203e+03 -1.2237203e+03]\n",
      "  [-7.9966000e+00 -7.9966000e+00 -7.9966000e+00 ... -7.9966000e+00\n",
      "   -7.9966000e+00 -7.9966000e+00]\n",
      "  ...\n",
      "  [ 1.0236500e+01 -2.0410600e+01  2.4530200e+01 ... -1.0860700e+01\n",
      "    9.3630000e-01  9.1754000e+00]\n",
      "  [ 1.2353512e+03 -1.3476559e+03  1.0009763e+03 ...  7.7636700e+02\n",
      "   -7.7636700e+02  6.5917950e+02]\n",
      "  [ 1.7604828e+03 -2.4414060e+02 -1.3694763e+03 ...  2.7980804e+03\n",
      "   -2.3727417e+03  1.5907288e+03]]\n",
      "\n",
      " [[-1.1683230e+03 -1.1683230e+03 -1.1683230e+03 ... -1.2091236e+03\n",
      "   -1.2091236e+03 -1.2091236e+03]\n",
      "  [-1.2237203e+03 -1.2237203e+03 -1.2237203e+03 ... -1.2642933e+03\n",
      "   -1.2642933e+03 -1.2642933e+03]\n",
      "  [-7.9966000e+00 -7.9966000e+00 -7.9966000e+00 ... -7.9969000e+00\n",
      "   -7.9969000e+00 -7.9969000e+00]\n",
      "  ...\n",
      "  [-1.7227300e+01  2.3094600e+01 -2.5778600e+01 ... -5.1807000e+00\n",
      "    8.5512000e+00 -1.1297600e+01]\n",
      "  [-4.4433580e+02  1.6601560e+02  1.2207030e+02 ... -1.6601560e+02\n",
      "    1.5136710e+02 -1.4160150e+02]\n",
      "  [-5.7601930e+02 -5.2642820e+02  1.5544891e+03 ... -8.9073180e+02\n",
      "    5.6266780e+02 -2.0980830e+02]]\n",
      "\n",
      " [[-1.2091236e+03 -1.2091236e+03 -1.2091236e+03 ... -1.2499132e+03\n",
      "   -1.2499132e+03 -1.2499132e+03]\n",
      "  [-1.2642933e+03 -1.2642933e+03 -1.2642933e+03 ... -1.2938568e+03\n",
      "   -1.2938568e+03 -1.2938568e+03]\n",
      "  [-7.9969000e+00 -7.9969000e+00 -7.9969000e+00 ... -7.9745000e+00\n",
      "   -7.9745000e+00 -7.9745000e+00]\n",
      "  ...\n",
      "  [ 1.3482300e+01 -1.5542000e+01  1.6665600e+01 ...  1.2483600e+01\n",
      "   -1.2171500e+01  1.2483600e+01]\n",
      "  [ 1.1230470e+02 -8.3007800e+01  5.8593700e+01 ...  6.8359400e+01\n",
      "   -6.8359400e+01  7.8125000e+01]\n",
      "  [-1.5258790e+02  4.9781800e+02 -8.4304810e+02 ... -7.6675420e+02\n",
      "    7.5340270e+02 -7.5149540e+02]]]\n",
      "12/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/249_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (192, 49, 1024), Y_shape: (192,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR21_20200508_PGS_11_BSD_11/246_2020_05_08.csv\n",
      "[[[-7.1216780e+02 -7.1216780e+02 -7.1216780e+02 ... -7.0061600e+02\n",
      "   -7.0061600e+02 -7.0061600e+02]\n",
      "  [-7.0000060e+02 -7.0000060e+02 -7.0000060e+02 ... -7.0480820e+02\n",
      "   -7.0480820e+02 -7.0480820e+02]\n",
      "  [ 4.9440000e+00  4.9440000e+00  4.9440000e+00 ...  6.1400000e-01\n",
      "    6.1400000e-01  6.1400000e-01]\n",
      "  ...\n",
      "  [ 2.0598000e+00  2.6215000e+00  2.9961000e+00 ...  4.6813400e+01\n",
      "    4.3005900e+01  3.7325900e+01]\n",
      "  [ 2.2460930e+02  2.2460930e+02  2.5878900e+02 ...  8.1445290e+03\n",
      "    7.7685525e+03  7.0947246e+03]\n",
      "  [ 3.4523010e+02  3.4523010e+02  4.0626530e+02 ...  7.3719025e+03\n",
      "    7.0571899e+03  6.4258575e+03]]\n",
      "\n",
      " [[-7.0061600e+02 -7.0061600e+02 -7.0061600e+02 ... -7.0170490e+02\n",
      "   -7.0170490e+02 -7.0170490e+02]\n",
      "  [-7.0480820e+02 -7.0480820e+02 -7.0480820e+02 ... -7.3217490e+02\n",
      "   -7.3217490e+02 -7.3217490e+02]\n",
      "  [ 6.1400000e-01  6.1400000e-01  6.1400000e-01 ... -1.1147000e+00\n",
      "   -1.1147000e+00 -1.1147000e+00]\n",
      "  ...\n",
      "  [ 2.9773300e+01  2.0847600e+01  1.0611000e+01 ... -3.7451000e+00\n",
      "   -1.4730600e+01 -2.2532800e+01]\n",
      "  [ 6.0400374e+03  4.7998033e+03  3.4033194e+03 ...  8.9062475e+03\n",
      "    6.9970684e+03  3.0664054e+03]\n",
      "  [ 5.4969788e+03  4.2533875e+03  2.8553009e+03 ...  1.1005402e+03\n",
      "   -9.4604490e+02 -2.5348663e+03]]\n",
      "\n",
      " [[-7.0170490e+02 -7.0170490e+02 -7.0170490e+02 ... -7.2012960e+02\n",
      "   -7.2012960e+02 -7.2012960e+02]\n",
      "  [-7.3217490e+02 -7.3217490e+02 -7.3217490e+02 ... -7.7252030e+02\n",
      "   -7.7252030e+02 -7.7252030e+02]\n",
      "  [-1.1147000e+00 -1.1147000e+00 -1.1147000e+00 ... -5.9385000e+00\n",
      "   -5.9385000e+00 -5.9385000e+00]\n",
      "  ...\n",
      "  [-2.6215500e+01 -2.7588700e+01 -2.5216800e+01 ... -4.4316700e+01\n",
      "   -4.3130700e+01 -2.6090700e+01]\n",
      "  [-1.9628901e+03 -6.7871075e+03 -9.8388644e+03 ... -3.4179700e+01\n",
      "    6.3964830e+02  1.3476559e+03]\n",
      "  [-3.5591125e+03 -4.0416718e+03 -4.1580200e+03 ... -5.3310394e+03\n",
      "   -6.8264008e+03 -6.1588287e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.1275237e+03 -1.1275237e+03 -1.1275237e+03 ... -1.1683234e+03\n",
      "   -1.1683234e+03 -1.1683234e+03]\n",
      "  [-1.1829203e+03 -1.1829203e+03 -1.1829203e+03 ... -1.2237203e+03\n",
      "   -1.2237203e+03 -1.2237203e+03]\n",
      "  [-7.9965000e+00 -7.9965000e+00 -7.9965000e+00 ... -7.9969000e+00\n",
      "   -7.9969000e+00 -7.9969000e+00]\n",
      "  ...\n",
      "  [ 9.3003000e+00 -1.9037400e+01  2.3406700e+01 ... -1.1172800e+01\n",
      "    1.3108000e+00  8.7385000e+00]\n",
      "  [ 1.1914059e+03 -1.3232418e+03  1.0107419e+03 ...  8.0566380e+02\n",
      "   -8.0566380e+02  6.7871070e+02]\n",
      "  [ 1.7204285e+03 -2.9563900e+02 -1.2454987e+03 ...  2.7980804e+03\n",
      "   -2.3918152e+03  1.6212463e+03]]\n",
      "\n",
      " [[-1.1683234e+03 -1.1683234e+03 -1.1683234e+03 ... -1.2091234e+03\n",
      "   -1.2091234e+03 -1.2091234e+03]\n",
      "  [-1.2237203e+03 -1.2237203e+03 -1.2237203e+03 ... -1.2642933e+03\n",
      "   -1.2642933e+03 -1.2642933e+03]\n",
      "  [-7.9969000e+00 -7.9969000e+00 -7.9969000e+00 ... -7.9966000e+00\n",
      "   -7.9966000e+00 -7.9966000e+00]\n",
      "  ...\n",
      "  [-1.6790400e+01  2.2907300e+01 -2.5903400e+01 ... -4.9934000e+00\n",
      "    8.3640000e+00 -1.1110400e+01]\n",
      "  [-4.5898420e+02  1.9531240e+02  1.0742180e+02 ... -1.7089840e+02\n",
      "    1.5625000e+02 -1.4648430e+02]\n",
      "  [-6.1607360e+02 -4.8828130e+02  1.5239716e+03 ... -9.1552730e+02\n",
      "    5.8746340e+02 -2.2315980e+02]]\n",
      "\n",
      " [[-1.2091234e+03 -1.2091234e+03 -1.2091234e+03 ... -1.2499131e+03\n",
      "   -1.2499131e+03 -1.2499131e+03]\n",
      "  [-1.2642933e+03 -1.2642933e+03 -1.2642933e+03 ... -1.2938568e+03\n",
      "   -1.2938568e+03 -1.2938568e+03]\n",
      "  [-7.9966000e+00 -7.9966000e+00 -7.9966000e+00 ... -7.9744000e+00\n",
      "   -7.9744000e+00 -7.9744000e+00]\n",
      "  ...\n",
      "  [ 1.3295000e+01 -1.5417200e+01  1.6665600e+01 ...  1.2546000e+01\n",
      "   -1.2421100e+01  1.2546000e+01]\n",
      "  [ 1.2207030e+02 -9.2773400e+01  5.8593700e+01 ...  7.8125000e+01\n",
      "   -8.7890600e+01  7.3242200e+01]\n",
      "  [-1.3732910e+02  4.8828130e+02 -8.4114070e+02 ... -7.7438350e+02\n",
      "    7.5721740e+02 -7.6103210e+02]]]\n",
      "13/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/246_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (208, 49, 1024), Y_shape: (208,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR21_20200508_PGS_11_BSD_11/247_2020_05_08.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-7.1216810e+02 -7.1216810e+02 -7.1216810e+02 ... -7.0061590e+02\n",
      "   -7.0061590e+02 -7.0061590e+02]\n",
      "  [-7.0000060e+02 -7.0000060e+02 -7.0000060e+02 ... -7.0480820e+02\n",
      "   -7.0480820e+02 -7.0480820e+02]\n",
      "  [ 4.9438000e+00  4.9438000e+00  4.9438000e+00 ...  6.1460000e-01\n",
      "    6.1460000e-01  6.1460000e-01]\n",
      "  ...\n",
      "  [ 2.1846000e+00  2.4967000e+00  2.8712000e+00 ...  4.6938200e+01\n",
      "    4.3318000e+01  3.7700400e+01]\n",
      "  [ 2.3925770e+02  2.3925770e+02  2.6855460e+02 ...  8.1640602e+03\n",
      "    7.8173806e+03  7.1533183e+03]\n",
      "  [ 3.3950810e+02  3.3950810e+02  3.9863590e+02 ...  7.3928833e+03\n",
      "    7.0705414e+03  6.4525604e+03]]\n",
      "\n",
      " [[-7.0061590e+02 -7.0061590e+02 -7.0061590e+02 ... -7.0170440e+02\n",
      "   -7.0170440e+02 -7.0170440e+02]\n",
      "  [-7.0480820e+02 -7.0480820e+02 -7.0480820e+02 ... -7.3217490e+02\n",
      "   -7.3217490e+02 -7.3217490e+02]\n",
      "  [ 6.1460000e-01  6.1460000e-01  6.1460000e-01 ... -1.1150000e+00\n",
      "   -1.1150000e+00 -1.1150000e+00]\n",
      "  ...\n",
      "  [ 2.9648500e+01  2.0597900e+01  1.0548600e+01 ... -3.9323000e+00\n",
      "   -1.5167500e+01 -2.2595300e+01]\n",
      "  [ 6.1083967e+03  4.8095690e+03  3.4277334e+03 ...  8.7695288e+03\n",
      "    7.0703105e+03  3.3105460e+03]\n",
      "  [ 5.4988861e+03  4.2762756e+03  2.8610229e+03 ...  1.0948181e+03\n",
      "   -9.6130370e+02 -2.5939941e+03]]\n",
      "\n",
      " [[-7.0170440e+02 -7.0170440e+02 -7.0170440e+02 ... -7.2012960e+02\n",
      "   -7.2012960e+02 -7.2012960e+02]\n",
      "  [-7.3217490e+02 -7.3217490e+02 -7.3217490e+02 ... -7.7252030e+02\n",
      "   -7.7252030e+02 -7.7252030e+02]\n",
      "  [-1.1150000e+00 -1.1150000e+00 -1.1150000e+00 ... -5.9385000e+00\n",
      "   -5.9385000e+00 -5.9385000e+00]\n",
      "  ...\n",
      "  [-2.5965800e+01 -2.7214200e+01 -2.5029600e+01 ... -4.4503900e+01\n",
      "   -4.3068300e+01 -2.6402700e+01]\n",
      "  [-1.6064449e+03 -6.5820294e+03 -9.9072238e+03 ... -1.1230470e+02\n",
      "    5.4687480e+02  1.3037106e+03]\n",
      "  [-3.5743713e+03 -4.0473938e+03 -4.1275024e+03 ... -5.2871704e+03\n",
      "   -6.8073273e+03 -6.1969757e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.1275236e+03 -1.1275236e+03 -1.1275236e+03 ... -1.1683236e+03\n",
      "   -1.1683236e+03 -1.1683236e+03]\n",
      "  [-1.1829203e+03 -1.1829203e+03 -1.1829203e+03 ... -1.2237203e+03\n",
      "   -1.2237203e+03 -1.2237203e+03]\n",
      "  [-7.9959000e+00 -7.9959000e+00 -7.9959000e+00 ... -7.9969000e+00\n",
      "   -7.9969000e+00 -7.9969000e+00]\n",
      "  ...\n",
      "  [ 1.0735900e+01 -2.1034800e+01  2.5279200e+01 ... -1.0611000e+01\n",
      "    9.3630000e-01  8.9258000e+00]\n",
      "  [ 1.2451168e+03 -1.3281246e+03  9.5214820e+02 ...  7.7148420e+02\n",
      "   -7.7148420e+02  6.5429670e+02]\n",
      "  [ 1.7547607e+03 -1.9073490e+02 -1.4457703e+03 ...  2.7923584e+03\n",
      "   -2.3632050e+03  1.5869141e+03]]\n",
      "\n",
      " [[-1.1683236e+03 -1.1683236e+03 -1.1683236e+03 ... -1.2091232e+03\n",
      "   -1.2091232e+03 -1.2091232e+03]\n",
      "  [-1.2237203e+03 -1.2237203e+03 -1.2237203e+03 ... -1.2642933e+03\n",
      "   -1.2642933e+03 -1.2642933e+03]\n",
      "  [-7.9969000e+00 -7.9969000e+00 -7.9969000e+00 ... -7.9967000e+00\n",
      "   -7.9967000e+00 -7.9967000e+00]\n",
      "  ...\n",
      "  [-1.7227300e+01  2.3281900e+01 -2.5716100e+01 ... -5.1183000e+00\n",
      "    8.6137000e+00 -1.1172800e+01]\n",
      "  [-4.4433580e+02  1.6601560e+02  1.2695310e+02 ... -1.6113280e+02\n",
      "    1.7089840e+02 -1.4160150e+02]\n",
      "  [-5.7792660e+02 -5.3215030e+02  1.5659332e+03 ... -8.9645390e+02\n",
      "    5.7220460e+02 -2.1362300e+02]]\n",
      "\n",
      " [[-1.2091232e+03 -1.2091232e+03 -1.2091232e+03 ... -1.2499133e+03\n",
      "   -1.2499133e+03 -1.2499133e+03]\n",
      "  [-1.2642933e+03 -1.2642933e+03 -1.2642933e+03 ... -1.2938568e+03\n",
      "   -1.2938568e+03 -1.2938568e+03]\n",
      "  [-7.9967000e+00 -7.9967000e+00 -7.9967000e+00 ... -7.9746000e+00\n",
      "   -7.9746000e+00 -7.9746000e+00]\n",
      "  ...\n",
      "  [ 1.3482300e+01 -1.5354800e+01  1.6728000e+01 ...  1.2546000e+01\n",
      "   -1.2171500e+01  1.2608400e+01]\n",
      "  [ 1.0253900e+02 -7.8125000e+01  4.3945300e+01 ...  7.8125000e+01\n",
      "   -7.3242200e+01  7.8125000e+01]\n",
      "  [-1.4686580e+02  4.9781800e+02 -8.3732600e+02 ... -7.6484680e+02\n",
      "    7.5531010e+02 -7.5149540e+02]]]\n",
      "14/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/247_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (224, 49, 1024), Y_shape: (224,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR21_20200508_PGS_11_BSD_11/248_2020_05_08.csv\n",
      "[[[-7.1216830e+02 -7.1216830e+02 -7.1216830e+02 ... -7.0061580e+02\n",
      "   -7.0061580e+02 -7.0061580e+02]\n",
      "  [-7.0000060e+02 -7.0000060e+02 -7.0000060e+02 ... -7.0480820e+02\n",
      "   -7.0480820e+02 -7.0480820e+02]\n",
      "  [ 4.9439000e+00  4.9439000e+00  4.9439000e+00 ...  6.1450000e-01\n",
      "    6.1450000e-01  6.1450000e-01]\n",
      "  ...\n",
      "  [ 2.1846000e+00  2.6215000e+00  2.9336000e+00 ...  4.6875800e+01\n",
      "    4.3380400e+01  3.7513100e+01]\n",
      "  [ 2.4414060e+02  2.4414060e+02  2.7832020e+02 ...  8.1396462e+03\n",
      "    7.8027322e+03  7.1240214e+03]\n",
      "  [ 3.4904480e+02  3.4904480e+02  4.0626530e+02 ...  7.3909760e+03\n",
      "    7.0724487e+03  6.4544678e+03]]\n",
      "\n",
      " [[-7.0061580e+02 -7.0061580e+02 -7.0061580e+02 ... -7.0170440e+02\n",
      "   -7.0170440e+02 -7.0170440e+02]\n",
      "  [-7.0480820e+02 -7.0480820e+02 -7.0480820e+02 ... -7.3217490e+02\n",
      "   -7.3217490e+02 -7.3217490e+02]\n",
      "  [ 6.1450000e-01  6.1450000e-01  6.1450000e-01 ... -1.1149000e+00\n",
      "   -1.1149000e+00 -1.1149000e+00]\n",
      "  ...\n",
      "  [ 3.0147800e+01  2.0847600e+01  1.0673500e+01 ... -3.9323000e+00\n",
      "   -1.4793000e+01 -2.2595300e+01]\n",
      "  [ 6.0546858e+03  4.8095690e+03  3.3886709e+03 ...  8.9648412e+03\n",
      "    6.9970684e+03  3.0566398e+03]\n",
      "  [ 5.5007935e+03  4.2858124e+03  2.8553009e+03 ...  1.0948181e+03\n",
      "   -9.4223020e+02 -2.5215149e+03]]\n",
      "\n",
      " [[-7.0170440e+02 -7.0170440e+02 -7.0170440e+02 ... -7.2012960e+02\n",
      "   -7.2012960e+02 -7.2012960e+02]\n",
      "  [-7.3217490e+02 -7.3217490e+02 -7.3217490e+02 ... -7.7252030e+02\n",
      "   -7.7252030e+02 -7.7252030e+02]\n",
      "  [-1.1149000e+00 -1.1149000e+00 -1.1149000e+00 ... -5.9386000e+00\n",
      "   -5.9386000e+00 -5.9386000e+00]\n",
      "  ...\n",
      "  [-2.6215500e+01 -2.7651100e+01 -2.5341600e+01 ... -4.4379100e+01\n",
      "   -4.3380400e+01 -2.5903400e+01]\n",
      "  [-2.0263666e+03 -6.8261700e+03 -9.8876925e+03 ... -9.7656000e+00\n",
      "    6.3476540e+02  1.2988278e+03]\n",
      "  [-3.5514832e+03 -4.0435791e+03 -4.1637421e+03 ... -5.3386688e+03\n",
      "   -6.8378448e+03 -6.1740875e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.1275231e+03 -1.1275231e+03 -1.1275231e+03 ... -1.1683237e+03\n",
      "   -1.1683237e+03 -1.1683237e+03]\n",
      "  [-1.1829203e+03 -1.1829203e+03 -1.1829203e+03 ... -1.2237203e+03\n",
      "   -1.2237203e+03 -1.2237203e+03]\n",
      "  [-7.9967000e+00 -7.9967000e+00 -7.9967000e+00 ... -7.9971000e+00\n",
      "   -7.9971000e+00 -7.9971000e+00]\n",
      "  ...\n",
      "  [ 9.5499000e+00 -1.9411900e+01  2.3968500e+01 ... -1.0985500e+01\n",
      "    1.3108000e+00  8.7385000e+00]\n",
      "  [ 1.1767575e+03 -1.3330074e+03  1.0302731e+03 ...  8.0566380e+02\n",
      "   -7.9101540e+02  6.7871070e+02]\n",
      "  [ 1.7433167e+03 -2.8610230e+02 -1.2798309e+03 ...  2.8038025e+03\n",
      "   -2.3899078e+03  1.6231537e+03]]\n",
      "\n",
      " [[-1.1683237e+03 -1.1683237e+03 -1.1683237e+03 ... -1.2091231e+03\n",
      "   -1.2091231e+03 -1.2091231e+03]\n",
      "  [-1.2237203e+03 -1.2237203e+03 -1.2237203e+03 ... -1.2642933e+03\n",
      "   -1.2642933e+03 -1.2642933e+03]\n",
      "  [-7.9971000e+00 -7.9971000e+00 -7.9971000e+00 ... -7.9967000e+00\n",
      "   -7.9967000e+00 -7.9967000e+00]\n",
      "  ...\n",
      "  [-1.6790400e+01  2.3032200e+01 -2.6028200e+01 ... -5.0558000e+00\n",
      "    8.4264000e+00 -1.1235200e+01]\n",
      "  [-4.6874990e+02  1.8066400e+02  1.1718750e+02 ... -1.8066400e+02\n",
      "    1.6113280e+02 -1.4160150e+02]\n",
      "  [-6.1416630e+02 -4.9781800e+02  1.5354156e+03 ... -9.1362000e+02\n",
      "    5.8937070e+02 -2.2315980e+02]]\n",
      "\n",
      " [[-1.2091231e+03 -1.2091231e+03 -1.2091231e+03 ... -1.2499134e+03\n",
      "   -1.2499134e+03 -1.2499134e+03]\n",
      "  [-1.2642933e+03 -1.2642933e+03 -1.2642933e+03 ... -1.2938568e+03\n",
      "   -1.2938568e+03 -1.2938568e+03]\n",
      "  [-7.9967000e+00 -7.9967000e+00 -7.9967000e+00 ... -7.9745000e+00\n",
      "   -7.9745000e+00 -7.9745000e+00]\n",
      "  ...\n",
      "  [ 1.3419800e+01 -1.5604500e+01  1.6977700e+01 ...  1.2546000e+01\n",
      "   -1.2421100e+01  1.2670800e+01]\n",
      "  [ 1.1718750e+02 -8.3007800e+01  5.3710900e+01 ...  8.3007800e+01\n",
      "   -8.3007800e+01  7.3242200e+01]\n",
      "  [-1.4495850e+02  4.9209590e+02 -8.4304810e+02 ... -7.7438350e+02\n",
      "    7.6103210e+02 -7.6293950e+02]]]\n",
      "15/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/248_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (240, 49, 1024), Y_shape: (240,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR21_20200508_PGS_11_BSD_11/240_2020_05_08.csv\n",
      "[[[-7.12167900e+02 -7.12167900e+02 -7.12167900e+02 ... -7.00615900e+02\n",
      "   -7.00615900e+02 -7.00615900e+02]\n",
      "  [-7.00000600e+02 -7.00000600e+02 -7.00000600e+02 ... -7.04808200e+02\n",
      "   -7.04808200e+02 -7.04808200e+02]\n",
      "  [ 4.94380000e+00  4.94380000e+00  4.94380000e+00 ...  6.14400000e-01\n",
      "    6.14400000e-01  6.14400000e-01]\n",
      "  ...\n",
      "  [ 2.30950000e+00  2.62150000e+00  2.99610000e+00 ...  4.65637000e+01\n",
      "    4.29435000e+01  3.70762000e+01]\n",
      "  [ 2.44140600e+02  2.44140600e+02  2.83203000e+02 ...  8.04198990e+03\n",
      "    7.71972440e+03  7.05077930e+03]\n",
      "  [ 3.54766800e+02  3.54766800e+02  4.13894700e+02 ...  7.34519960e+03\n",
      "    7.02667240e+03  6.39534000e+03]]\n",
      "\n",
      " [[-7.00615900e+02 -7.00615900e+02 -7.00615900e+02 ... -7.01705000e+02\n",
      "   -7.01705000e+02 -7.01705000e+02]\n",
      "  [-7.04808200e+02 -7.04808200e+02 -7.04808200e+02 ... -7.32174900e+02\n",
      "   -7.32174900e+02 -7.32174900e+02]\n",
      "  [ 6.14400000e-01  6.14400000e-01  6.14400000e-01 ... -1.11480000e+00\n",
      "   -1.11480000e+00 -1.11480000e+00]\n",
      "  ...\n",
      "  [ 2.96485000e+01  2.03482000e+01  1.04238000e+01 ... -3.62020000e+00\n",
      "   -1.45434000e+01 -2.19087000e+01]\n",
      "  [ 6.02538890e+03  4.77538930e+03  3.37890530e+03 ...  9.25780990e+03\n",
      "    7.31933390e+03  3.28613190e+03]\n",
      "  [ 5.45120240e+03  4.22096250e+03  2.80189510e+03 ...  1.13677980e+03\n",
      "   -9.05990600e+02 -2.47383120e+03]]\n",
      "\n",
      " [[-7.01705000e+02 -7.01705000e+02 -7.01705000e+02 ... -7.20129700e+02\n",
      "   -7.20129700e+02 -7.20129700e+02]\n",
      "  [-7.32174900e+02 -7.32174900e+02 -7.32174900e+02 ... -7.72520300e+02\n",
      "   -7.72520300e+02 -7.72520300e+02]\n",
      "  [-1.11480000e+00 -1.11480000e+00 -1.11480000e+00 ... -5.93810000e+00\n",
      "   -5.93810000e+00 -5.93810000e+00]\n",
      "  ...\n",
      "  [-2.59658000e+01 -2.72766000e+01 -2.57161000e+01 ... -4.45039000e+01\n",
      "   -4.36925000e+01 -2.61531000e+01]\n",
      "  [-1.97265570e+03 -7.04101370e+03 -1.01513644e+04 ... -1.07421800e+02\n",
      "    6.39648300e+02  1.43066370e+03]\n",
      "  [-3.48663330e+03 -3.98826600e+03 -4.11796570e+03 ... -5.31005860e+03\n",
      "   -6.83021550e+03 -6.21604920e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.12752420e+03 -1.12752420e+03 -1.12752420e+03 ... -1.16832360e+03\n",
      "   -1.16832360e+03 -1.16832360e+03]\n",
      "  [-1.18292030e+03 -1.18292030e+03 -1.18292030e+03 ... -1.22372030e+03\n",
      "   -1.22372030e+03 -1.22372030e+03]\n",
      "  [-7.99600000e+00 -7.99600000e+00 -7.99600000e+00 ... -7.99600000e+00\n",
      "   -7.99600000e+00 -7.99600000e+00]\n",
      "  ...\n",
      "  [ 1.54172000e+01 -2.49671000e+01  2.64652000e+01 ... -9.67480000e+00\n",
      "    1.24800000e-01  9.36270000e+00]\n",
      "  [ 1.24511680e+03 -1.17675750e+03  7.27538900e+02 ...  7.51952900e+02\n",
      "   -7.37304500e+02  6.10351400e+02]\n",
      "  [ 1.41334530e+03  3.52859500e+02 -2.01034550e+03 ...  2.66647340e+03\n",
      "   -2.23159790e+03  1.46484380e+03]]\n",
      "\n",
      " [[-1.16832360e+03 -1.16832360e+03 -1.16832360e+03 ... -1.20912420e+03\n",
      "   -1.20912420e+03 -1.20912420e+03]\n",
      "  [-1.22372030e+03 -1.22372030e+03 -1.22372030e+03 ... -1.26429330e+03\n",
      "   -1.26429330e+03 -1.26429330e+03]\n",
      "  [-7.99600000e+00 -7.99600000e+00 -7.99600000e+00 ... -7.99660000e+00\n",
      "   -7.99660000e+00 -7.99660000e+00]\n",
      "  ...\n",
      "  [-1.67904000e+01  2.25328000e+01 -2.49671000e+01 ... -4.93100000e+00\n",
      "    8.30160000e+00 -1.07983000e+01]\n",
      "  [-4.00390500e+02  1.56250000e+02  1.12304700e+02 ... -1.70898400e+02\n",
      "    1.51367100e+02 -1.41601500e+02]\n",
      "  [-4.80651900e+02 -5.87463400e+02  1.56974790e+03 ... -8.81195100e+02\n",
      "    5.62667800e+02 -2.04086300e+02]]\n",
      "\n",
      " [[-1.20912420e+03 -1.20912420e+03 -1.20912420e+03 ... -1.24991350e+03\n",
      "   -1.24991350e+03 -1.24991350e+03]\n",
      "  [-1.26429330e+03 -1.26429330e+03 -1.26429330e+03 ... -1.29385680e+03\n",
      "   -1.29385680e+03 -1.29385680e+03]\n",
      "  [-7.99660000e+00 -7.99660000e+00 -7.99660000e+00 ... -7.97380000e+00\n",
      "   -7.97380000e+00 -7.97380000e+00]\n",
      "  ...\n",
      "  [ 1.31702000e+01 -1.52300000e+01  1.65407000e+01 ...  1.25460000e+01\n",
      "   -1.24211000e+01  1.26084000e+01]\n",
      "  [ 1.22070300e+02 -8.78906000e+01  5.37109000e+01 ...  6.34765000e+01\n",
      "   -7.32422000e+01  7.32422000e+01]\n",
      "  [-1.58309900e+02  5.05447400e+02 -8.48770100e+02 ... -7.72476200e+02\n",
      "    7.66754200e+02 -7.57217400e+02]]]\n",
      "16/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/240_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (256, 49, 1024), Y_shape: (256,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR21_20200508_PGS_11_BSD_11/242_2020_05_08.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-7.1216810e+02 -7.1216810e+02 -7.1216810e+02 ... -7.0061600e+02\n",
      "   -7.0061600e+02 -7.0061600e+02]\n",
      "  [-7.0000060e+02 -7.0000060e+02 -7.0000060e+02 ... -7.0480820e+02\n",
      "   -7.0480820e+02 -7.0480820e+02]\n",
      "  [ 4.9438000e+00  4.9438000e+00  4.9438000e+00 ...  6.1440000e-01\n",
      "    6.1440000e-01  6.1440000e-01]\n",
      "  ...\n",
      "  [ 2.1846000e+00  2.5591000e+00  2.9961000e+00 ...  4.6501300e+01\n",
      "    4.3130700e+01  3.7263400e+01]\n",
      "  [ 2.3437490e+02  2.3437490e+02  2.7832020e+02 ...  8.0126931e+03\n",
      "    7.6953103e+03  7.0165996e+03]\n",
      "  [ 3.4713750e+02  3.4713750e+02  4.0626530e+02 ...  7.3528290e+03\n",
      "    7.0190430e+03  6.3877106e+03]]\n",
      "\n",
      " [[-7.0061600e+02 -7.0061600e+02 -7.0061600e+02 ... -7.0170470e+02\n",
      "   -7.0170470e+02 -7.0170470e+02]\n",
      "  [-7.0480820e+02 -7.0480820e+02 -7.0480820e+02 ... -7.3217490e+02\n",
      "   -7.3217490e+02 -7.3217490e+02]\n",
      "  [ 6.1440000e-01  6.1440000e-01  6.1440000e-01 ... -1.1148000e+00\n",
      "   -1.1148000e+00 -1.1148000e+00]\n",
      "  ...\n",
      "  [ 2.9274000e+01  2.0285800e+01  1.0236500e+01 ... -4.0572000e+00\n",
      "   -1.5542000e+01 -2.2720100e+01]\n",
      "  [ 5.9765608e+03  4.6923815e+03  3.2910147e+03 ...  8.6962866e+03\n",
      "    6.9531231e+03  3.2275382e+03]\n",
      "  [ 5.4435730e+03  4.1942596e+03  2.7885437e+03 ...  1.0814667e+03\n",
      "   -9.8991390e+02 -2.6187897e+03]]\n",
      "\n",
      " [[-7.0170470e+02 -7.0170470e+02 -7.0170470e+02 ... -7.2012970e+02\n",
      "   -7.2012970e+02 -7.2012970e+02]\n",
      "  [-7.3217490e+02 -7.3217490e+02 -7.3217490e+02 ... -7.7252030e+02\n",
      "   -7.7252030e+02 -7.7252030e+02]\n",
      "  [-1.1148000e+00 -1.1148000e+00 -1.1148000e+00 ... -5.9384000e+00\n",
      "   -5.9384000e+00 -5.9384000e+00]\n",
      "  ...\n",
      "  [-2.6277900e+01 -2.7214200e+01 -2.5216800e+01 ... -4.4316700e+01\n",
      "   -4.3318000e+01 -2.6465200e+01]\n",
      "  [-1.7285151e+03 -6.7236309e+03 -9.9511691e+03 ... -1.1718750e+02\n",
      "    5.6152330e+02  1.3818356e+03]\n",
      "  [-3.6048889e+03 -4.0912628e+03 -4.1561127e+03 ... -5.2871704e+03\n",
      "   -6.8149567e+03 -6.2217712e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.1275241e+03 -1.1275241e+03 -1.1275241e+03 ... -1.1683238e+03\n",
      "   -1.1683238e+03 -1.1683238e+03]\n",
      "  [-1.1829203e+03 -1.1829203e+03 -1.1829203e+03 ... -1.2237203e+03\n",
      "   -1.2237203e+03 -1.2237203e+03]\n",
      "  [-7.9960000e+00 -7.9960000e+00 -7.9960000e+00 ... -7.9963000e+00\n",
      "   -7.9963000e+00 -7.9963000e+00]\n",
      "  ...\n",
      "  [ 1.1609700e+01 -2.1471700e+01  2.5341600e+01 ... -1.0548600e+01\n",
      "    9.9870000e-01  8.9882000e+00]\n",
      "  [ 1.2988278e+03 -1.3281246e+03  9.0820290e+02 ...  7.7148420e+02\n",
      "   -7.6171850e+02  6.4941390e+02]\n",
      "  [ 1.6555786e+03 -7.0571900e+01 -1.5506744e+03 ...  2.7618408e+03\n",
      "   -2.3422241e+03  1.5735626e+03]]\n",
      "\n",
      " [[-1.1683238e+03 -1.1683238e+03 -1.1683238e+03 ... -1.2091235e+03\n",
      "   -1.2091235e+03 -1.2091235e+03]\n",
      "  [-1.2237203e+03 -1.2237203e+03 -1.2237203e+03 ... -1.2642933e+03\n",
      "   -1.2642933e+03 -1.2642933e+03]\n",
      "  [-7.9963000e+00 -7.9963000e+00 -7.9963000e+00 ... -7.9968000e+00\n",
      "   -7.9968000e+00 -7.9968000e+00]\n",
      "  ...\n",
      "  [-1.6790400e+01  2.3032200e+01 -2.5653700e+01 ... -5.1183000e+00\n",
      "    8.4264000e+00 -1.1172800e+01]\n",
      "  [-4.4433580e+02  1.7089840e+02  1.1230470e+02 ... -1.6113280e+02\n",
      "    1.4648430e+02 -1.3671870e+02]\n",
      "  [-5.7792660e+02 -5.1307680e+02  1.5487671e+03 ... -9.0789790e+02\n",
      "    5.7792660e+02 -2.1553040e+02]]\n",
      "\n",
      " [[-1.2091235e+03 -1.2091235e+03 -1.2091235e+03 ... -1.2499136e+03\n",
      "   -1.2499136e+03 -1.2499136e+03]\n",
      "  [-1.2642933e+03 -1.2642933e+03 -1.2642933e+03 ... -1.2938568e+03\n",
      "   -1.2938568e+03 -1.2938568e+03]\n",
      "  [-7.9968000e+00 -7.9968000e+00 -7.9968000e+00 ... -7.9741000e+00\n",
      "   -7.9741000e+00 -7.9741000e+00]\n",
      "  ...\n",
      "  [ 1.3482300e+01 -1.5417200e+01  1.6915200e+01 ...  1.2670800e+01\n",
      "   -1.2421100e+01  1.2795700e+01]\n",
      "  [ 1.1230470e+02 -7.3242200e+01  4.8828100e+01 ...  7.8125000e+01\n",
      "   -6.8359400e+01  6.8359400e+01]\n",
      "  [-1.4686580e+02  5.0354000e+02 -8.5067750e+02 ... -7.8010560e+02\n",
      "    7.6866150e+02 -7.6675420e+02]]]\n",
      "17/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/242_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (272, 49, 1024), Y_shape: (272,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR21_20200508_PGS_11_BSD_11/245_2020_05_08.csv\n",
      "[[[-7.1216810e+02 -7.1216810e+02 -7.1216810e+02 ... -7.0061540e+02\n",
      "   -7.0061540e+02 -7.0061540e+02]\n",
      "  [-7.0000060e+02 -7.0000060e+02 -7.0000060e+02 ... -7.0480820e+02\n",
      "   -7.0480820e+02 -7.0480820e+02]\n",
      "  [ 4.9442000e+00  4.9442000e+00  4.9442000e+00 ...  6.1430000e-01\n",
      "    6.1430000e-01  6.1430000e-01]\n",
      "  ...\n",
      "  [ 2.1846000e+00  2.4967000e+00  2.9961000e+00 ...  4.6688500e+01\n",
      "    4.3193100e+01  3.7513100e+01]\n",
      "  [ 2.3437490e+02  2.3437490e+02  2.7343740e+02 ...  8.1005837e+03\n",
      "    7.7246072e+03  7.0800761e+03]\n",
      "  [ 3.4713750e+02  3.4713750e+02  4.0054320e+02 ...  7.3757172e+03\n",
      "    7.0514679e+03  6.4392090e+03]]\n",
      "\n",
      " [[-7.0061540e+02 -7.0061540e+02 -7.0061540e+02 ... -7.0170460e+02\n",
      "   -7.0170460e+02 -7.0170460e+02]\n",
      "  [-7.0480820e+02 -7.0480820e+02 -7.0480820e+02 ... -7.3217490e+02\n",
      "   -7.3217490e+02 -7.3217490e+02]\n",
      "  [ 6.1430000e-01  6.1430000e-01  6.1430000e-01 ... -1.1152000e+00\n",
      "   -1.1152000e+00 -1.1152000e+00]\n",
      "  ...\n",
      "  [ 2.9461200e+01  2.0597900e+01  1.0361400e+01 ... -3.9323000e+00\n",
      "   -1.4917900e+01 -2.2408000e+01]\n",
      "  [ 6.0302718e+03  4.7949205e+03  3.4082022e+03 ...  8.9160131e+03\n",
      "    6.9628887e+03  3.0078117e+03]\n",
      "  [ 5.4874420e+03  4.2495728e+03  2.8381348e+03 ...  1.0852814e+03\n",
      "   -9.4223020e+02 -2.5367737e+03]]\n",
      "\n",
      " [[-7.0170460e+02 -7.0170460e+02 -7.0170460e+02 ... -7.2012980e+02\n",
      "   -7.2012980e+02 -7.2012980e+02]\n",
      "  [-7.3217490e+02 -7.3217490e+02 -7.3217490e+02 ... -7.7252030e+02\n",
      "   -7.7252030e+02 -7.7252030e+02]\n",
      "  [-1.1152000e+00 -1.1152000e+00 -1.1152000e+00 ... -5.9387000e+00\n",
      "   -5.9387000e+00 -5.9387000e+00]\n",
      "  ...\n",
      "  [-2.6215500e+01 -2.7588700e+01 -2.5404100e+01 ... -4.4441500e+01\n",
      "   -4.3318000e+01 -2.5965800e+01]\n",
      "  [-2.0166010e+03 -6.8652325e+03 -9.9023410e+03 ... -1.4648400e+01\n",
      "    6.5917950e+02  1.3281246e+03]\n",
      "  [-3.5667419e+03 -4.0626526e+03 -4.1942596e+03 ... -5.3386688e+03\n",
      "   -6.8435669e+03 -6.1817169e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.1275241e+03 -1.1275241e+03 -1.1275241e+03 ... -1.1683233e+03\n",
      "   -1.1683233e+03 -1.1683233e+03]\n",
      "  [-1.1829203e+03 -1.1829203e+03 -1.1829203e+03 ... -1.2237203e+03\n",
      "   -1.2237203e+03 -1.2237203e+03]\n",
      "  [-7.9968000e+00 -7.9968000e+00 -7.9968000e+00 ... -7.9966000e+00\n",
      "   -7.9966000e+00 -7.9966000e+00]\n",
      "  ...\n",
      "  [ 1.0611000e+01 -2.0285800e+01  2.4904700e+01 ... -1.0735900e+01\n",
      "    1.1235000e+00  8.8633000e+00]\n",
      "  [ 1.1816403e+03 -1.2695309e+03  9.1796850e+02 ...  7.9101540e+02\n",
      "   -7.9589820e+02  6.6894510e+02]\n",
      "  [ 1.7318726e+03 -2.0027160e+02 -1.4076233e+03 ...  2.7904510e+03\n",
      "   -2.3746490e+03  1.6040802e+03]]\n",
      "\n",
      " [[-1.1683233e+03 -1.1683233e+03 -1.1683233e+03 ... -1.2091237e+03\n",
      "   -1.2091237e+03 -1.2091237e+03]\n",
      "  [-1.2237203e+03 -1.2237203e+03 -1.2237203e+03 ... -1.2642933e+03\n",
      "   -1.2642933e+03 -1.2642933e+03]\n",
      "  [-7.9966000e+00 -7.9966000e+00 -7.9966000e+00 ... -7.9969000e+00\n",
      "   -7.9969000e+00 -7.9969000e+00]\n",
      "  ...\n",
      "  [-1.6915200e+01  2.3157000e+01 -2.5841000e+01 ... -5.1183000e+00\n",
      "    8.4888000e+00 -1.1172800e+01]\n",
      "  [-4.4433580e+02  1.7578120e+02  1.0742180e+02 ... -1.6601560e+02\n",
      "    1.6113280e+02 -1.4648430e+02]\n",
      "  [-6.0653690e+02 -5.0544740e+02  1.5411377e+03 ... -9.0789790e+02\n",
      "    5.7983400e+02 -2.2506710e+02]]\n",
      "\n",
      " [[-1.2091237e+03 -1.2091237e+03 -1.2091237e+03 ... -1.2499134e+03\n",
      "   -1.2499134e+03 -1.2499134e+03]\n",
      "  [-1.2642933e+03 -1.2642933e+03 -1.2642933e+03 ... -1.2938568e+03\n",
      "   -1.2938568e+03 -1.2938568e+03]\n",
      "  [-7.9969000e+00 -7.9969000e+00 -7.9969000e+00 ... -7.9742000e+00\n",
      "   -7.9742000e+00 -7.9742000e+00]\n",
      "  ...\n",
      "  [ 1.3544700e+01 -1.5479600e+01  1.7040100e+01 ...  1.2733200e+01\n",
      "   -1.2421100e+01  1.2733200e+01]\n",
      "  [ 1.2207030e+02 -8.7890600e+01  4.8828100e+01 ...  6.8359400e+01\n",
      "   -7.8125000e+01  6.8359400e+01]\n",
      "  [-1.4686580e+02  5.0354000e+02 -8.4877010e+02 ... -7.8010560e+02\n",
      "    7.6866150e+02 -7.7056880e+02]]]\n",
      "18/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/245_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (288, 49, 1024), Y_shape: (288,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR21_20200508_PGS_11_BSD_11/244_2020_05_08.csv\n",
      "[[[-7.1216810e+02 -7.1216810e+02 -7.1216810e+02 ... -7.0061590e+02\n",
      "   -7.0061590e+02 -7.0061590e+02]\n",
      "  [-7.0000060e+02 -7.0000060e+02 -7.0000060e+02 ... -7.0480820e+02\n",
      "   -7.0480820e+02 -7.0480820e+02]\n",
      "  [ 4.9440000e+00  4.9440000e+00  4.9440000e+00 ...  6.1460000e-01\n",
      "    6.1460000e-01  6.1460000e-01]\n",
      "  ...\n",
      "  [ 2.1846000e+00  2.5591000e+00  2.9336000e+00 ...  4.6813400e+01\n",
      "    4.3005900e+01  3.7513100e+01]\n",
      "  [ 2.3925770e+02  2.3925770e+02  2.7832020e+02 ...  8.1005837e+03\n",
      "    7.7539041e+03  7.0996074e+03]\n",
      "  [ 3.5095210e+02  3.5095210e+02  4.0054320e+02 ...  7.3738098e+03\n",
      "    7.0457458e+03  6.4296722e+03]]\n",
      "\n",
      " [[-7.0061590e+02 -7.0061590e+02 -7.0061590e+02 ... -7.0170440e+02\n",
      "   -7.0170440e+02 -7.0170440e+02]\n",
      "  [-7.0480820e+02 -7.0480820e+02 -7.0480820e+02 ... -7.3217490e+02\n",
      "   -7.3217490e+02 -7.3217490e+02]\n",
      "  [ 6.1460000e-01  6.1460000e-01  6.1460000e-01 ... -1.1151000e+00\n",
      "   -1.1151000e+00 -1.1151000e+00]\n",
      "  ...\n",
      "  [ 2.9336400e+01  2.0722700e+01  1.0361400e+01 ... -3.7451000e+00\n",
      "   -1.5167500e+01 -2.2345600e+01]\n",
      "  [ 6.0498030e+03  4.7753893e+03  3.4082022e+03 ...  8.8476538e+03\n",
      "    6.9824199e+03  3.0957023e+03]\n",
      "  [ 5.4893494e+03  4.2419434e+03  2.8553009e+03 ...  1.0852814e+03\n",
      "   -9.4604490e+02 -2.5596619e+03]]\n",
      "\n",
      " [[-7.0170440e+02 -7.0170440e+02 -7.0170440e+02 ... -7.2012970e+02\n",
      "   -7.2012970e+02 -7.2012970e+02]\n",
      "  [-7.3217490e+02 -7.3217490e+02 -7.3217490e+02 ... -7.7252030e+02\n",
      "   -7.7252030e+02 -7.7252030e+02]\n",
      "  [-1.1151000e+00 -1.1151000e+00 -1.1151000e+00 ... -5.9383000e+00\n",
      "   -5.9383000e+00 -5.9383000e+00]\n",
      "  ...\n",
      "  [-2.5903400e+01 -2.7401400e+01 -2.5279200e+01 ... -4.4566300e+01\n",
      "   -4.3068300e+01 -2.5965800e+01]\n",
      "  [-1.9384760e+03 -6.7968731e+03 -9.8779269e+03 ... -6.8359400e+01\n",
      "    5.7128890e+02  1.3134762e+03]\n",
      "  [-3.5400391e+03 -4.0454865e+03 -4.1408539e+03 ... -5.3119659e+03\n",
      "   -6.8168640e+03 -6.1645508e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.1275239e+03 -1.1275239e+03 -1.1275239e+03 ... -1.1683239e+03\n",
      "   -1.1683239e+03 -1.1683239e+03]\n",
      "  [-1.1829203e+03 -1.1829203e+03 -1.1829203e+03 ... -1.2237203e+03\n",
      "   -1.2237203e+03 -1.2237203e+03]\n",
      "  [-7.9963000e+00 -7.9963000e+00 -7.9963000e+00 ... -7.9967000e+00\n",
      "   -7.9967000e+00 -7.9967000e+00]\n",
      "  ...\n",
      "  [ 1.0548600e+01 -2.0410600e+01  2.4530200e+01 ... -1.0611000e+01\n",
      "    1.0611000e+00  8.8633000e+00]\n",
      "  [ 1.1962887e+03 -1.3085934e+03  9.6191380e+02 ...  7.9101540e+02\n",
      "   -7.8613260e+02  6.7871070e+02]\n",
      "  [ 1.7032623e+03 -1.8501280e+02 -1.4133453e+03 ...  2.7885437e+03\n",
      "   -2.3632050e+03  1.5926361e+03]]\n",
      "\n",
      " [[-1.1683239e+03 -1.1683239e+03 -1.1683239e+03 ... -1.2091234e+03\n",
      "   -1.2091234e+03 -1.2091234e+03]\n",
      "  [-1.2237203e+03 -1.2237203e+03 -1.2237203e+03 ... -1.2642933e+03\n",
      "   -1.2642933e+03 -1.2642933e+03]\n",
      "  [-7.9967000e+00 -7.9967000e+00 -7.9967000e+00 ... -7.9968000e+00\n",
      "   -7.9968000e+00 -7.9968000e+00]\n",
      "  ...\n",
      "  [-1.6915200e+01  2.3094600e+01 -2.5653700e+01 ... -5.1183000e+00\n",
      "    8.5512000e+00 -1.1110400e+01]\n",
      "  [-4.5410140e+02  1.8066400e+02  1.0253900e+02 ... -1.7578120e+02\n",
      "    1.6113280e+02 -1.5136710e+02]\n",
      "  [-5.8937070e+02 -5.0163270e+02  1.5373230e+03 ... -9.0026860e+02\n",
      "    5.7411190e+02 -2.1743770e+02]]\n",
      "\n",
      " [[-1.2091234e+03 -1.2091234e+03 -1.2091234e+03 ... -1.2499136e+03\n",
      "   -1.2499136e+03 -1.2499136e+03]\n",
      "  [-1.2642933e+03 -1.2642933e+03 -1.2642933e+03 ... -1.2938568e+03\n",
      "   -1.2938568e+03 -1.2938568e+03]\n",
      "  [-7.9968000e+00 -7.9968000e+00 -7.9968000e+00 ... -7.9744000e+00\n",
      "   -7.9744000e+00 -7.9744000e+00]\n",
      "  ...\n",
      "  [ 1.3544700e+01 -1.5354800e+01  1.6728000e+01 ...  1.2670800e+01\n",
      "   -1.2296300e+01  1.2733200e+01]\n",
      "  [ 1.2207030e+02 -8.7890600e+01  5.3710900e+01 ...  7.8125000e+01\n",
      "   -7.3242200e+01  6.8359400e+01]\n",
      "  [-1.4114380e+02  4.9591060e+02 -8.4114070e+02 ... -7.6675420e+02\n",
      "    7.6293950e+02 -7.5721740e+02]]]\n",
      "19/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/244_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (304, 49, 1024), Y_shape: (304,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR21_20200508_PGS_11_BSD_11/243_2020_05_08.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-7.1216810e+02 -7.1216810e+02 -7.1216810e+02 ... -7.0061610e+02\n",
      "   -7.0061610e+02 -7.0061610e+02]\n",
      "  [-7.0000060e+02 -7.0000060e+02 -7.0000060e+02 ... -7.0480820e+02\n",
      "   -7.0480820e+02 -7.0480820e+02]\n",
      "  [ 4.9438000e+00  4.9438000e+00  4.9438000e+00 ...  6.1470000e-01\n",
      "    6.1470000e-01  6.1470000e-01]\n",
      "  ...\n",
      "  [ 2.1846000e+00  2.6215000e+00  2.9961000e+00 ...  4.6563700e+01\n",
      "    4.3005900e+01  3.7076200e+01]\n",
      "  [ 2.3925770e+02  2.3925770e+02  2.7832020e+02 ...  8.0419899e+03\n",
      "    7.7148416e+03  7.0410137e+03]\n",
      "  [ 3.5285950e+02  3.5285950e+02  4.1198730e+02 ...  7.3528290e+03\n",
      "    7.0266724e+03  6.3953400e+03]]\n",
      "\n",
      " [[-7.0061610e+02 -7.0061610e+02 -7.0061610e+02 ... -7.0170450e+02\n",
      "   -7.0170450e+02 -7.0170450e+02]\n",
      "  [-7.0480820e+02 -7.0480820e+02 -7.0480820e+02 ... -7.3217490e+02\n",
      "   -7.3217490e+02 -7.3217490e+02]\n",
      "  [ 6.1470000e-01  6.1470000e-01  6.1470000e-01 ... -1.1150000e+00\n",
      "   -1.1150000e+00 -1.1150000e+00]\n",
      "  ...\n",
      "  [ 2.9398800e+01  2.0535500e+01  1.0361400e+01 ... -3.8699000e+00\n",
      "   -1.5105100e+01 -2.2657700e+01]\n",
      "  [ 6.0156233e+03  4.7363268e+03  3.3642569e+03 ...  8.8378882e+03\n",
      "    6.9531231e+03  3.0517570e+03]\n",
      "  [ 5.4645538e+03  4.2076111e+03  2.8209686e+03 ...  1.0890961e+03\n",
      "   -9.7084050e+02 -2.5806427e+03]]\n",
      "\n",
      " [[-7.0170450e+02 -7.0170450e+02 -7.0170450e+02 ... -7.2012960e+02\n",
      "   -7.2012960e+02 -7.2012960e+02]\n",
      "  [-7.3217490e+02 -7.3217490e+02 -7.3217490e+02 ... -7.7252030e+02\n",
      "   -7.7252030e+02 -7.7252030e+02]\n",
      "  [-1.1150000e+00 -1.1150000e+00 -1.1150000e+00 ... -5.9382000e+00\n",
      "   -5.9382000e+00 -5.9382000e+00]\n",
      "  ...\n",
      "  [-2.6527600e+01 -2.7713500e+01 -2.5466500e+01 ... -4.4566300e+01\n",
      "   -4.3318000e+01 -2.5965800e+01]\n",
      "  [-1.9287104e+03 -6.7871075e+03 -9.9072238e+03 ...  0.0000000e+00\n",
      "    6.3476540e+02  1.3330074e+03]\n",
      "  [-3.5934448e+03 -4.0817261e+03 -4.1809082e+03 ... -5.3215027e+03\n",
      "   -6.8168640e+03 -6.1874390e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.1275237e+03 -1.1275237e+03 -1.1275237e+03 ... -1.1683239e+03\n",
      "   -1.1683239e+03 -1.1683239e+03]\n",
      "  [-1.1829203e+03 -1.1829203e+03 -1.1829203e+03 ... -1.2237203e+03\n",
      "   -1.2237203e+03 -1.2237203e+03]\n",
      "  [-7.9965000e+00 -7.9965000e+00 -7.9965000e+00 ... -7.9966000e+00\n",
      "   -7.9966000e+00 -7.9966000e+00]\n",
      "  ...\n",
      "  [ 1.1235200e+01 -2.1222100e+01  2.4967100e+01 ... -1.0735900e+01\n",
      "    9.3630000e-01  9.0506000e+00]\n",
      "  [ 1.2158200e+03 -1.2939450e+03  9.4726540e+02 ...  7.8124980e+02\n",
      "   -7.8613260e+02  6.7382790e+02]\n",
      "  [ 1.6918182e+03 -1.2969970e+02 -1.4915466e+03 ...  2.7751923e+03\n",
      "   -2.3593903e+03  1.5907288e+03]]\n",
      "\n",
      " [[-1.1683239e+03 -1.1683239e+03 -1.1683239e+03 ... -1.2091235e+03\n",
      "   -1.2091235e+03 -1.2091235e+03]\n",
      "  [-1.2237203e+03 -1.2237203e+03 -1.2237203e+03 ... -1.2642933e+03\n",
      "   -1.2642933e+03 -1.2642933e+03]\n",
      "  [-7.9966000e+00 -7.9966000e+00 -7.9966000e+00 ... -7.9968000e+00\n",
      "   -7.9968000e+00 -7.9968000e+00]\n",
      "  ...\n",
      "  [-1.6790400e+01  2.2907300e+01 -2.5841000e+01 ... -5.1183000e+00\n",
      "    8.6137000e+00 -1.1235200e+01]\n",
      "  [-4.5410140e+02  1.8066400e+02  1.0253900e+02 ... -1.6113280e+02\n",
      "    1.5136710e+02 -1.5136710e+02]\n",
      "  [-5.8174130e+02 -5.1498410e+02  1.5487671e+03 ... -8.9836120e+02\n",
      "    5.8364870e+02 -2.1934510e+02]]\n",
      "\n",
      " [[-1.2091235e+03 -1.2091235e+03 -1.2091235e+03 ... -1.2499136e+03\n",
      "   -1.2499136e+03 -1.2499136e+03]\n",
      "  [-1.2642933e+03 -1.2642933e+03 -1.2642933e+03 ... -1.2938568e+03\n",
      "   -1.2938568e+03 -1.2938568e+03]\n",
      "  [-7.9968000e+00 -7.9968000e+00 -7.9968000e+00 ... -7.9743000e+00\n",
      "   -7.9743000e+00 -7.9743000e+00]\n",
      "  ...\n",
      "  [ 1.3419800e+01 -1.5604500e+01  1.6665600e+01 ...  1.2670800e+01\n",
      "   -1.2483600e+01  1.2670800e+01]\n",
      "  [ 1.1718750e+02 -7.8125000e+01  5.8593700e+01 ...  9.2773400e+01\n",
      "   -8.3007800e+01  7.8125000e+01]\n",
      "  [-1.4305110e+02  4.9591060e+02 -8.4877010e+02 ... -7.7629090e+02\n",
      "    7.6866150e+02 -7.6484680e+02]]]\n",
      "20/120 folders downloaded\n",
      "downloaded folder: NR21_20200508_PGS_11_BSD_11/243_2020_05_08.csv\n",
      "Shape of collected datafram: X_shape: (320, 49, 1024), Y_shape: (320,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR02_20200423_PGS_31_BSD_21/046_2020_04_23.csv\n",
      "[[[-7.12167900e+02 -7.12167900e+02 -7.12167900e+02 ... -7.00615600e+02\n",
      "   -7.00615600e+02 -7.00615600e+02]\n",
      "  [-7.00000600e+02 -7.00000600e+02 -7.00000600e+02 ... -7.04808200e+02\n",
      "   -7.04808200e+02 -7.04808200e+02]\n",
      "  [ 4.94360000e+00  4.94360000e+00  4.94360000e+00 ...  6.14900000e-01\n",
      "    6.14900000e-01  6.14900000e-01]\n",
      "  ...\n",
      "  [ 1.87250000e+00  1.99740000e+00  2.37190000e+00 ...  4.48160000e+01\n",
      "    4.05716000e+01  3.48292000e+01]\n",
      "  [ 1.75781200e+02  1.75781200e+02  2.05078100e+02 ...  7.28027140e+03\n",
      "    7.03613080e+03  6.42089660e+03]\n",
      "  [ 2.84194900e+02  2.84194900e+02  3.26156600e+02 ...  7.08580020e+03\n",
      "    6.72721860e+03  6.06155400e+03]]\n",
      "\n",
      " [[-7.00615600e+02 -7.00615600e+02 -7.00615600e+02 ... -7.01704500e+02\n",
      "   -7.01704500e+02 -7.01704500e+02]\n",
      "  [-7.04808200e+02 -7.04808200e+02 -7.04808200e+02 ... -7.32174900e+02\n",
      "   -7.32174900e+02 -7.32174900e+02]\n",
      "  [ 6.14900000e-01  6.14900000e-01  6.14900000e-01 ... -1.11570000e+00\n",
      "   -1.11570000e+00 -1.11570000e+00]\n",
      "  ...\n",
      "  [ 2.74014000e+01  1.87254000e+01  9.11300000e+00 ... -1.12350000e+00\n",
      "   -1.12976000e+01 -1.97865000e+01]\n",
      "  [ 5.69824060e+03  4.72656120e+03  3.55956930e+03 ...  1.03027315e+04\n",
      "    6.49413880e+03  1.07421840e+03]\n",
      "  [ 5.11741640e+03  3.91578670e+03  2.55775450e+03 ...  1.23977660e+03\n",
      "   -4.78744500e+02 -2.00462340e+03]]\n",
      "\n",
      " [[-7.01704500e+02 -7.01704500e+02 -7.01704500e+02 ... -7.20130300e+02\n",
      "   -7.20130300e+02 -7.20130300e+02]\n",
      "  [-7.32174900e+02 -7.32174900e+02 -7.32174900e+02 ... -7.72520300e+02\n",
      "   -7.72520300e+02 -7.72520300e+02]\n",
      "  [-1.11570000e+00 -1.11570000e+00 -1.11570000e+00 ... -5.93790000e+00\n",
      "   -5.93790000e+00 -5.93790000e+00]\n",
      "  ...\n",
      "  [-2.46550000e+01 -2.74638000e+01 -2.57786000e+01 ... -4.57523000e+01\n",
      "   -4.12582000e+01 -2.15966000e+01]\n",
      "  [-4.26757690e+03 -8.18847430e+03 -1.02001925e+04 ...  1.01562470e+03\n",
      "    2.36816340e+03  2.03613220e+03]\n",
      "  [-3.20053100e+03 -3.85856630e+03 -4.17137150e+03 ... -5.75256350e+03\n",
      "   -6.92939760e+03 -5.79833980e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.12752420e+03 -1.12752420e+03 -1.12752420e+03 ... -1.16832350e+03\n",
      "   -1.16832350e+03 -1.16832350e+03]\n",
      "  [-1.18292030e+03 -1.18292030e+03 -1.18292030e+03 ... -1.22372030e+03\n",
      "   -1.22372030e+03 -1.22372030e+03]\n",
      "  [-7.99690000e+00 -7.99690000e+00 -7.99690000e+00 ... -7.99570000e+00\n",
      "   -7.99570000e+00 -7.99570000e+00]\n",
      "  ...\n",
      "  [ 8.36400000e+00 -1.68528000e+01  2.04731000e+01 ... -1.06735000e+01\n",
      "    2.05980000e+00  7.36530000e+00]\n",
      "  [ 1.67480420e+03 -1.49902300e+03  8.10546600e+02 ...  9.37499700e+02\n",
      "   -9.96093500e+02  9.32616900e+02]\n",
      "  [ 1.48200990e+03 -2.21252400e+02 -1.11198430e+03 ...  2.66075130e+03\n",
      "   -2.29072570e+03  1.60789490e+03]]\n",
      "\n",
      " [[-1.16832350e+03 -1.16832350e+03 -1.16832350e+03 ... -1.20912420e+03\n",
      "   -1.20912420e+03 -1.20912420e+03]\n",
      "  [-1.22372030e+03 -1.22372030e+03 -1.22372030e+03 ... -1.26429330e+03\n",
      "   -1.26429330e+03 -1.26429330e+03]\n",
      "  [-7.99570000e+00 -7.99570000e+00 -7.99570000e+00 ... -7.99620000e+00\n",
      "   -7.99620000e+00 -7.99620000e+00]\n",
      "  ...\n",
      "  [-1.55420000e+01  2.13469000e+01 -2.43430000e+01 ... -4.30680000e+00\n",
      "    7.17810000e+00 -1.00493000e+01]\n",
      "  [-6.68945100e+02  3.27148300e+02  1.46484000e+01 ... -2.39257700e+02\n",
      "    2.14843700e+02 -2.00195300e+02]\n",
      "  [-6.67572000e+02 -3.83377100e+02  1.39045720e+03 ... -9.47952300e+02\n",
      "    6.33239700e+02 -2.91824300e+02]]\n",
      "\n",
      " [[-1.20912420e+03 -1.20912420e+03 -1.20912420e+03 ... -1.24991340e+03\n",
      "   -1.24991340e+03 -1.24991340e+03]\n",
      "  [-1.26429330e+03 -1.26429330e+03 -1.26429330e+03 ... -1.29385680e+03\n",
      "   -1.29385680e+03 -1.29385680e+03]\n",
      "  [-7.99620000e+00 -7.99620000e+00 -7.99620000e+00 ... -7.97420000e+00\n",
      "   -7.97420000e+00 -7.97420000e+00]\n",
      "  ...\n",
      "  [ 1.27332000e+01 -1.45434000e+01  1.61038000e+01 ...  1.24836000e+01\n",
      "   -1.23587000e+01  1.24211000e+01]\n",
      "  [ 1.70898400e+02 -1.17187500e+02  8.30078000e+01 ...  6.34765000e+01\n",
      "   -5.85937000e+01  8.30078000e+01]\n",
      "  [-5.53131000e+01  4.06265300e+02 -7.43866000e+02 ... -7.24792500e+02\n",
      "    7.11441000e+02 -7.01904300e+02]]]\n",
      "21/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/046_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (336, 49, 1024), Y_shape: (336,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR02_20200423_PGS_31_BSD_21/037_2020_04_23.csv\n",
      "[[[-7.12167600e+02 -7.12167600e+02 -7.12167600e+02 ... -7.00615600e+02\n",
      "   -7.00615600e+02 -7.00615600e+02]\n",
      "  [-7.00000600e+02 -7.00000600e+02 -7.00000600e+02 ... -7.04808200e+02\n",
      "   -7.04808200e+02 -7.04808200e+02]\n",
      "  [ 4.94350000e+00  4.94350000e+00  4.94350000e+00 ...  6.15200000e-01\n",
      "    6.15200000e-01  6.15200000e-01]\n",
      "  ...\n",
      "  [ 1.87250000e+00  1.99740000e+00  2.30950000e+00 ...  4.60019000e+01\n",
      "    4.16951000e+01  3.57030000e+01]\n",
      "  [ 1.70898400e+02  1.70898400e+02  1.95312400e+02 ...  7.53417760e+03\n",
      "    7.27538860e+03  6.68456840e+03]\n",
      "  [ 2.74658200e+02  2.74658200e+02  3.14712500e+02 ...  7.26318360e+03\n",
      "    6.89888000e+03  6.21604920e+03]]\n",
      "\n",
      " [[-7.00615600e+02 -7.00615600e+02 -7.00615600e+02 ... -7.01704300e+02\n",
      "   -7.01704300e+02 -7.01704300e+02]\n",
      "  [-7.04808200e+02 -7.04808200e+02 -7.04808200e+02 ... -7.32174900e+02\n",
      "   -7.32174900e+02 -7.32174900e+02]\n",
      "  [ 6.15200000e-01  6.15200000e-01  6.15200000e-01 ... -1.11600000e+00\n",
      "   -1.11600000e+00 -1.11600000e+00]\n",
      "  ...\n",
      "  [ 2.84001000e+01  1.95992000e+01  9.98690000e+00 ...  0.00000000e+00\n",
      "   -1.00493000e+01 -1.83508000e+01]\n",
      "  [ 6.00097490e+03  4.95117050e+03  3.71093650e+03 ...  1.10595672e+04\n",
      "    7.25585730e+03  1.61621050e+03]\n",
      "  [ 5.26046750e+03  4.04930110e+03  2.68173220e+03 ...  1.34086610e+03\n",
      "   -3.12805200e+02 -1.80053710e+03]]\n",
      "\n",
      " [[-7.01704300e+02 -7.01704300e+02 -7.01704300e+02 ... -7.20130000e+02\n",
      "   -7.20130000e+02 -7.20130000e+02]\n",
      "  [-7.32174900e+02 -7.32174900e+02 -7.32174900e+02 ... -7.72520300e+02\n",
      "   -7.72520300e+02 -7.72520300e+02]\n",
      "  [-1.11600000e+00 -1.11600000e+00 -1.11600000e+00 ... -5.93760000e+00\n",
      "   -5.93760000e+00 -5.93760000e+00]\n",
      "  ...\n",
      "  [-2.33443000e+01 -2.69021000e+01 -2.60907000e+01 ... -4.56899000e+01\n",
      "   -4.16327000e+01 -2.19087000e+01]\n",
      "  [-4.09179570e+03 -8.37890390e+03 -1.06103486e+04 ...  1.25488250e+03\n",
      "    2.65624930e+03  2.23144470e+03]\n",
      "  [-3.00407410e+03 -3.69834900e+03 -4.11987300e+03 ... -5.74874880e+03\n",
      "   -6.94656370e+03 -5.85365300e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.12752490e+03 -1.12752490e+03 -1.12752490e+03 ... -1.16832410e+03\n",
      "   -1.16832410e+03 -1.16832410e+03]\n",
      "  [-1.18292030e+03 -1.18292030e+03 -1.18292030e+03 ... -1.22372030e+03\n",
      "   -1.22372030e+03 -1.22372030e+03]\n",
      "  [-7.99620000e+00 -7.99620000e+00 -7.99620000e+00 ... -7.99530000e+00\n",
      "   -7.99530000e+00 -7.99530000e+00]\n",
      "  ...\n",
      "  [ 1.29205000e+01 -2.16590000e+01  2.31570000e+01 ... -9.42510000e+00\n",
      "    3.12100000e-01  9.30030000e+00]\n",
      "  [ 1.62597610e+03 -1.33300740e+03  5.17578000e+02 ...  9.22851300e+02\n",
      "   -9.42382500e+02  8.39843500e+02]\n",
      "  [ 1.38282780e+03  1.58309900e+02 -1.63841250e+03 ...  2.64358520e+03\n",
      "   -2.20489500e+03  1.44386290e+03]]\n",
      "\n",
      " [[-1.16832410e+03 -1.16832410e+03 -1.16832410e+03 ... -1.20912440e+03\n",
      "   -1.20912440e+03 -1.20912440e+03]\n",
      "  [-1.22372030e+03 -1.22372030e+03 -1.22372030e+03 ... -1.26429330e+03\n",
      "   -1.26429330e+03 -1.26429330e+03]\n",
      "  [-7.99530000e+00 -7.99530000e+00 -7.99530000e+00 ... -7.99610000e+00\n",
      "   -7.99610000e+00 -7.99610000e+00]\n",
      "  ...\n",
      "  [-1.69777000e+01  2.21583000e+01 -2.44678000e+01 ... -5.11830000e+00\n",
      "    8.05190000e+00 -1.09231000e+01]\n",
      "  [-5.76171700e+02  2.34374900e+02  1.02539000e+02 ... -2.09960900e+02\n",
      "    1.95312400e+02 -1.75781200e+02]\n",
      "  [-4.63485700e+02 -5.81741300e+02  1.54685970e+03 ... -8.58306900e+02\n",
      "    5.58853100e+02 -2.04086300e+02]]\n",
      "\n",
      " [[-1.20912440e+03 -1.20912440e+03 -1.20912440e+03 ... -1.24991370e+03\n",
      "   -1.24991370e+03 -1.24991370e+03]\n",
      "  [-1.26429330e+03 -1.26429330e+03 -1.26429330e+03 ... -1.29385680e+03\n",
      "   -1.29385680e+03 -1.29385680e+03]\n",
      "  [-7.99610000e+00 -7.99610000e+00 -7.99610000e+00 ... -7.97390000e+00\n",
      "   -7.97390000e+00 -7.97390000e+00]\n",
      "  ...\n",
      "  [ 1.31702000e+01 -1.52300000e+01  1.62286000e+01 ...  1.21715000e+01\n",
      "   -1.24211000e+01  1.22963000e+01]\n",
      "  [ 1.26953100e+02 -7.81250000e+01  4.88281000e+01 ...  6.34765000e+01\n",
      "   -6.83594000e+01  4.39453000e+01]\n",
      "  [-1.50680500e+02  4.88281300e+02 -8.23974600e+02 ... -7.26699800e+02\n",
      "    7.15255700e+02 -7.32421900e+02]]]\n",
      "22/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/037_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (352, 49, 1024), Y_shape: (352,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR02_20200423_PGS_31_BSD_21/041_2020_04_23.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-7.12167400e+02 -7.12167400e+02 -7.12167400e+02 ... -7.00615300e+02\n",
      "   -7.00615300e+02 -7.00615300e+02]\n",
      "  [-7.00000600e+02 -7.00000600e+02 -7.00000600e+02 ... -7.04808200e+02\n",
      "   -7.04808200e+02 -7.04808200e+02]\n",
      "  [ 4.94390000e+00  4.94390000e+00  4.94390000e+00 ...  6.14700000e-01\n",
      "    6.14700000e-01  6.14700000e-01]\n",
      "  ...\n",
      "  [ 1.87250000e+00  2.05980000e+00  2.37190000e+00 ...  4.53778000e+01\n",
      "    4.13206000e+01  3.52037000e+01]\n",
      "  [ 1.70898400e+02  1.70898400e+02  2.09960900e+02 ...  7.37304480e+03\n",
      "    7.11425580e+03  6.50878720e+03]\n",
      "  [ 2.86102300e+02  2.86102300e+02  3.29971300e+02 ...  7.13920590e+03\n",
      "    6.77871700e+03  6.11877440e+03]]\n",
      "\n",
      " [[-7.00615300e+02 -7.00615300e+02 -7.00615300e+02 ... -7.01704400e+02\n",
      "   -7.01704400e+02 -7.01704400e+02]\n",
      "  [-7.04808200e+02 -7.04808200e+02 -7.04808200e+02 ... -7.32174900e+02\n",
      "   -7.32174900e+02 -7.32174900e+02]\n",
      "  [ 6.14700000e-01  6.14700000e-01  6.14700000e-01 ... -1.11590000e+00\n",
      "   -1.11590000e+00 -1.11590000e+00]\n",
      "  ...\n",
      "  [ 2.75887000e+01  1.89126000e+01  9.36270000e+00 ... -1.18590000e+00\n",
      "   -1.12352000e+01 -1.95368000e+01]\n",
      "  [ 5.77148280e+03  4.79003770e+03  3.58398340e+03 ...  1.03808565e+04\n",
      "    6.66992000e+03  1.33789030e+03]\n",
      "  [ 5.15556340e+03  3.95774840e+03  2.60925290e+03 ...  1.23596190e+03\n",
      "   -5.01632700e+02 -2.00462340e+03]]\n",
      "\n",
      " [[-7.01704400e+02 -7.01704400e+02 -7.01704400e+02 ... -7.20130200e+02\n",
      "   -7.20130200e+02 -7.20130200e+02]\n",
      "  [-7.32174900e+02 -7.32174900e+02 -7.32174900e+02 ... -7.72520300e+02\n",
      "   -7.72520300e+02 -7.72520300e+02]\n",
      "  [-1.11590000e+00 -1.11590000e+00 -1.11590000e+00 ... -5.93790000e+00\n",
      "   -5.93790000e+00 -5.93790000e+00]\n",
      "  ...\n",
      "  [-2.40933000e+01 -2.68397000e+01 -2.54041000e+01 ... -4.56274000e+01\n",
      "   -4.16327000e+01 -2.17838000e+01]\n",
      "  [-4.00878790e+03 -7.99316180e+03 -1.01806612e+04 ...  1.04003880e+03\n",
      "    2.37792900e+03  2.08496040e+03]\n",
      "  [-3.16238400e+03 -3.79943850e+03 -4.11605830e+03 ... -5.74684140e+03\n",
      "   -6.92367550e+03 -5.81741330e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.12752460e+03 -1.12752460e+03 -1.12752460e+03 ... -1.16832420e+03\n",
      "   -1.16832420e+03 -1.16832420e+03]\n",
      "  [-1.18292030e+03 -1.18292030e+03 -1.18292030e+03 ... -1.22372030e+03\n",
      "   -1.22372030e+03 -1.22372030e+03]\n",
      "  [-7.99610000e+00 -7.99610000e+00 -7.99610000e+00 ... -7.99600000e+00\n",
      "   -7.99600000e+00 -7.99600000e+00]\n",
      "  ...\n",
      "  [ 8.73850000e+00 -1.75394000e+01  2.07227000e+01 ... -1.06110000e+01\n",
      "    1.49800000e+00  8.05190000e+00]\n",
      "  [ 1.71386670e+03 -1.57714800e+03  8.44726300e+02 ...  9.66796600e+02\n",
      "   -1.03515600e+03  9.13085700e+02]\n",
      "  [ 1.46865840e+03 -1.96456900e+02 -1.16348270e+03 ...  2.70080570e+03\n",
      "   -2.30026250e+03  1.58500670e+03]]\n",
      "\n",
      " [[-1.16832420e+03 -1.16832420e+03 -1.16832420e+03 ... -1.20912430e+03\n",
      "   -1.20912430e+03 -1.20912430e+03]\n",
      "  [-1.22372030e+03 -1.22372030e+03 -1.22372030e+03 ... -1.26429330e+03\n",
      "   -1.26429330e+03 -1.26429330e+03]\n",
      "  [-7.99600000e+00 -7.99600000e+00 -7.99600000e+00 ... -7.99630000e+00\n",
      "   -7.99630000e+00 -7.99630000e+00]\n",
      "  ...\n",
      "  [-1.62286000e+01  2.15966000e+01 -2.47175000e+01 ... -5.18070000e+00\n",
      "    8.05190000e+00 -1.10480000e+01]\n",
      "  [-6.59179500e+02  3.02734300e+02  6.34765000e+01 ... -2.29492100e+02\n",
      "    1.85546800e+02 -1.75781200e+02]\n",
      "  [-6.25610400e+02 -4.36782800e+02  1.43051150e+03 ... -9.04083300e+02\n",
      "    5.70297200e+02 -2.28881800e+02]]\n",
      "\n",
      " [[-1.20912430e+03 -1.20912430e+03 -1.20912430e+03 ... -1.24991370e+03\n",
      "   -1.24991370e+03 -1.24991370e+03]\n",
      "  [-1.26429330e+03 -1.26429330e+03 -1.26429330e+03 ... -1.29385680e+03\n",
      "   -1.29385680e+03 -1.29385680e+03]\n",
      "  [-7.99630000e+00 -7.99630000e+00 -7.99630000e+00 ... -7.97400000e+00\n",
      "   -7.97400000e+00 -7.97400000e+00]\n",
      "  ...\n",
      "  [ 1.32326000e+01 -1.52924000e+01  1.66656000e+01 ...  1.26084000e+01\n",
      "   -1.27332000e+01  1.27332000e+01]\n",
      "  [ 1.26953100e+02 -9.27734000e+01  7.32422000e+01 ...  5.37109000e+01\n",
      "   -4.88281000e+01  5.85937000e+01]\n",
      "  [-1.37329100e+02  4.76837200e+02 -8.27789300e+02 ... -7.89642300e+02\n",
      "    7.68661500e+02 -7.72476200e+02]]]\n",
      "23/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/041_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (368, 49, 1024), Y_shape: (368,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR02_20200423_PGS_31_BSD_21/038_2020_04_23.csv\n",
      "[[[-7.12167600e+02 -7.12167600e+02 -7.12167600e+02 ... -7.00615500e+02\n",
      "   -7.00615500e+02 -7.00615500e+02]\n",
      "  [-7.00000600e+02 -7.00000600e+02 -7.00000600e+02 ... -7.04808200e+02\n",
      "   -7.04808200e+02 -7.04808200e+02]\n",
      "  [ 4.94380000e+00  4.94380000e+00  4.94380000e+00 ...  6.14700000e-01\n",
      "    6.14700000e-01  6.14700000e-01]\n",
      "  ...\n",
      "  [ 1.87250000e+00  2.18460000e+00  2.49670000e+00 ...  4.55650000e+01\n",
      "    4.16951000e+01  3.57030000e+01]\n",
      "  [ 1.80664000e+02  1.80664000e+02  2.00195300e+02 ...  7.47558380e+03\n",
      "    7.20702920e+03  6.63574030e+03]\n",
      "  [ 2.99453700e+02  2.99453700e+02  3.39508100e+02 ...  7.22885130e+03\n",
      "    6.86264040e+03  6.17408750e+03]]\n",
      "\n",
      " [[-7.00615500e+02 -7.00615500e+02 -7.00615500e+02 ... -7.01704200e+02\n",
      "   -7.01704200e+02 -7.01704200e+02]\n",
      "  [-7.04808200e+02 -7.04808200e+02 -7.04808200e+02 ... -7.32174900e+02\n",
      "   -7.32174900e+02 -7.32174900e+02]\n",
      "  [ 6.14700000e-01  6.14700000e-01  6.14700000e-01 ... -1.11600000e+00\n",
      "   -1.11600000e+00 -1.11600000e+00]\n",
      "  ...\n",
      "  [ 2.79008000e+01  1.91623000e+01  9.67480000e+00 ... -6.24200000e-01\n",
      "   -1.06735000e+01 -1.92247000e+01]\n",
      "  [ 5.91796710e+03  4.88769390e+03  3.68652240e+03 ...  1.06933564e+04\n",
      "    7.02148240e+03  1.58203080e+03]\n",
      "  [ 5.22613530e+03  4.01878360e+03  2.65121460e+03 ...  1.28936770e+03\n",
      "   -4.19616700e+02 -1.91116330e+03]]\n",
      "\n",
      " [[-7.01704200e+02 -7.01704200e+02 -7.01704200e+02 ... -7.20129900e+02\n",
      "   -7.20129900e+02 -7.20129900e+02]\n",
      "  [-7.32174900e+02 -7.32174900e+02 -7.32174900e+02 ... -7.72520300e+02\n",
      "   -7.72520300e+02 -7.72520300e+02]\n",
      "  [-1.11600000e+00 -1.11600000e+00 -1.11600000e+00 ... -5.93760000e+00\n",
      "   -5.93760000e+00 -5.93760000e+00]\n",
      "  ...\n",
      "  [-2.36564000e+01 -2.69021000e+01 -2.58410000e+01 ... -4.57523000e+01\n",
      "   -4.16327000e+01 -2.20335000e+01]\n",
      "  [-3.95995980e+03 -8.05175560e+03 -1.02343721e+04 ...  1.25488250e+03\n",
      "    2.63671800e+03  2.23144470e+03]\n",
      "  [-3.10134890e+03 -3.75747680e+03 -4.12559510e+03 ... -5.73539730e+03\n",
      "   -6.95228580e+03 -5.85556030e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.12752460e+03 -1.12752460e+03 -1.12752460e+03 ... -1.16832450e+03\n",
      "   -1.16832450e+03 -1.16832450e+03]\n",
      "  [-1.18292030e+03 -1.18292030e+03 -1.18292030e+03 ... -1.22372030e+03\n",
      "   -1.22372030e+03 -1.22372030e+03]\n",
      "  [-7.99600000e+00 -7.99600000e+00 -7.99600000e+00 ... -7.99650000e+00\n",
      "   -7.99650000e+00 -7.99650000e+00]\n",
      "  ...\n",
      "  [ 9.92440000e+00 -1.85381000e+01  2.12221000e+01 ... -1.01741000e+01\n",
      "    1.06110000e+00  8.30160000e+00]\n",
      "  [ 1.77246040e+03 -1.55761680e+03  7.76367000e+02 ...  9.32616900e+02\n",
      "   -9.71679400e+02  8.78906000e+02]\n",
      "  [ 1.42097470e+03 -8.20160000e+01 -1.28364560e+03 ...  2.64930730e+03\n",
      "   -2.24113460e+03  1.51634220e+03]]\n",
      "\n",
      " [[-1.16832450e+03 -1.16832450e+03 -1.16832450e+03 ... -1.20912420e+03\n",
      "   -1.20912420e+03 -1.20912420e+03]\n",
      "  [-1.22372030e+03 -1.22372030e+03 -1.22372030e+03 ... -1.26429330e+03\n",
      "   -1.26429330e+03 -1.26429330e+03]\n",
      "  [-7.99650000e+00 -7.99650000e+00 -7.99650000e+00 ... -7.99580000e+00\n",
      "   -7.99580000e+00 -7.99580000e+00]\n",
      "  ...\n",
      "  [-1.61038000e+01  2.14717000e+01 -2.44054000e+01 ... -4.68130000e+00\n",
      "    7.49010000e+00 -1.06110000e+01]\n",
      "  [-6.44531100e+02  2.83203000e+02  5.85937000e+01 ... -2.29492100e+02\n",
      "    2.05078100e+02 -1.90429600e+02]\n",
      "  [-5.66482500e+02 -4.76837200e+02  1.44767760e+03 ... -9.19342000e+02\n",
      "    6.00814800e+02 -2.55584700e+02]]\n",
      "\n",
      " [[-1.20912420e+03 -1.20912420e+03 -1.20912420e+03 ... -1.24991400e+03\n",
      "   -1.24991400e+03 -1.24991400e+03]\n",
      "  [-1.26429330e+03 -1.26429330e+03 -1.26429330e+03 ... -1.29385680e+03\n",
      "   -1.29385680e+03 -1.29385680e+03]\n",
      "  [-7.99580000e+00 -7.99580000e+00 -7.99580000e+00 ... -7.97420000e+00\n",
      "   -7.97420000e+00 -7.97420000e+00]\n",
      "  ...\n",
      "  [ 1.24836000e+01 -1.49179000e+01  1.62286000e+01 ...  1.22339000e+01\n",
      "   -1.24211000e+01  1.22963000e+01]\n",
      "  [ 1.51367100e+02 -1.02539000e+02  7.81250000e+01 ...  6.83594000e+01\n",
      "   -6.34765000e+01  5.85937000e+01]\n",
      "  [-8.77380000e+01  4.38690200e+02 -7.74383500e+02 ... -7.24792500e+02\n",
      "    7.11441000e+02 -7.05719000e+02]]]\n",
      "24/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/038_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (384, 49, 1024), Y_shape: (384,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR02_20200423_PGS_31_BSD_21/039_2020_04_23.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-7.12167900e+02 -7.12167900e+02 -7.12167900e+02 ... -7.00615300e+02\n",
      "   -7.00615300e+02 -7.00615300e+02]\n",
      "  [-7.00000600e+02 -7.00000600e+02 -7.00000600e+02 ... -7.04808200e+02\n",
      "   -7.04808200e+02 -7.04808200e+02]\n",
      "  [ 4.94360000e+00  4.94360000e+00  4.94360000e+00 ...  6.15100000e-01\n",
      "    6.15100000e-01  6.15100000e-01]\n",
      "  ...\n",
      "  [ 2.24700000e+00  2.37190000e+00  2.74640000e+00 ...  4.52529000e+01\n",
      "    4.10709000e+01  3.52037000e+01]\n",
      "  [ 2.05078100e+02  2.05078100e+02  2.49023400e+02 ...  7.38281040e+03\n",
      "    7.14355270e+03  6.57226380e+03]\n",
      "  [ 3.45230100e+02  3.45230100e+02  3.92913800e+02 ...  7.17926030e+03\n",
      "    6.78825380e+03  6.12449650e+03]]\n",
      "\n",
      " [[-7.00615300e+02 -7.00615300e+02 -7.00615300e+02 ... -7.01704100e+02\n",
      "   -7.01704100e+02 -7.01704100e+02]\n",
      "  [-7.04808200e+02 -7.04808200e+02 -7.04808200e+02 ... -7.32174900e+02\n",
      "   -7.32174900e+02 -7.32174900e+02]\n",
      "  [ 6.15100000e-01  6.15100000e-01  6.15100000e-01 ... -1.11660000e+00\n",
      "   -1.11660000e+00 -1.11660000e+00]\n",
      "  ...\n",
      "  [ 2.79632000e+01  1.90999000e+01  9.54990000e+00 ... -3.12100000e-01\n",
      "   -1.08607000e+01 -1.97865000e+01]\n",
      "  [ 5.84960770e+03  4.86816270e+03  3.62792870e+03 ...  1.01611300e+04\n",
      "    6.23046700e+03  8.93554400e+02]\n",
      "  [ 5.17654420e+03  3.98063660e+03  2.61688230e+03 ...  1.33323670e+03\n",
      "   -3.75747700e+02 -1.94740300e+03]]\n",
      "\n",
      " [[-7.01704100e+02 -7.01704100e+02 -7.01704100e+02 ... -7.20130300e+02\n",
      "   -7.20130300e+02 -7.20130300e+02]\n",
      "  [-7.32174900e+02 -7.32174900e+02 -7.32174900e+02 ... -7.72520300e+02\n",
      "   -7.72520300e+02 -7.72520300e+02]\n",
      "  [-1.11660000e+00 -1.11660000e+00 -1.11660000e+00 ... -5.93790000e+00\n",
      "   -5.93790000e+00 -5.93790000e+00]\n",
      "  ...\n",
      "  [-2.45926000e+01 -2.77135000e+01 -2.59034000e+01 ... -4.53778000e+01\n",
      "   -4.08837000e+01 -2.14093000e+01]\n",
      "  [-4.23339730e+03 -8.03710710e+03 -1.00878878e+04 ...  1.31835900e+03\n",
      "    2.53906180e+03  2.03124940e+03]\n",
      "  [-3.18908690e+03 -3.84902950e+03 -4.18281560e+03 ... -5.78308110e+03\n",
      "   -6.88552860e+03 -5.74684140e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.12752450e+03 -1.12752450e+03 -1.12752450e+03 ... -1.16832390e+03\n",
      "   -1.16832390e+03 -1.16832390e+03]\n",
      "  [-1.18292030e+03 -1.18292030e+03 -1.18292030e+03 ... -1.22372030e+03\n",
      "   -1.22372030e+03 -1.22372030e+03]\n",
      "  [-7.99590000e+00 -7.99590000e+00 -7.99590000e+00 ... -7.99560000e+00\n",
      "   -7.99560000e+00 -7.99560000e+00]\n",
      "  ...\n",
      "  [ 7.98950000e+00 -1.61038000e+01  1.96616000e+01 ... -1.06110000e+01\n",
      "    1.56040000e+00  7.98950000e+00]\n",
      "  [ 1.70898390e+03 -1.53320270e+03  8.30077900e+02 ...  9.86327800e+02\n",
      "   -1.05468720e+03  9.32616900e+02]\n",
      "  [ 1.39045720e+03 -2.15530400e+02 -1.06620790e+03 ...  2.71415710e+03\n",
      "   -2.31933590e+03  1.58691410e+03]]\n",
      "\n",
      " [[-1.16832390e+03 -1.16832390e+03 -1.16832390e+03 ... -1.20912470e+03\n",
      "   -1.20912470e+03 -1.20912470e+03]\n",
      "  [-1.22372030e+03 -1.22372030e+03 -1.22372030e+03 ... -1.26429330e+03\n",
      "   -1.26429330e+03 -1.26429330e+03]\n",
      "  [-7.99560000e+00 -7.99560000e+00 -7.99560000e+00 ... -7.99630000e+00\n",
      "   -7.99630000e+00 -7.99630000e+00]\n",
      "  ...\n",
      "  [-1.64159000e+01  2.20335000e+01 -2.50296000e+01 ... -5.30550000e+00\n",
      "    8.30160000e+00 -1.12352000e+01]\n",
      "  [-6.54296700e+02  2.92968700e+02  5.37109000e+01 ... -2.14843700e+02\n",
      "    1.90429600e+02 -1.75781200e+02]\n",
      "  [-6.35147100e+02 -4.61578400e+02  1.46484380e+03 ... -8.92639200e+02\n",
      "    5.56945800e+02 -2.11715700e+02]]\n",
      "\n",
      " [[-1.20912470e+03 -1.20912470e+03 -1.20912470e+03 ... -1.24991330e+03\n",
      "   -1.24991330e+03 -1.24991330e+03]\n",
      "  [-1.26429330e+03 -1.26429330e+03 -1.26429330e+03 ... -1.29385680e+03\n",
      "   -1.29385680e+03 -1.29385680e+03]\n",
      "  [-7.99630000e+00 -7.99630000e+00 -7.99630000e+00 ... -7.97390000e+00\n",
      "   -7.97390000e+00 -7.97390000e+00]\n",
      "  ...\n",
      "  [ 1.34823000e+01 -1.54172000e+01  1.69777000e+01 ...  1.27332000e+01\n",
      "   -1.26084000e+01  1.26708000e+01]\n",
      "  [ 1.26953100e+02 -8.30078000e+01  2.92969000e+01 ...  6.83594000e+01\n",
      "   -4.88281000e+01  5.85937000e+01]\n",
      "  [-1.62124600e+02  5.05447400e+02 -8.60214200e+02 ... -7.78198200e+02\n",
      "    7.64846800e+02 -7.66754200e+02]]]\n",
      "25/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/039_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (400, 49, 1024), Y_shape: (400,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR02_20200423_PGS_31_BSD_21/040_2020_04_23.csv\n",
      "[[[-7.12167800e+02 -7.12167800e+02 -7.12167800e+02 ... -7.00615300e+02\n",
      "   -7.00615300e+02 -7.00615300e+02]\n",
      "  [-7.00000600e+02 -7.00000600e+02 -7.00000600e+02 ... -7.04808200e+02\n",
      "   -7.04808200e+02 -7.04808200e+02]\n",
      "  [ 4.94360000e+00  4.94360000e+00  4.94360000e+00 ...  6.15000000e-01\n",
      "    6.15000000e-01  6.15000000e-01]\n",
      "  ...\n",
      "  [ 1.99740000e+00  2.30950000e+00  2.55910000e+00 ...  4.53778000e+01\n",
      "    4.13830000e+01  3.52661000e+01]\n",
      "  [ 1.85546800e+02  1.85546800e+02  2.14843700e+02 ...  7.39745890e+03\n",
      "    7.14355270e+03  6.55273250e+03]\n",
      "  [ 3.07083100e+02  3.07083100e+02  3.54766800e+02 ...  7.16209410e+03\n",
      "    6.79779050e+03  6.13403320e+03]]\n",
      "\n",
      " [[-7.00615300e+02 -7.00615300e+02 -7.00615300e+02 ... -7.01704400e+02\n",
      "   -7.01704400e+02 -7.01704400e+02]\n",
      "  [-7.04808200e+02 -7.04808200e+02 -7.04808200e+02 ... -7.32174900e+02\n",
      "   -7.32174900e+02 -7.32174900e+02]\n",
      "  [ 6.15000000e-01  6.15000000e-01  6.15000000e-01 ... -1.11600000e+00\n",
      "   -1.11600000e+00 -1.11600000e+00]\n",
      "  ...\n",
      "  [ 2.76511000e+01  1.89750000e+01  9.48750000e+00 ... -9.98700000e-01\n",
      "   -1.11104000e+01 -1.94744000e+01]\n",
      "  [ 5.82519370e+03  4.83398300e+03  3.61328020e+03 ...  1.04589815e+04\n",
      "    6.79199030e+03  1.41113240e+03]\n",
      "  [ 5.17463680e+03  3.97491460e+03  2.63023380e+03 ...  1.23977660e+03\n",
      "   -4.74929800e+02 -1.97982790e+03]]\n",
      "\n",
      " [[-7.01704400e+02 -7.01704400e+02 -7.01704400e+02 ... -7.20130100e+02\n",
      "   -7.20130100e+02 -7.20130100e+02]\n",
      "  [-7.32174900e+02 -7.32174900e+02 -7.32174900e+02 ... -7.72520300e+02\n",
      "   -7.72520300e+02 -7.72520300e+02]\n",
      "  [-1.11600000e+00 -1.11600000e+00 -1.11600000e+00 ... -5.93780000e+00\n",
      "   -5.93780000e+00 -5.93780000e+00]\n",
      "  ...\n",
      "  [-2.40309000e+01 -2.66524000e+01 -2.53416000e+01 ... -4.53778000e+01\n",
      "   -4.13830000e+01 -2.19087000e+01]\n",
      "  [-3.96484260e+03 -7.98827900e+03 -1.01123019e+04 ...  1.08398410e+03\n",
      "    2.50488210e+03  2.16308530e+03]\n",
      "  [-3.14521790e+03 -3.77273560e+03 -4.09507750e+03 ... -5.72204590e+03\n",
      "   -6.91604610e+03 -5.81741330e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.12752420e+03 -1.12752420e+03 -1.12752420e+03 ... -1.16832380e+03\n",
      "   -1.16832380e+03 -1.16832380e+03]\n",
      "  [-1.18292030e+03 -1.18292030e+03 -1.18292030e+03 ... -1.22372030e+03\n",
      "   -1.22372030e+03 -1.22372030e+03]\n",
      "  [-7.99630000e+00 -7.99630000e+00 -7.99630000e+00 ... -7.99600000e+00\n",
      "   -7.99600000e+00 -7.99600000e+00]\n",
      "  ...\n",
      "  [ 8.98820000e+00 -1.76642000e+01  2.09724000e+01 ... -1.03614000e+01\n",
      "    1.12350000e+00  8.67610000e+00]\n",
      "  [ 1.69433550e+03 -1.53320270e+03  8.10546600e+02 ...  9.86327800e+02\n",
      "   -1.02050750e+03  9.08202900e+02]\n",
      "  [ 1.48200990e+03 -1.83105500e+02 -1.17683410e+03 ...  2.73323060e+03\n",
      "   -2.32315060e+03  1.56974790e+03]]\n",
      "\n",
      " [[-1.16832380e+03 -1.16832380e+03 -1.16832380e+03 ... -1.20912440e+03\n",
      "   -1.20912440e+03 -1.20912440e+03]\n",
      "  [-1.22372030e+03 -1.22372030e+03 -1.22372030e+03 ... -1.26429330e+03\n",
      "   -1.26429330e+03 -1.26429330e+03]\n",
      "  [-7.99600000e+00 -7.99600000e+00 -7.99600000e+00 ... -7.99660000e+00\n",
      "   -7.99660000e+00 -7.99660000e+00]\n",
      "  ...\n",
      "  [-1.67280000e+01  2.24704000e+01 -2.50920000e+01 ... -5.61760000e+00\n",
      "    8.67610000e+00 -1.15473000e+01]\n",
      "  [-6.24999800e+02  2.73437400e+02  1.07421800e+02 ... -2.24609300e+02\n",
      "    1.85546800e+02 -1.56250000e+02]\n",
      "  [-5.81741300e+02 -4.90188600e+02  1.52397160e+03 ... -8.64028900e+02\n",
      "    5.32150300e+02 -1.75476100e+02]]\n",
      "\n",
      " [[-1.20912440e+03 -1.20912440e+03 -1.20912440e+03 ... -1.24991330e+03\n",
      "   -1.24991330e+03 -1.24991330e+03]\n",
      "  [-1.26429330e+03 -1.26429330e+03 -1.26429330e+03 ... -1.29385680e+03\n",
      "   -1.29385680e+03 -1.29385680e+03]\n",
      "  [-7.99660000e+00 -7.99660000e+00 -7.99660000e+00 ... -7.97380000e+00\n",
      "   -7.97380000e+00 -7.97380000e+00]\n",
      "  ...\n",
      "  [ 1.37943000e+01 -1.55420000e+01  1.69152000e+01 ...  1.27332000e+01\n",
      "   -1.27332000e+01  1.27957000e+01]\n",
      "  [ 1.31835900e+02 -6.83594000e+01  2.44141000e+01 ...  5.85937000e+01\n",
      "   -3.41797000e+01  4.88281000e+01]\n",
      "  [-1.75476100e+02  5.37872300e+02 -8.71658300e+02 ... -8.04901100e+02\n",
      "    7.93457000e+02 -7.95364400e+02]]]\n",
      "26/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/040_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (416, 49, 1024), Y_shape: (416,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR02_20200423_PGS_31_BSD_21/045_2020_04_23.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-7.12168100e+02 -7.12168100e+02 -7.12168100e+02 ... -7.00615300e+02\n",
      "   -7.00615300e+02 -7.00615300e+02]\n",
      "  [-7.00000600e+02 -7.00000600e+02 -7.00000600e+02 ... -7.04808200e+02\n",
      "   -7.04808200e+02 -7.04808200e+02]\n",
      "  [ 4.94340000e+00  4.94340000e+00  4.94340000e+00 ...  6.15100000e-01\n",
      "    6.15100000e-01  6.15100000e-01]\n",
      "  ...\n",
      "  [ 1.93500000e+00  2.18460000e+00  2.55910000e+00 ...  4.46912000e+01\n",
      "    4.05092000e+01  3.46419000e+01]\n",
      "  [ 1.90429600e+02  1.90429600e+02  2.09960900e+02 ...  7.25097450e+03\n",
      "    6.99218550e+03  6.40136540e+03]\n",
      "  [ 2.99453700e+02  2.99453700e+02  3.49044800e+02 ...  7.08580020e+03\n",
      "    6.71958920e+03  6.05392460e+03]]\n",
      "\n",
      " [[-7.00615300e+02 -7.00615300e+02 -7.00615300e+02 ... -7.01704300e+02\n",
      "   -7.01704300e+02 -7.01704300e+02]\n",
      "  [-7.04808200e+02 -7.04808200e+02 -7.04808200e+02 ... -7.32174900e+02\n",
      "   -7.32174900e+02 -7.32174900e+02]\n",
      "  [ 6.15100000e-01  6.15100000e-01  6.15100000e-01 ... -1.11600000e+00\n",
      "   -1.11600000e+00 -1.11600000e+00]\n",
      "  ...\n",
      "  [ 2.71518000e+01  1.85381000e+01  9.05060000e+00 ... -1.37320000e+00\n",
      "   -1.15473000e+01 -1.97240000e+01]\n",
      "  [ 5.64452970e+03  4.71679560e+03  3.55468650e+03 ...  1.03857393e+04\n",
      "    6.74804500e+03  1.34277310e+03]\n",
      "  [ 5.10978700e+03  3.89862060e+03  2.53105160e+03 ...  1.21307370e+03\n",
      "   -5.28335600e+02 -2.04277040e+03]]\n",
      "\n",
      " [[-7.01704300e+02 -7.01704300e+02 -7.01704300e+02 ... -7.20130100e+02\n",
      "   -7.20130100e+02 -7.20130100e+02]\n",
      "  [-7.32174900e+02 -7.32174900e+02 -7.32174900e+02 ... -7.72520300e+02\n",
      "   -7.72520300e+02 -7.72520300e+02]\n",
      "  [-1.11600000e+00 -1.11600000e+00 -1.11600000e+00 ... -5.93770000e+00\n",
      "   -5.93770000e+00 -5.93770000e+00]\n",
      "  ...\n",
      "  [-2.44054000e+01 -2.70893000e+01 -2.56537000e+01 ... -4.58147000e+01\n",
      "   -4.11958000e+01 -2.19711000e+01]\n",
      "  [-4.06249890e+03 -8.11034930e+03 -1.02001925e+04 ...  1.03027310e+03\n",
      "    2.36816340e+03  2.06542910e+03]\n",
      "  [-3.19099430e+03 -3.81469730e+03 -4.11033630e+03 ... -5.72776790e+03\n",
      "   -6.91032410e+03 -5.80787660e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.12752480e+03 -1.12752480e+03 -1.12752480e+03 ... -1.16832370e+03\n",
      "   -1.16832370e+03 -1.16832370e+03]\n",
      "  [-1.18292030e+03 -1.18292030e+03 -1.18292030e+03 ... -1.22372030e+03\n",
      "   -1.22372030e+03 -1.22372030e+03]\n",
      "  [-7.99620000e+00 -7.99620000e+00 -7.99620000e+00 ... -7.99560000e+00\n",
      "   -7.99560000e+00 -7.99560000e+00]\n",
      "  ...\n",
      "  [ 8.30160000e+00 -1.66031000e+01  2.00985000e+01 ... -1.06110000e+01\n",
      "    1.49800000e+00  8.17670000e+00]\n",
      "  [ 1.67968700e+03 -1.56249960e+03  9.03320100e+02 ...  9.66796600e+02\n",
      "   -1.00585910e+03  9.08202900e+02]\n",
      "  [ 1.44386290e+03 -2.02179000e+02 -1.09672550e+03 ...  2.71415710e+03\n",
      "   -2.31552120e+03  1.59835820e+03]]\n",
      "\n",
      " [[-1.16832370e+03 -1.16832370e+03 -1.16832370e+03 ... -1.20912480e+03\n",
      "   -1.20912480e+03 -1.20912480e+03]\n",
      "  [-1.22372030e+03 -1.22372030e+03 -1.22372030e+03 ... -1.26429330e+03\n",
      "   -1.26429330e+03 -1.26429330e+03]\n",
      "  [-7.99560000e+00 -7.99560000e+00 -7.99560000e+00 ... -7.99630000e+00\n",
      "   -7.99630000e+00 -7.99630000e+00]\n",
      "  ...\n",
      "  [-1.61662000e+01  2.18462000e+01 -2.49047000e+01 ... -5.11830000e+00\n",
      "    8.11430000e+00 -1.09231000e+01]\n",
      "  [-6.39648300e+02  3.02734300e+02  6.34765000e+01 ... -2.24609300e+02\n",
      "    1.75781200e+02 -1.66015600e+02]\n",
      "  [-6.37054400e+02 -4.25338700e+02  1.44767760e+03 ... -9.00268600e+02\n",
      "    5.76019300e+02 -2.11715700e+02]]\n",
      "\n",
      " [[-1.20912480e+03 -1.20912480e+03 -1.20912480e+03 ... -1.24991350e+03\n",
      "   -1.24991350e+03 -1.24991350e+03]\n",
      "  [-1.26429330e+03 -1.26429330e+03 -1.26429330e+03 ... -1.29385680e+03\n",
      "   -1.29385680e+03 -1.29385680e+03]\n",
      "  [-7.99630000e+00 -7.99630000e+00 -7.99630000e+00 ... -7.97390000e+00\n",
      "   -7.97390000e+00 -7.97390000e+00]\n",
      "  ...\n",
      "  [ 1.34198000e+01 -1.51051000e+01  1.68528000e+01 ...  1.25460000e+01\n",
      "   -1.25460000e+01  1.26708000e+01]\n",
      "  [ 1.41601500e+02 -6.34765000e+01  5.85937000e+01 ...  3.90625000e+01\n",
      "   -4.88281000e+01  6.34765000e+01]\n",
      "  [-1.35421800e+02  4.99725300e+02 -8.37326000e+02 ... -7.64846800e+02\n",
      "    7.57217400e+02 -7.53402700e+02]]]\n",
      "27/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/045_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (432, 49, 1024), Y_shape: (432,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR02_20200423_PGS_31_BSD_21/042_2020_04_23.csv\n",
      "[[[-7.12167900e+02 -7.12167900e+02 -7.12167900e+02 ... -7.00615600e+02\n",
      "   -7.00615600e+02 -7.00615600e+02]\n",
      "  [-7.00000600e+02 -7.00000600e+02 -7.00000600e+02 ... -7.04808200e+02\n",
      "   -7.04808200e+02 -7.04808200e+02]\n",
      "  [ 4.94370000e+00  4.94370000e+00  4.94370000e+00 ...  6.15000000e-01\n",
      "    6.15000000e-01  6.15000000e-01]\n",
      "  ...\n",
      "  [ 1.81010000e+00  2.18460000e+00  2.49670000e+00 ...  4.50033000e+01\n",
      "    4.10085000e+01  3.49540000e+01]\n",
      "  [ 1.80664000e+02  1.80664000e+02  2.00195300e+02 ...  7.31445110e+03\n",
      "    7.07031050e+03  6.46972480e+03]\n",
      "  [ 2.86102300e+02  2.86102300e+02  3.39508100e+02 ...  7.12966920e+03\n",
      "    6.76155090e+03  6.10542300e+03]]\n",
      "\n",
      " [[-7.00615600e+02 -7.00615600e+02 -7.00615600e+02 ... -7.01704100e+02\n",
      "   -7.01704100e+02 -7.01704100e+02]\n",
      "  [-7.04808200e+02 -7.04808200e+02 -7.04808200e+02 ... -7.32174900e+02\n",
      "   -7.32174900e+02 -7.32174900e+02]\n",
      "  [ 6.15000000e-01  6.15000000e-01  6.15000000e-01 ... -1.11620000e+00\n",
      "   -1.11620000e+00 -1.11620000e+00]\n",
      "  ...\n",
      "  [ 2.77135000e+01  1.89126000e+01  9.23780000e+00 ... -1.49800000e+00\n",
      "   -1.16097000e+01 -1.95992000e+01]\n",
      "  [ 5.71288900e+03  4.75585800e+03  3.58398340e+03 ...  1.04785127e+04\n",
      "    6.77734190e+03  1.33789030e+03]\n",
      "  [ 5.14984130e+03  3.94439700e+03  2.58636470e+03 ...  1.18827820e+03\n",
      "   -5.53131100e+02 -2.02178960e+03]]\n",
      "\n",
      " [[-7.01704100e+02 -7.01704100e+02 -7.01704100e+02 ... -7.20130100e+02\n",
      "   -7.20130100e+02 -7.20130100e+02]\n",
      "  [-7.32174900e+02 -7.32174900e+02 -7.32174900e+02 ... -7.72520300e+02\n",
      "   -7.72520300e+02 -7.72520300e+02]\n",
      "  [-1.11620000e+00 -1.11620000e+00 -1.11620000e+00 ... -5.93780000e+00\n",
      "   -5.93780000e+00 -5.93780000e+00]\n",
      "  ...\n",
      "  [-2.43430000e+01 -2.71518000e+01 -2.56537000e+01 ... -4.55650000e+01\n",
      "   -4.10709000e+01 -2.16590000e+01]\n",
      "  [-4.06738170e+03 -8.07616960e+03 -1.01415987e+04 ...  1.10839810e+03\n",
      "    2.45117120e+03  2.10449160e+03]\n",
      "  [-3.17001340e+03 -3.78799440e+03 -4.10842900e+03 ... -5.73730470e+03\n",
      "   -6.91413880e+03 -5.79452510e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.12752440e+03 -1.12752440e+03 -1.12752440e+03 ... -1.16832470e+03\n",
      "   -1.16832470e+03 -1.16832470e+03]\n",
      "  [-1.18292030e+03 -1.18292030e+03 -1.18292030e+03 ... -1.22372030e+03\n",
      "   -1.22372030e+03 -1.22372030e+03]\n",
      "  [-7.99600000e+00 -7.99600000e+00 -7.99600000e+00 ... -7.99620000e+00\n",
      "   -7.99620000e+00 -7.99620000e+00]\n",
      "  ...\n",
      "  [ 8.30160000e+00 -1.66031000e+01  2.00361000e+01 ... -1.06110000e+01\n",
      "    1.62290000e+00  8.11430000e+00]\n",
      "  [ 1.70898390e+03 -1.60156210e+03  9.03320100e+02 ...  1.00097630e+03\n",
      "   -1.04980440e+03  9.27734100e+02]\n",
      "  [ 1.44195560e+03 -2.07901000e+02 -1.09481810e+03 ...  2.72178650e+03\n",
      "   -2.32505800e+03  1.60026550e+03]]\n",
      "\n",
      " [[-1.16832470e+03 -1.16832470e+03 -1.16832470e+03 ... -1.20912380e+03\n",
      "   -1.20912380e+03 -1.20912380e+03]\n",
      "  [-1.22372030e+03 -1.22372030e+03 -1.22372030e+03 ... -1.26429330e+03\n",
      "   -1.26429330e+03 -1.26429330e+03]\n",
      "  [-7.99620000e+00 -7.99620000e+00 -7.99620000e+00 ... -7.99580000e+00\n",
      "   -7.99580000e+00 -7.99580000e+00]\n",
      "  ...\n",
      "  [-1.63535000e+01  2.22832000e+01 -2.52168000e+01 ... -5.11830000e+00\n",
      "    7.98950000e+00 -1.07983000e+01]\n",
      "  [-6.59179500e+02  2.92968700e+02  7.32422000e+01 ... -2.29492100e+02\n",
      "    2.05078100e+02 -1.75781200e+02]\n",
      "  [-6.27517700e+02 -4.55856300e+02  1.48010250e+03 ... -8.98361200e+02\n",
      "    5.76019300e+02 -2.28881800e+02]]\n",
      "\n",
      " [[-1.20912380e+03 -1.20912380e+03 -1.20912380e+03 ... -1.24991400e+03\n",
      "   -1.24991400e+03 -1.24991400e+03]\n",
      "  [-1.26429330e+03 -1.26429330e+03 -1.26429330e+03 ... -1.29385680e+03\n",
      "   -1.29385680e+03 -1.29385680e+03]\n",
      "  [-7.99580000e+00 -7.99580000e+00 -7.99580000e+00 ... -7.97420000e+00\n",
      "   -7.97420000e+00 -7.97420000e+00]\n",
      "  ...\n",
      "  [ 1.31702000e+01 -1.51051000e+01  1.68528000e+01 ...  1.25460000e+01\n",
      "   -1.23587000e+01  1.24836000e+01]\n",
      "  [ 1.36718700e+02 -8.30078000e+01  4.88281000e+01 ...  5.37109000e+01\n",
      "   -4.88281000e+01  6.34765000e+01]\n",
      "  [-1.25885000e+02  4.74929800e+02 -8.23974600e+02 ... -7.51495400e+02\n",
      "    7.41958600e+02 -7.24792500e+02]]]\n",
      "28/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/042_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (448, 49, 1024), Y_shape: (448,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR02_20200423_PGS_31_BSD_21/043_2020_04_23.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-7.12167900e+02 -7.12167900e+02 -7.12167900e+02 ... -7.00615600e+02\n",
      "   -7.00615600e+02 -7.00615600e+02]\n",
      "  [-7.00000600e+02 -7.00000600e+02 -7.00000600e+02 ... -7.04808200e+02\n",
      "   -7.04808200e+02 -7.04808200e+02]\n",
      "  [ 4.94380000e+00  4.94380000e+00  4.94380000e+00 ...  6.14900000e-01\n",
      "    6.14900000e-01  6.14900000e-01]\n",
      "  ...\n",
      "  [ 1.81010000e+00  2.12220000e+00  2.43430000e+00 ...  4.46288000e+01\n",
      "    4.08213000e+01  3.50788000e+01]\n",
      "  [ 1.61132800e+02  1.61132800e+02  2.00195300e+02 ...  7.31445110e+03\n",
      "    7.06054490e+03  6.44042790e+03]\n",
      "  [ 2.88009600e+02  2.88009600e+02  3.29971300e+02 ...  7.08580020e+03\n",
      "    6.71768190e+03  6.06727600e+03]]\n",
      "\n",
      " [[-7.00615600e+02 -7.00615600e+02 -7.00615600e+02 ... -7.01704000e+02\n",
      "   -7.01704000e+02 -7.01704000e+02]\n",
      "  [-7.04808200e+02 -7.04808200e+02 -7.04808200e+02 ... -7.32174900e+02\n",
      "   -7.32174900e+02 -7.32174900e+02]\n",
      "  [ 6.14900000e-01  6.14900000e-01  6.14900000e-01 ... -1.11600000e+00\n",
      "   -1.11600000e+00 -1.11600000e+00]\n",
      "  ...\n",
      "  [ 2.70893000e+01  1.87878000e+01  9.11300000e+00 ... -1.49800000e+00\n",
      "   -1.14849000e+01 -2.01610000e+01]\n",
      "  [ 5.70312340e+03  4.72656120e+03  3.52538960e+03 ...  1.03320284e+04\n",
      "    6.51367010e+03  1.09863250e+03]\n",
      "  [ 5.11741640e+03  3.91769410e+03  2.58445740e+03 ...  1.16920470e+03\n",
      "   -5.41687000e+02 -2.04658510e+03]]\n",
      "\n",
      " [[-7.01704000e+02 -7.01704000e+02 -7.01704000e+02 ... -7.20130200e+02\n",
      "   -7.20130200e+02 -7.20130200e+02]\n",
      "  [-7.32174900e+02 -7.32174900e+02 -7.32174900e+02 ... -7.72520300e+02\n",
      "   -7.72520300e+02 -7.72520300e+02]\n",
      "  [-1.11600000e+00 -1.11600000e+00 -1.11600000e+00 ... -5.93780000e+00\n",
      "   -5.93780000e+00 -5.93780000e+00]\n",
      "  ...\n",
      "  [-2.44054000e+01 -2.71518000e+01 -2.53416000e+01 ... -4.54402000e+01\n",
      "   -4.08213000e+01 -2.14093000e+01]\n",
      "  [-4.20898320e+03 -8.12011490e+03 -1.01367159e+04 ...  1.05957000e+03\n",
      "    2.35351500e+03  2.03124940e+03]\n",
      "  [-3.22341920e+03 -3.82804870e+03 -4.13322450e+03 ... -5.75637820e+03\n",
      "   -6.89888000e+03 -5.74874880e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.12752420e+03 -1.12752420e+03 -1.12752420e+03 ... -1.16832400e+03\n",
      "   -1.16832400e+03 -1.16832400e+03]\n",
      "  [-1.18292030e+03 -1.18292030e+03 -1.18292030e+03 ... -1.22372030e+03\n",
      "   -1.22372030e+03 -1.22372030e+03]\n",
      "  [-7.99630000e+00 -7.99630000e+00 -7.99630000e+00 ... -7.99640000e+00\n",
      "   -7.99640000e+00 -7.99640000e+00]\n",
      "  ...\n",
      "  [ 7.98950000e+00 -1.62911000e+01  1.95992000e+01 ... -1.07983000e+01\n",
      "    1.62290000e+00  8.30160000e+00]\n",
      "  [ 1.64550740e+03 -1.54296830e+03  8.78906000e+02 ...  1.01562470e+03\n",
      "   -1.03027310e+03  9.42382500e+02]\n",
      "  [ 1.42478940e+03 -2.25067100e+02 -1.05094910e+03 ...  2.73323060e+03\n",
      "   -2.33650210e+03  1.61170960e+03]]\n",
      "\n",
      " [[-1.16832400e+03 -1.16832400e+03 -1.16832400e+03 ... -1.20912380e+03\n",
      "   -1.20912380e+03 -1.20912380e+03]\n",
      "  [-1.22372030e+03 -1.22372030e+03 -1.22372030e+03 ... -1.26429330e+03\n",
      "   -1.26429330e+03 -1.26429330e+03]\n",
      "  [-7.99640000e+00 -7.99640000e+00 -7.99640000e+00 ... -7.99630000e+00\n",
      "   -7.99630000e+00 -7.99630000e+00]\n",
      "  ...\n",
      "  [-1.63535000e+01  2.19711000e+01 -2.52792000e+01 ... -5.43040000e+00\n",
      "    8.23920000e+00 -1.12352000e+01]\n",
      "  [-6.68945100e+02  2.97851500e+02  5.85937000e+01 ... -2.14843700e+02\n",
      "    1.95312400e+02 -1.46484300e+02]\n",
      "  [-6.37054400e+02 -4.42504900e+02  1.44958500e+03 ... -8.85009800e+02\n",
      "    5.70297200e+02 -2.05993700e+02]]\n",
      "\n",
      " [[-1.20912380e+03 -1.20912380e+03 -1.20912380e+03 ... -1.24991370e+03\n",
      "   -1.24991370e+03 -1.24991370e+03]\n",
      "  [-1.26429330e+03 -1.26429330e+03 -1.26429330e+03 ... -1.29385680e+03\n",
      "   -1.29385680e+03 -1.29385680e+03]\n",
      "  [-7.99630000e+00 -7.99630000e+00 -7.99630000e+00 ... -7.97430000e+00\n",
      "   -7.97430000e+00 -7.97430000e+00]\n",
      "  ...\n",
      "  [ 1.36071000e+01 -1.57293000e+01  1.70401000e+01 ...  1.26708000e+01\n",
      "   -1.26708000e+01  1.27332000e+01]\n",
      "  [ 1.26953100e+02 -7.81250000e+01  3.90625000e+01 ...  5.85937000e+01\n",
      "   -4.88281000e+01  4.39453000e+01]\n",
      "  [-1.67846700e+02  5.24520900e+02 -8.71658300e+02 ... -7.80105600e+02\n",
      "    7.76290900e+02 -7.72476200e+02]]]\n",
      "29/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/043_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (464, 49, 1024), Y_shape: (464,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR02_20200423_PGS_31_BSD_21/044_2020_04_23.csv\n",
      "[[[-7.12167800e+02 -7.12167800e+02 -7.12167800e+02 ... -7.00615900e+02\n",
      "   -7.00615900e+02 -7.00615900e+02]\n",
      "  [-7.00000600e+02 -7.00000600e+02 -7.00000600e+02 ... -7.04808200e+02\n",
      "   -7.04808200e+02 -7.04808200e+02]\n",
      "  [ 4.94370000e+00  4.94370000e+00  4.94370000e+00 ...  6.15200000e-01\n",
      "    6.15200000e-01  6.15200000e-01]\n",
      "  ...\n",
      "  [ 1.99740000e+00  2.37190000e+00  2.62150000e+00 ...  4.49408000e+01\n",
      "    4.07588000e+01  3.50788000e+01]\n",
      "  [ 1.85546800e+02  1.85546800e+02  2.29492100e+02 ...  7.30956830e+03\n",
      "    7.06054490e+03  6.45507630e+03]\n",
      "  [ 3.16619900e+02  3.16619900e+02  3.62396200e+02 ...  7.12203980e+03\n",
      "    6.75010680e+03  6.09016420e+03]]\n",
      "\n",
      " [[-7.00615900e+02 -7.00615900e+02 -7.00615900e+02 ... -7.01704500e+02\n",
      "   -7.01704500e+02 -7.01704500e+02]\n",
      "  [-7.04808200e+02 -7.04808200e+02 -7.04808200e+02 ... -7.32174900e+02\n",
      "   -7.32174900e+02 -7.32174900e+02]\n",
      "  [ 6.15200000e-01  6.15200000e-01  6.15200000e-01 ... -1.11580000e+00\n",
      "   -1.11580000e+00 -1.11580000e+00]\n",
      "  ...\n",
      "  [ 2.75887000e+01  1.88502000e+01  9.36270000e+00 ... -1.37320000e+00\n",
      "   -1.16097000e+01 -1.98489000e+01]\n",
      "  [ 5.73242030e+03  4.77538930e+03  3.58398340e+03 ...  1.04052705e+04\n",
      "    6.76269340e+03  1.37206990e+03]\n",
      "  [ 5.14984130e+03  3.93676760e+03  2.57110600e+03 ...  1.19590760e+03\n",
      "   -5.37872300e+02 -2.02369690e+03]]\n",
      "\n",
      " [[-7.01704500e+02 -7.01704500e+02 -7.01704500e+02 ... -7.20130100e+02\n",
      "   -7.20130100e+02 -7.20130100e+02]\n",
      "  [-7.32174900e+02 -7.32174900e+02 -7.32174900e+02 ... -7.72520300e+02\n",
      "   -7.72520300e+02 -7.72520300e+02]\n",
      "  [-1.11580000e+00 -1.11580000e+00 -1.11580000e+00 ... -5.93770000e+00\n",
      "   -5.93770000e+00 -5.93770000e+00]\n",
      "  ...\n",
      "  [-2.41557000e+01 -2.69645000e+01 -2.55289000e+01 ... -4.56274000e+01\n",
      "   -4.11334000e+01 -2.17838000e+01]\n",
      "  [-3.99902230e+03 -8.02245870e+03 -1.01074191e+04 ...  1.05468720e+03\n",
      "    2.39746030e+03  2.03613220e+03]\n",
      "  [-3.18145750e+03 -3.79371640e+03 -4.09507750e+03 ... -5.73158260e+03\n",
      "   -6.92176820e+03 -5.79833980e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.12752430e+03 -1.12752430e+03 -1.12752430e+03 ... -1.16832370e+03\n",
      "   -1.16832370e+03 -1.16832370e+03]\n",
      "  [-1.18292030e+03 -1.18292030e+03 -1.18292030e+03 ... -1.22372030e+03\n",
      "   -1.22372030e+03 -1.22372030e+03]\n",
      "  [-7.99640000e+00 -7.99640000e+00 -7.99640000e+00 ... -7.99580000e+00\n",
      "   -7.99580000e+00 -7.99580000e+00]\n",
      "  ...\n",
      "  [ 8.05190000e+00 -1.61038000e+01  1.95368000e+01 ... -1.06110000e+01\n",
      "    1.56040000e+00  7.92710000e+00]\n",
      "  [ 1.65039020e+03 -1.55273390e+03  8.98437200e+02 ...  9.76562200e+02\n",
      "   -1.03027310e+03  9.32616900e+02]\n",
      "  [ 1.40190120e+03 -2.07901000e+02 -1.05667110e+03 ...  2.71034240e+03\n",
      "   -2.31552120e+03  1.60217290e+03]]\n",
      "\n",
      " [[-1.16832370e+03 -1.16832370e+03 -1.16832370e+03 ... -1.20912450e+03\n",
      "   -1.20912450e+03 -1.20912450e+03]\n",
      "  [-1.22372030e+03 -1.22372030e+03 -1.22372030e+03 ... -1.26429330e+03\n",
      "   -1.26429330e+03 -1.26429330e+03]\n",
      "  [-7.99580000e+00 -7.99580000e+00 -7.99580000e+00 ... -7.99630000e+00\n",
      "   -7.99630000e+00 -7.99630000e+00]\n",
      "  ...\n",
      "  [-1.61662000e+01  2.17838000e+01 -2.49047000e+01 ... -5.18070000e+00\n",
      "    8.17670000e+00 -1.09855000e+01]\n",
      "  [-6.68945100e+02  3.12499900e+02  5.37109000e+01 ... -2.00195300e+02\n",
      "    2.00195300e+02 -1.70898400e+02]\n",
      "  [-6.52313200e+02 -4.25338700e+02  1.43241880e+03 ... -9.02175900e+02\n",
      "    5.72204600e+02 -2.21252400e+02]]\n",
      "\n",
      " [[-1.20912450e+03 -1.20912450e+03 -1.20912450e+03 ... -1.24991360e+03\n",
      "   -1.24991360e+03 -1.24991360e+03]\n",
      "  [-1.26429330e+03 -1.26429330e+03 -1.26429330e+03 ... -1.29385680e+03\n",
      "   -1.29385680e+03 -1.29385680e+03]\n",
      "  [-7.99630000e+00 -7.99630000e+00 -7.99630000e+00 ... -7.97410000e+00\n",
      "   -7.97410000e+00 -7.97410000e+00]\n",
      "  ...\n",
      "  [ 1.34823000e+01 -1.51051000e+01  1.69152000e+01 ...  1.25460000e+01\n",
      "   -1.24836000e+01  1.25460000e+01]\n",
      "  [ 1.31835900e+02 -8.30078000e+01  3.90625000e+01 ...  6.83594000e+01\n",
      "   -6.83594000e+01  5.85937000e+01]\n",
      "  [-1.37329100e+02  4.92095900e+02 -8.39233400e+02 ... -7.45773300e+02\n",
      "    7.34329200e+02 -7.26699800e+02]]]\n",
      "30/120 folders downloaded\n",
      "downloaded folder: NR02_20200423_PGS_31_BSD_21/044_2020_04_23.csv\n",
      "Shape of collected datafram: X_shape: (480, 49, 1024), Y_shape: (480,)\n",
      "/Users/fabiankolb/Documents/Universität/TUM_Master/Masterarbeit/CODE/data/NR22_20200508_PGS_11_BSD_P1/265_2020_05_08.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-187-30593e950105>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeSeriesData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-186-cd37b6f6d6f2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtraining_folders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_folders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_folder_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_train_BSD_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_of_test_BSD_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate_data_from_BSD_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_folders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_of_interest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-185-7c003655955d>\u001b[0m in \u001b[0;36mconcatenate_data_from_BSD_state\u001b[0;34m(folders, data_path, features_of_interest, window_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_BSD_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mdata_BSD_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_BSD_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#write csv in numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mfeature_index_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_of_interest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#get index for all features of interest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mdata_BSD_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_BSD_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_index_list\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#slice numpy array such that just features of interest are included\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, like)\u001b[0m\n\u001b[1;32m   2138\u001b[0m         rows = list(\n\u001b[1;32m   2139\u001b[0m             zip(*[[conv._loose_call(_r) for _r in map(itemgetter(i), rows)]\n\u001b[0;32m-> 2140\u001b[0;31m                   for (i, conv) in enumerate(converters)]))\n\u001b[0m\u001b[1;32m   2141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2142\u001b[0m         rows = list(\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2138\u001b[0m         rows = list(\n\u001b[1;32m   2139\u001b[0m             zip(*[[conv._loose_call(_r) for _r in map(itemgetter(i), rows)]\n\u001b[0;32m-> 2140\u001b[0;31m                   for (i, conv) in enumerate(converters)]))\n\u001b[0m\u001b[1;32m   2141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2142\u001b[0m         rows = list(\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         rows = list(\n\u001b[0;32m-> 2139\u001b[0;31m             zip(*[[conv._loose_call(_r) for _r in map(itemgetter(i), rows)]\n\u001b[0m\u001b[1;32m   2140\u001b[0m                   for (i, conv) in enumerate(converters)]))\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = TimeSeriesData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size,hidden_size,num_layers):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        formula [(W−K+2P)/S]+1.\n",
    "        \"\"\"\n",
    "        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=2, stride=1)#input: 1024\n",
    "        self.conv2 = nn.Conv1d(64,32,kernel_size=1, stride = 1, padding=1)#input: [(1025-2+2*0)/1]+1 = 1023\n",
    "        self.batch1 =nn.BatchNorm1d(32)#input: [(1023-1+2*1)/1]+1 = 1025\n",
    "        self.conv3 = nn.Conv1d(32,32,kernel_size=1, stride = 1, padding=1) #input:1025\n",
    "        self.batch2 =nn.BatchNorm1d(32)#input: [(1025-2+0)/1]+1 = 1027\n",
    "        self.LSTM = nn.LSTM(input_size=1027, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        #self.fc1 = nn.Linear(32*hidden_size, output_size)\n",
    "        self.fc1 = nn.Linear(32*1027, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.selu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = F.selu(self.batch1(x))\n",
    "        x = self.conv3(x)\n",
    "        x = F.selu(self.batch2(x))\n",
    "        #x, h = self.LSTM(x) \n",
    "        x = torch.reshape(x,(x.shape[0],x.shape[1]*x.shape[2]))\n",
    "        x = self.fc1(x)\n",
    "        output = x\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNLSTM(\n",
      "  (conv1): Conv1d(49, 64, kernel_size=(2,), stride=(1,))\n",
      "  (conv2): Conv1d(64, 32, kernel_size=(1,), stride=(1,), padding=(1,))\n",
      "  (batch1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv1d(32, 32, kernel_size=(1,), stride=(1,), padding=(1,))\n",
      "  (batch2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (LSTM): LSTM(1027, 1000, num_layers=2, batch_first=True)\n",
      "  (fc1): Linear(in_features=32864, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 49\n",
    "output_size = 4\n",
    "hidden_size = 1000\n",
    "num_layers = 2\n",
    "\n",
    "model = CNNLSTM(input_size, output_size,hidden_size, num_layers)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train/test dimensions\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "#split dataset randomly\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "#define batch size for dataloader\n",
    "batch_size = 4\n",
    "\n",
    "#dataloader\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Step [20/384], Loss: 176.2216\n",
      "Epoch [1/300], Step [40/384], Loss: 115.6225\n",
      "Epoch [1/300], Step [60/384], Loss: 113.7861\n",
      "Epoch [1/300], Step [80/384], Loss: 6.2788\n",
      "Epoch [1/300], Step [100/384], Loss: 336.2570\n",
      "Epoch [1/300], Step [120/384], Loss: 1048.8601\n",
      "Epoch [1/300], Step [140/384], Loss: 208.6692\n",
      "Epoch [1/300], Step [160/384], Loss: 358.9186\n",
      "Epoch [1/300], Step [180/384], Loss: 132.0727\n",
      "Epoch [1/300], Step [200/384], Loss: 449.2500\n",
      "Epoch [1/300], Step [220/384], Loss: 185.4740\n",
      "Epoch [1/300], Step [240/384], Loss: 84.5571\n",
      "Epoch [1/300], Step [260/384], Loss: 209.4806\n",
      "Epoch [1/300], Step [280/384], Loss: 389.3882\n",
      "Epoch [1/300], Step [300/384], Loss: 100.8899\n",
      "Epoch [1/300], Step [320/384], Loss: 95.0956\n",
      "Epoch [1/300], Step [340/384], Loss: 352.0946\n",
      "Epoch [1/300], Step [360/384], Loss: 270.8908\n",
      "Epoch [1/300], Step [380/384], Loss: 28.8085\n",
      "Epoch [2/300], Step [20/384], Loss: 201.6741\n",
      "Epoch [2/300], Step [40/384], Loss: 243.2604\n",
      "Epoch [2/300], Step [60/384], Loss: 349.5515\n",
      "Epoch [2/300], Step [80/384], Loss: 850.5218\n",
      "Epoch [2/300], Step [100/384], Loss: 375.6591\n",
      "Epoch [2/300], Step [120/384], Loss: 195.1398\n",
      "Epoch [2/300], Step [140/384], Loss: 303.9767\n",
      "Epoch [2/300], Step [160/384], Loss: 243.0852\n",
      "Epoch [2/300], Step [180/384], Loss: 343.2138\n",
      "Epoch [2/300], Step [200/384], Loss: 180.3932\n",
      "Epoch [2/300], Step [220/384], Loss: 74.0069\n",
      "Epoch [2/300], Step [240/384], Loss: 798.5521\n",
      "Epoch [2/300], Step [260/384], Loss: 245.9857\n",
      "Epoch [2/300], Step [280/384], Loss: 136.3091\n",
      "Epoch [2/300], Step [300/384], Loss: 57.6484\n",
      "Epoch [2/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [2/300], Step [340/384], Loss: 462.1468\n",
      "Epoch [2/300], Step [360/384], Loss: 412.8425\n",
      "Epoch [2/300], Step [380/384], Loss: 186.5971\n",
      "Epoch [3/300], Step [20/384], Loss: 54.3235\n",
      "Epoch [3/300], Step [40/384], Loss: 273.8480\n",
      "Epoch [3/300], Step [60/384], Loss: 124.8888\n",
      "Epoch [3/300], Step [80/384], Loss: 546.7826\n",
      "Epoch [3/300], Step [100/384], Loss: 645.2771\n",
      "Epoch [3/300], Step [120/384], Loss: 252.0900\n",
      "Epoch [3/300], Step [140/384], Loss: 214.8069\n",
      "Epoch [3/300], Step [160/384], Loss: 420.0273\n",
      "Epoch [3/300], Step [180/384], Loss: 107.8010\n",
      "Epoch [3/300], Step [200/384], Loss: 315.9930\n",
      "Epoch [3/300], Step [220/384], Loss: 73.1513\n",
      "Epoch [3/300], Step [240/384], Loss: 674.7268\n",
      "Epoch [3/300], Step [260/384], Loss: 184.8130\n",
      "Epoch [3/300], Step [280/384], Loss: 217.2345\n",
      "Epoch [3/300], Step [300/384], Loss: 226.2684\n",
      "Epoch [3/300], Step [320/384], Loss: 490.1893\n",
      "Epoch [3/300], Step [340/384], Loss: 325.7801\n",
      "Epoch [3/300], Step [360/384], Loss: 133.6876\n",
      "Epoch [3/300], Step [380/384], Loss: 327.1679\n",
      "Epoch [4/300], Step [20/384], Loss: 389.5373\n",
      "Epoch [4/300], Step [40/384], Loss: 140.1238\n",
      "Epoch [4/300], Step [60/384], Loss: 596.8061\n",
      "Epoch [4/300], Step [80/384], Loss: 170.4752\n",
      "Epoch [4/300], Step [100/384], Loss: 176.6543\n",
      "Epoch [4/300], Step [120/384], Loss: 90.7874\n",
      "Epoch [4/300], Step [140/384], Loss: 252.5672\n",
      "Epoch [4/300], Step [160/384], Loss: 233.7051\n",
      "Epoch [4/300], Step [180/384], Loss: 238.5690\n",
      "Epoch [4/300], Step [200/384], Loss: 186.4563\n",
      "Epoch [4/300], Step [220/384], Loss: 322.2566\n",
      "Epoch [4/300], Step [240/384], Loss: 69.3368\n",
      "Epoch [4/300], Step [260/384], Loss: 36.2346\n",
      "Epoch [4/300], Step [280/384], Loss: 191.7397\n",
      "Epoch [4/300], Step [300/384], Loss: 153.3120\n",
      "Epoch [4/300], Step [320/384], Loss: 98.8276\n",
      "Epoch [4/300], Step [340/384], Loss: 268.0759\n",
      "Epoch [4/300], Step [360/384], Loss: 139.0225\n",
      "Epoch [4/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [5/300], Step [20/384], Loss: 3.3205\n",
      "Epoch [5/300], Step [40/384], Loss: 404.5790\n",
      "Epoch [5/300], Step [60/384], Loss: 121.1616\n",
      "Epoch [5/300], Step [80/384], Loss: 335.6649\n",
      "Epoch [5/300], Step [100/384], Loss: 184.9553\n",
      "Epoch [5/300], Step [120/384], Loss: 13.5513\n",
      "Epoch [5/300], Step [140/384], Loss: 294.4239\n",
      "Epoch [5/300], Step [160/384], Loss: 49.8862\n",
      "Epoch [5/300], Step [180/384], Loss: 178.8657\n",
      "Epoch [5/300], Step [200/384], Loss: 142.7625\n",
      "Epoch [5/300], Step [220/384], Loss: 202.9361\n",
      "Epoch [5/300], Step [240/384], Loss: 446.6769\n",
      "Epoch [5/300], Step [260/384], Loss: 168.6686\n",
      "Epoch [5/300], Step [280/384], Loss: 142.3040\n",
      "Epoch [5/300], Step [300/384], Loss: 65.0047\n",
      "Epoch [5/300], Step [320/384], Loss: 120.6942\n",
      "Epoch [5/300], Step [340/384], Loss: 30.5632\n",
      "Epoch [5/300], Step [360/384], Loss: 279.5207\n",
      "Epoch [5/300], Step [380/384], Loss: 170.4192\n",
      "Epoch [6/300], Step [20/384], Loss: 494.0749\n",
      "Epoch [6/300], Step [40/384], Loss: 274.7858\n",
      "Epoch [6/300], Step [60/384], Loss: 189.8079\n",
      "Epoch [6/300], Step [80/384], Loss: 347.3825\n",
      "Epoch [6/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [6/300], Step [120/384], Loss: 220.6780\n",
      "Epoch [6/300], Step [140/384], Loss: 43.5644\n",
      "Epoch [6/300], Step [160/384], Loss: 132.6863\n",
      "Epoch [6/300], Step [180/384], Loss: 42.7672\n",
      "Epoch [6/300], Step [200/384], Loss: 174.1095\n",
      "Epoch [6/300], Step [220/384], Loss: 285.3135\n",
      "Epoch [6/300], Step [240/384], Loss: 314.9284\n",
      "Epoch [6/300], Step [260/384], Loss: 230.6310\n",
      "Epoch [6/300], Step [280/384], Loss: 272.7675\n",
      "Epoch [6/300], Step [300/384], Loss: 268.5583\n",
      "Epoch [6/300], Step [320/384], Loss: 207.8508\n",
      "Epoch [6/300], Step [340/384], Loss: 64.2267\n",
      "Epoch [6/300], Step [360/384], Loss: 469.4888\n",
      "Epoch [6/300], Step [380/384], Loss: 224.2397\n",
      "Epoch [7/300], Step [20/384], Loss: 104.3498\n",
      "Epoch [7/300], Step [40/384], Loss: 155.4766\n",
      "Epoch [7/300], Step [60/384], Loss: 112.1083\n",
      "Epoch [7/300], Step [80/384], Loss: 509.2223\n",
      "Epoch [7/300], Step [100/384], Loss: 29.5501\n",
      "Epoch [7/300], Step [120/384], Loss: 82.8133\n",
      "Epoch [7/300], Step [140/384], Loss: 186.8637\n",
      "Epoch [7/300], Step [160/384], Loss: 169.5114\n",
      "Epoch [7/300], Step [180/384], Loss: 292.1115\n",
      "Epoch [7/300], Step [200/384], Loss: 210.3516\n",
      "Epoch [7/300], Step [220/384], Loss: 129.0535\n",
      "Epoch [7/300], Step [240/384], Loss: 477.2210\n",
      "Epoch [7/300], Step [260/384], Loss: 412.3375\n",
      "Epoch [7/300], Step [280/384], Loss: 50.5470\n",
      "Epoch [7/300], Step [300/384], Loss: 185.0083\n",
      "Epoch [7/300], Step [320/384], Loss: 194.1347\n",
      "Epoch [7/300], Step [340/384], Loss: 221.8221\n",
      "Epoch [7/300], Step [360/384], Loss: 138.9414\n",
      "Epoch [7/300], Step [380/384], Loss: 468.2784\n",
      "Epoch [8/300], Step [20/384], Loss: 151.7154\n",
      "Epoch [8/300], Step [40/384], Loss: 222.2658\n",
      "Epoch [8/300], Step [60/384], Loss: 76.7399\n",
      "Epoch [8/300], Step [80/384], Loss: 190.1827\n",
      "Epoch [8/300], Step [100/384], Loss: 61.1448\n",
      "Epoch [8/300], Step [120/384], Loss: 86.5241\n",
      "Epoch [8/300], Step [140/384], Loss: 54.1831\n",
      "Epoch [8/300], Step [160/384], Loss: 21.9387\n",
      "Epoch [8/300], Step [180/384], Loss: 140.0605\n",
      "Epoch [8/300], Step [200/384], Loss: 214.6870\n",
      "Epoch [8/300], Step [220/384], Loss: 109.7477\n",
      "Epoch [8/300], Step [240/384], Loss: 38.6229\n",
      "Epoch [8/300], Step [260/384], Loss: 500.5539\n",
      "Epoch [8/300], Step [280/384], Loss: 42.1405\n",
      "Epoch [8/300], Step [300/384], Loss: 127.3064\n",
      "Epoch [8/300], Step [320/384], Loss: 64.9748\n",
      "Epoch [8/300], Step [340/384], Loss: 246.6681\n",
      "Epoch [8/300], Step [360/384], Loss: 209.2066\n",
      "Epoch [8/300], Step [380/384], Loss: 234.8328\n",
      "Epoch [9/300], Step [20/384], Loss: 257.5560\n",
      "Epoch [9/300], Step [40/384], Loss: 675.7160\n",
      "Epoch [9/300], Step [60/384], Loss: 36.1283\n",
      "Epoch [9/300], Step [80/384], Loss: 339.5772\n",
      "Epoch [9/300], Step [100/384], Loss: 411.8564\n",
      "Epoch [9/300], Step [120/384], Loss: 236.5098\n",
      "Epoch [9/300], Step [140/384], Loss: 112.9347\n",
      "Epoch [9/300], Step [160/384], Loss: 61.2569\n",
      "Epoch [9/300], Step [180/384], Loss: 109.8526\n",
      "Epoch [9/300], Step [200/384], Loss: 47.2060\n",
      "Epoch [9/300], Step [220/384], Loss: 121.6198\n",
      "Epoch [9/300], Step [240/384], Loss: 691.6053\n",
      "Epoch [9/300], Step [260/384], Loss: 179.2279\n",
      "Epoch [9/300], Step [280/384], Loss: 8.6242\n",
      "Epoch [9/300], Step [300/384], Loss: 493.7797\n",
      "Epoch [9/300], Step [320/384], Loss: 232.0803\n",
      "Epoch [9/300], Step [340/384], Loss: 260.1858\n",
      "Epoch [9/300], Step [360/384], Loss: 272.2631\n",
      "Epoch [9/300], Step [380/384], Loss: 536.5875\n",
      "Epoch [10/300], Step [20/384], Loss: 238.9319\n",
      "Epoch [10/300], Step [40/384], Loss: 37.3160\n",
      "Epoch [10/300], Step [60/384], Loss: 346.6078\n",
      "Epoch [10/300], Step [80/384], Loss: 269.3884\n",
      "Epoch [10/300], Step [100/384], Loss: 431.4458\n",
      "Epoch [10/300], Step [120/384], Loss: 141.5560\n",
      "Epoch [10/300], Step [140/384], Loss: 67.7078\n",
      "Epoch [10/300], Step [160/384], Loss: 222.5265\n",
      "Epoch [10/300], Step [180/384], Loss: 145.3696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Step [200/384], Loss: 276.3358\n",
      "Epoch [10/300], Step [220/384], Loss: 86.9628\n",
      "Epoch [10/300], Step [240/384], Loss: 89.7641\n",
      "Epoch [10/300], Step [260/384], Loss: 373.2453\n",
      "Epoch [10/300], Step [280/384], Loss: 36.4367\n",
      "Epoch [10/300], Step [300/384], Loss: 236.5370\n",
      "Epoch [10/300], Step [320/384], Loss: 175.7839\n",
      "Epoch [10/300], Step [340/384], Loss: 296.2721\n",
      "Epoch [10/300], Step [360/384], Loss: 91.3030\n",
      "Epoch [10/300], Step [380/384], Loss: 53.0387\n",
      "Epoch [11/300], Step [20/384], Loss: 150.1892\n",
      "Epoch [11/300], Step [40/384], Loss: 226.7455\n",
      "Epoch [11/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [11/300], Step [80/384], Loss: 152.6787\n",
      "Epoch [11/300], Step [100/384], Loss: 92.1328\n",
      "Epoch [11/300], Step [120/384], Loss: 56.0465\n",
      "Epoch [11/300], Step [140/384], Loss: 291.5152\n",
      "Epoch [11/300], Step [160/384], Loss: 162.8896\n",
      "Epoch [11/300], Step [180/384], Loss: 372.0794\n",
      "Epoch [11/300], Step [200/384], Loss: 177.1702\n",
      "Epoch [11/300], Step [220/384], Loss: 58.3453\n",
      "Epoch [11/300], Step [240/384], Loss: 152.3551\n",
      "Epoch [11/300], Step [260/384], Loss: 118.3514\n",
      "Epoch [11/300], Step [280/384], Loss: 50.0812\n",
      "Epoch [11/300], Step [300/384], Loss: 303.9891\n",
      "Epoch [11/300], Step [320/384], Loss: 186.6983\n",
      "Epoch [11/300], Step [340/384], Loss: 195.0657\n",
      "Epoch [11/300], Step [360/384], Loss: 95.6604\n",
      "Epoch [11/300], Step [380/384], Loss: 30.2537\n",
      "Epoch [12/300], Step [20/384], Loss: 178.2685\n",
      "Epoch [12/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [12/300], Step [60/384], Loss: 518.9617\n",
      "Epoch [12/300], Step [80/384], Loss: 291.7367\n",
      "Epoch [12/300], Step [100/384], Loss: 118.8459\n",
      "Epoch [12/300], Step [120/384], Loss: 79.7891\n",
      "Epoch [12/300], Step [140/384], Loss: 114.7451\n",
      "Epoch [12/300], Step [160/384], Loss: 214.3064\n",
      "Epoch [12/300], Step [180/384], Loss: 109.0106\n",
      "Epoch [12/300], Step [200/384], Loss: 493.7366\n",
      "Epoch [12/300], Step [220/384], Loss: 662.8901\n",
      "Epoch [12/300], Step [240/384], Loss: 235.3129\n",
      "Epoch [12/300], Step [260/384], Loss: 442.6215\n",
      "Epoch [12/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [12/300], Step [300/384], Loss: 179.3795\n",
      "Epoch [12/300], Step [320/384], Loss: 111.7829\n",
      "Epoch [12/300], Step [340/384], Loss: 81.6074\n",
      "Epoch [12/300], Step [360/384], Loss: 569.5897\n",
      "Epoch [12/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [13/300], Step [20/384], Loss: 250.4509\n",
      "Epoch [13/300], Step [40/384], Loss: 249.1758\n",
      "Epoch [13/300], Step [60/384], Loss: 117.3707\n",
      "Epoch [13/300], Step [80/384], Loss: 93.3445\n",
      "Epoch [13/300], Step [100/384], Loss: 86.2428\n",
      "Epoch [13/300], Step [120/384], Loss: 94.1673\n",
      "Epoch [13/300], Step [140/384], Loss: 87.0645\n",
      "Epoch [13/300], Step [160/384], Loss: 345.6119\n",
      "Epoch [13/300], Step [180/384], Loss: 232.7471\n",
      "Epoch [13/300], Step [200/384], Loss: 95.5795\n",
      "Epoch [13/300], Step [220/384], Loss: 205.1053\n",
      "Epoch [13/300], Step [240/384], Loss: 226.7629\n",
      "Epoch [13/300], Step [260/384], Loss: 10.9011\n",
      "Epoch [13/300], Step [280/384], Loss: 316.1799\n",
      "Epoch [13/300], Step [300/384], Loss: 269.7089\n",
      "Epoch [13/300], Step [320/384], Loss: 87.7009\n",
      "Epoch [13/300], Step [340/384], Loss: 245.7417\n",
      "Epoch [13/300], Step [360/384], Loss: 39.7925\n",
      "Epoch [13/300], Step [380/384], Loss: 215.2525\n",
      "Epoch [14/300], Step [20/384], Loss: 617.6694\n",
      "Epoch [14/300], Step [40/384], Loss: 38.2819\n",
      "Epoch [14/300], Step [60/384], Loss: 55.4974\n",
      "Epoch [14/300], Step [80/384], Loss: 417.8641\n",
      "Epoch [14/300], Step [100/384], Loss: 91.1175\n",
      "Epoch [14/300], Step [120/384], Loss: 139.8832\n",
      "Epoch [14/300], Step [140/384], Loss: 61.5604\n",
      "Epoch [14/300], Step [160/384], Loss: 196.0593\n",
      "Epoch [14/300], Step [180/384], Loss: 73.7247\n",
      "Epoch [14/300], Step [200/384], Loss: 275.5109\n",
      "Epoch [14/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [14/300], Step [240/384], Loss: 349.1252\n",
      "Epoch [14/300], Step [260/384], Loss: 165.9750\n",
      "Epoch [14/300], Step [280/384], Loss: 435.9018\n",
      "Epoch [14/300], Step [300/384], Loss: 178.5326\n",
      "Epoch [14/300], Step [320/384], Loss: 743.6645\n",
      "Epoch [14/300], Step [340/384], Loss: 72.9626\n",
      "Epoch [14/300], Step [360/384], Loss: 59.4830\n",
      "Epoch [14/300], Step [380/384], Loss: 133.7612\n",
      "Epoch [15/300], Step [20/384], Loss: 167.2068\n",
      "Epoch [15/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [15/300], Step [60/384], Loss: 436.3457\n",
      "Epoch [15/300], Step [80/384], Loss: 43.0774\n",
      "Epoch [15/300], Step [100/384], Loss: 171.1128\n",
      "Epoch [15/300], Step [120/384], Loss: 303.5412\n",
      "Epoch [15/300], Step [140/384], Loss: 71.9913\n",
      "Epoch [15/300], Step [160/384], Loss: 32.9095\n",
      "Epoch [15/300], Step [180/384], Loss: 260.0076\n",
      "Epoch [15/300], Step [200/384], Loss: 158.8285\n",
      "Epoch [15/300], Step [220/384], Loss: 363.2318\n",
      "Epoch [15/300], Step [240/384], Loss: 7.0952\n",
      "Epoch [15/300], Step [260/384], Loss: 151.5861\n",
      "Epoch [15/300], Step [280/384], Loss: 207.8067\n",
      "Epoch [15/300], Step [300/384], Loss: 204.4653\n",
      "Epoch [15/300], Step [320/384], Loss: 145.0028\n",
      "Epoch [15/300], Step [340/384], Loss: 135.0941\n",
      "Epoch [15/300], Step [360/384], Loss: 179.5454\n",
      "Epoch [15/300], Step [380/384], Loss: 161.1415\n",
      "Epoch [16/300], Step [20/384], Loss: 109.8119\n",
      "Epoch [16/300], Step [40/384], Loss: 153.4469\n",
      "Epoch [16/300], Step [60/384], Loss: 147.4045\n",
      "Epoch [16/300], Step [80/384], Loss: 30.5962\n",
      "Epoch [16/300], Step [100/384], Loss: 174.3658\n",
      "Epoch [16/300], Step [120/384], Loss: 302.9380\n",
      "Epoch [16/300], Step [140/384], Loss: 406.7729\n",
      "Epoch [16/300], Step [160/384], Loss: 8.7759\n",
      "Epoch [16/300], Step [180/384], Loss: 159.8936\n",
      "Epoch [16/300], Step [200/384], Loss: 154.4903\n",
      "Epoch [16/300], Step [220/384], Loss: 78.9265\n",
      "Epoch [16/300], Step [240/384], Loss: 83.3685\n",
      "Epoch [16/300], Step [260/384], Loss: 202.4635\n",
      "Epoch [16/300], Step [280/384], Loss: 177.3757\n",
      "Epoch [16/300], Step [300/384], Loss: 70.2712\n",
      "Epoch [16/300], Step [320/384], Loss: 231.3077\n",
      "Epoch [16/300], Step [340/384], Loss: 566.0844\n",
      "Epoch [16/300], Step [360/384], Loss: 142.7024\n",
      "Epoch [16/300], Step [380/384], Loss: 154.3559\n",
      "Epoch [17/300], Step [20/384], Loss: 138.1393\n",
      "Epoch [17/300], Step [40/384], Loss: 103.5656\n",
      "Epoch [17/300], Step [60/384], Loss: 19.4599\n",
      "Epoch [17/300], Step [80/384], Loss: 62.2327\n",
      "Epoch [17/300], Step [100/384], Loss: 174.7430\n",
      "Epoch [17/300], Step [120/384], Loss: 290.6155\n",
      "Epoch [17/300], Step [140/384], Loss: 2.5739\n",
      "Epoch [17/300], Step [160/384], Loss: 348.2929\n",
      "Epoch [17/300], Step [180/384], Loss: 321.4147\n",
      "Epoch [17/300], Step [200/384], Loss: 278.7358\n",
      "Epoch [17/300], Step [220/384], Loss: 736.8032\n",
      "Epoch [17/300], Step [240/384], Loss: 152.9982\n",
      "Epoch [17/300], Step [260/384], Loss: 75.6152\n",
      "Epoch [17/300], Step [280/384], Loss: 157.6841\n",
      "Epoch [17/300], Step [300/384], Loss: 140.5795\n",
      "Epoch [17/300], Step [320/384], Loss: 157.7913\n",
      "Epoch [17/300], Step [340/384], Loss: 65.0504\n",
      "Epoch [17/300], Step [360/384], Loss: 188.4867\n",
      "Epoch [17/300], Step [380/384], Loss: 202.7452\n",
      "Epoch [18/300], Step [20/384], Loss: 97.4462\n",
      "Epoch [18/300], Step [40/384], Loss: 198.7834\n",
      "Epoch [18/300], Step [60/384], Loss: 125.7231\n",
      "Epoch [18/300], Step [80/384], Loss: 173.9565\n",
      "Epoch [18/300], Step [100/384], Loss: 105.6226\n",
      "Epoch [18/300], Step [120/384], Loss: 41.7366\n",
      "Epoch [18/300], Step [140/384], Loss: 200.4183\n",
      "Epoch [18/300], Step [160/384], Loss: 106.6475\n",
      "Epoch [18/300], Step [180/384], Loss: 253.9745\n",
      "Epoch [18/300], Step [200/384], Loss: 1.8944\n",
      "Epoch [18/300], Step [220/384], Loss: 23.8834\n",
      "Epoch [18/300], Step [240/384], Loss: 122.4074\n",
      "Epoch [18/300], Step [260/384], Loss: 74.5936\n",
      "Epoch [18/300], Step [280/384], Loss: 51.3304\n",
      "Epoch [18/300], Step [300/384], Loss: 35.4478\n",
      "Epoch [18/300], Step [320/384], Loss: 48.0543\n",
      "Epoch [18/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [18/300], Step [360/384], Loss: 206.1163\n",
      "Epoch [18/300], Step [380/384], Loss: 564.8677\n",
      "Epoch [19/300], Step [20/384], Loss: 96.3618\n",
      "Epoch [19/300], Step [40/384], Loss: 384.7192\n",
      "Epoch [19/300], Step [60/384], Loss: 207.3313\n",
      "Epoch [19/300], Step [80/384], Loss: 87.7501\n",
      "Epoch [19/300], Step [100/384], Loss: 64.4355\n",
      "Epoch [19/300], Step [120/384], Loss: 137.1384\n",
      "Epoch [19/300], Step [140/384], Loss: 162.3237\n",
      "Epoch [19/300], Step [160/384], Loss: 406.8105\n",
      "Epoch [19/300], Step [180/384], Loss: 174.8435\n",
      "Epoch [19/300], Step [200/384], Loss: 154.2420\n",
      "Epoch [19/300], Step [220/384], Loss: 253.0269\n",
      "Epoch [19/300], Step [240/384], Loss: 257.2250\n",
      "Epoch [19/300], Step [260/384], Loss: 86.2830\n",
      "Epoch [19/300], Step [280/384], Loss: 596.7119\n",
      "Epoch [19/300], Step [300/384], Loss: 115.7748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/300], Step [320/384], Loss: 359.3248\n",
      "Epoch [19/300], Step [340/384], Loss: 248.4505\n",
      "Epoch [19/300], Step [360/384], Loss: 49.6567\n",
      "Epoch [19/300], Step [380/384], Loss: 0.0681\n",
      "Epoch [20/300], Step [20/384], Loss: 150.2954\n",
      "Epoch [20/300], Step [40/384], Loss: 0.4534\n",
      "Epoch [20/300], Step [60/384], Loss: 109.6936\n",
      "Epoch [20/300], Step [80/384], Loss: 185.9288\n",
      "Epoch [20/300], Step [100/384], Loss: 126.8151\n",
      "Epoch [20/300], Step [120/384], Loss: 159.6874\n",
      "Epoch [20/300], Step [140/384], Loss: 5.4599\n",
      "Epoch [20/300], Step [160/384], Loss: 46.2326\n",
      "Epoch [20/300], Step [180/384], Loss: 160.0697\n",
      "Epoch [20/300], Step [200/384], Loss: 50.2716\n",
      "Epoch [20/300], Step [220/384], Loss: 446.4021\n",
      "Epoch [20/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [20/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [20/300], Step [280/384], Loss: 93.6576\n",
      "Epoch [20/300], Step [300/384], Loss: 250.4300\n",
      "Epoch [20/300], Step [320/384], Loss: 128.5643\n",
      "Epoch [20/300], Step [340/384], Loss: 148.0122\n",
      "Epoch [20/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [20/300], Step [380/384], Loss: 175.5372\n",
      "Epoch [21/300], Step [20/384], Loss: 2.6602\n",
      "Epoch [21/300], Step [40/384], Loss: 123.1169\n",
      "Epoch [21/300], Step [60/384], Loss: 64.2307\n",
      "Epoch [21/300], Step [80/384], Loss: 290.1529\n",
      "Epoch [21/300], Step [100/384], Loss: 587.0939\n",
      "Epoch [21/300], Step [120/384], Loss: 75.3737\n",
      "Epoch [21/300], Step [140/384], Loss: 176.9836\n",
      "Epoch [21/300], Step [160/384], Loss: 51.1969\n",
      "Epoch [21/300], Step [180/384], Loss: 49.9031\n",
      "Epoch [21/300], Step [200/384], Loss: 108.7431\n",
      "Epoch [21/300], Step [220/384], Loss: 296.5892\n",
      "Epoch [21/300], Step [240/384], Loss: 256.3574\n",
      "Epoch [21/300], Step [260/384], Loss: 65.7953\n",
      "Epoch [21/300], Step [280/384], Loss: 368.4022\n",
      "Epoch [21/300], Step [300/384], Loss: 250.3077\n",
      "Epoch [21/300], Step [320/384], Loss: 40.0371\n",
      "Epoch [21/300], Step [340/384], Loss: 182.0664\n",
      "Epoch [21/300], Step [360/384], Loss: 87.4288\n",
      "Epoch [21/300], Step [380/384], Loss: 107.4687\n",
      "Epoch [22/300], Step [20/384], Loss: 74.6897\n",
      "Epoch [22/300], Step [40/384], Loss: 129.4213\n",
      "Epoch [22/300], Step [60/384], Loss: 143.7151\n",
      "Epoch [22/300], Step [80/384], Loss: 67.1471\n",
      "Epoch [22/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [22/300], Step [120/384], Loss: 248.6510\n",
      "Epoch [22/300], Step [140/384], Loss: 351.8398\n",
      "Epoch [22/300], Step [160/384], Loss: 271.0641\n",
      "Epoch [22/300], Step [180/384], Loss: 55.5297\n",
      "Epoch [22/300], Step [200/384], Loss: 82.0863\n",
      "Epoch [22/300], Step [220/384], Loss: 36.8083\n",
      "Epoch [22/300], Step [240/384], Loss: 273.1543\n",
      "Epoch [22/300], Step [260/384], Loss: 18.4048\n",
      "Epoch [22/300], Step [280/384], Loss: 135.0772\n",
      "Epoch [22/300], Step [300/384], Loss: 22.7653\n",
      "Epoch [22/300], Step [320/384], Loss: 157.5126\n",
      "Epoch [22/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [22/300], Step [360/384], Loss: 273.3184\n",
      "Epoch [22/300], Step [380/384], Loss: 60.5820\n",
      "Epoch [23/300], Step [20/384], Loss: 245.0354\n",
      "Epoch [23/300], Step [40/384], Loss: 81.6001\n",
      "Epoch [23/300], Step [60/384], Loss: 48.4174\n",
      "Epoch [23/300], Step [80/384], Loss: 77.6823\n",
      "Epoch [23/300], Step [100/384], Loss: 89.5435\n",
      "Epoch [23/300], Step [120/384], Loss: 426.8755\n",
      "Epoch [23/300], Step [140/384], Loss: 77.6296\n",
      "Epoch [23/300], Step [160/384], Loss: 170.5035\n",
      "Epoch [23/300], Step [180/384], Loss: 225.8786\n",
      "Epoch [23/300], Step [200/384], Loss: 58.9414\n",
      "Epoch [23/300], Step [220/384], Loss: 230.5089\n",
      "Epoch [23/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [23/300], Step [260/384], Loss: 188.3714\n",
      "Epoch [23/300], Step [280/384], Loss: 103.9957\n",
      "Epoch [23/300], Step [300/384], Loss: 86.3530\n",
      "Epoch [23/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [23/300], Step [340/384], Loss: 25.4019\n",
      "Epoch [23/300], Step [360/384], Loss: 43.0518\n",
      "Epoch [23/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [24/300], Step [20/384], Loss: 50.5299\n",
      "Epoch [24/300], Step [40/384], Loss: 46.9272\n",
      "Epoch [24/300], Step [60/384], Loss: 69.1504\n",
      "Epoch [24/300], Step [80/384], Loss: 293.1129\n",
      "Epoch [24/300], Step [100/384], Loss: 0.4742\n",
      "Epoch [24/300], Step [120/384], Loss: 119.8317\n",
      "Epoch [24/300], Step [140/384], Loss: 93.7912\n",
      "Epoch [24/300], Step [160/384], Loss: 445.9630\n",
      "Epoch [24/300], Step [180/384], Loss: 73.7091\n",
      "Epoch [24/300], Step [200/384], Loss: 266.2441\n",
      "Epoch [24/300], Step [220/384], Loss: 350.1360\n",
      "Epoch [24/300], Step [240/384], Loss: 207.1621\n",
      "Epoch [24/300], Step [260/384], Loss: 120.8028\n",
      "Epoch [24/300], Step [280/384], Loss: 101.9425\n",
      "Epoch [24/300], Step [300/384], Loss: 255.9015\n",
      "Epoch [24/300], Step [320/384], Loss: 127.8611\n",
      "Epoch [24/300], Step [340/384], Loss: 188.3579\n",
      "Epoch [24/300], Step [360/384], Loss: 99.0078\n",
      "Epoch [24/300], Step [380/384], Loss: 162.5776\n",
      "Epoch [25/300], Step [20/384], Loss: 59.4945\n",
      "Epoch [25/300], Step [40/384], Loss: 3.7974\n",
      "Epoch [25/300], Step [60/384], Loss: 202.7737\n",
      "Epoch [25/300], Step [80/384], Loss: 28.5396\n",
      "Epoch [25/300], Step [100/384], Loss: 54.3520\n",
      "Epoch [25/300], Step [120/384], Loss: 74.6927\n",
      "Epoch [25/300], Step [140/384], Loss: 59.4582\n",
      "Epoch [25/300], Step [160/384], Loss: 31.7448\n",
      "Epoch [25/300], Step [180/384], Loss: 131.1904\n",
      "Epoch [25/300], Step [200/384], Loss: 143.9957\n",
      "Epoch [25/300], Step [220/384], Loss: 91.6459\n",
      "Epoch [25/300], Step [240/384], Loss: 127.4933\n",
      "Epoch [25/300], Step [260/384], Loss: 71.8903\n",
      "Epoch [25/300], Step [280/384], Loss: 4.5746\n",
      "Epoch [25/300], Step [300/384], Loss: 70.2044\n",
      "Epoch [25/300], Step [320/384], Loss: 145.4636\n",
      "Epoch [25/300], Step [340/384], Loss: 131.0608\n",
      "Epoch [25/300], Step [360/384], Loss: 76.7719\n",
      "Epoch [25/300], Step [380/384], Loss: 46.6378\n",
      "Epoch [26/300], Step [20/384], Loss: 114.6686\n",
      "Epoch [26/300], Step [40/384], Loss: 146.6223\n",
      "Epoch [26/300], Step [60/384], Loss: 76.2225\n",
      "Epoch [26/300], Step [80/384], Loss: 210.9285\n",
      "Epoch [26/300], Step [100/384], Loss: 29.1208\n",
      "Epoch [26/300], Step [120/384], Loss: 121.7205\n",
      "Epoch [26/300], Step [140/384], Loss: 184.2811\n",
      "Epoch [26/300], Step [160/384], Loss: 15.7362\n",
      "Epoch [26/300], Step [180/384], Loss: 90.6074\n",
      "Epoch [26/300], Step [200/384], Loss: 99.6091\n",
      "Epoch [26/300], Step [220/384], Loss: 86.2639\n",
      "Epoch [26/300], Step [240/384], Loss: 174.0945\n",
      "Epoch [26/300], Step [260/384], Loss: 151.3852\n",
      "Epoch [26/300], Step [280/384], Loss: 119.0429\n",
      "Epoch [26/300], Step [300/384], Loss: 209.2583\n",
      "Epoch [26/300], Step [320/384], Loss: 83.4108\n",
      "Epoch [26/300], Step [340/384], Loss: 44.2871\n",
      "Epoch [26/300], Step [360/384], Loss: 82.5447\n",
      "Epoch [26/300], Step [380/384], Loss: 234.1992\n",
      "Epoch [27/300], Step [20/384], Loss: 108.9398\n",
      "Epoch [27/300], Step [40/384], Loss: 17.3118\n",
      "Epoch [27/300], Step [60/384], Loss: 104.2850\n",
      "Epoch [27/300], Step [80/384], Loss: 35.7234\n",
      "Epoch [27/300], Step [100/384], Loss: 35.4985\n",
      "Epoch [27/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [27/300], Step [140/384], Loss: 154.4293\n",
      "Epoch [27/300], Step [160/384], Loss: 81.7106\n",
      "Epoch [27/300], Step [180/384], Loss: 163.7350\n",
      "Epoch [27/300], Step [200/384], Loss: 61.6390\n",
      "Epoch [27/300], Step [220/384], Loss: 91.4536\n",
      "Epoch [27/300], Step [240/384], Loss: 98.1848\n",
      "Epoch [27/300], Step [260/384], Loss: 15.9609\n",
      "Epoch [27/300], Step [280/384], Loss: 230.5779\n",
      "Epoch [27/300], Step [300/384], Loss: 0.0001\n",
      "Epoch [27/300], Step [320/384], Loss: 86.4329\n",
      "Epoch [27/300], Step [340/384], Loss: 83.6172\n",
      "Epoch [27/300], Step [360/384], Loss: 94.5831\n",
      "Epoch [27/300], Step [380/384], Loss: 96.3718\n",
      "Epoch [28/300], Step [20/384], Loss: 151.8625\n",
      "Epoch [28/300], Step [40/384], Loss: 227.7462\n",
      "Epoch [28/300], Step [60/384], Loss: 232.9529\n",
      "Epoch [28/300], Step [80/384], Loss: 24.3610\n",
      "Epoch [28/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [28/300], Step [120/384], Loss: 109.9825\n",
      "Epoch [28/300], Step [140/384], Loss: 344.7684\n",
      "Epoch [28/300], Step [160/384], Loss: 33.4232\n",
      "Epoch [28/300], Step [180/384], Loss: 111.6733\n",
      "Epoch [28/300], Step [200/384], Loss: 94.7195\n",
      "Epoch [28/300], Step [220/384], Loss: 172.9983\n",
      "Epoch [28/300], Step [240/384], Loss: 113.3232\n",
      "Epoch [28/300], Step [260/384], Loss: 469.4404\n",
      "Epoch [28/300], Step [280/384], Loss: 184.3857\n",
      "Epoch [28/300], Step [300/384], Loss: 5.7405\n",
      "Epoch [28/300], Step [320/384], Loss: 131.8803\n",
      "Epoch [28/300], Step [340/384], Loss: 179.2761\n",
      "Epoch [28/300], Step [360/384], Loss: 28.5501\n",
      "Epoch [28/300], Step [380/384], Loss: 197.9741\n",
      "Epoch [29/300], Step [20/384], Loss: 119.5774\n",
      "Epoch [29/300], Step [40/384], Loss: 95.1705\n",
      "Epoch [29/300], Step [60/384], Loss: 33.9871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/300], Step [80/384], Loss: 257.8250\n",
      "Epoch [29/300], Step [100/384], Loss: 161.9098\n",
      "Epoch [29/300], Step [120/384], Loss: 12.4032\n",
      "Epoch [29/300], Step [140/384], Loss: 231.7072\n",
      "Epoch [29/300], Step [160/384], Loss: 6.7877\n",
      "Epoch [29/300], Step [180/384], Loss: 320.6642\n",
      "Epoch [29/300], Step [200/384], Loss: 67.3881\n",
      "Epoch [29/300], Step [220/384], Loss: 67.6325\n",
      "Epoch [29/300], Step [240/384], Loss: 196.2690\n",
      "Epoch [29/300], Step [260/384], Loss: 144.8867\n",
      "Epoch [29/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [29/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [29/300], Step [320/384], Loss: 106.7221\n",
      "Epoch [29/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [29/300], Step [360/384], Loss: 143.5776\n",
      "Epoch [29/300], Step [380/384], Loss: 307.0196\n",
      "Epoch [30/300], Step [20/384], Loss: 18.9656\n",
      "Epoch [30/300], Step [40/384], Loss: 14.8899\n",
      "Epoch [30/300], Step [60/384], Loss: 88.0900\n",
      "Epoch [30/300], Step [80/384], Loss: 65.4928\n",
      "Epoch [30/300], Step [100/384], Loss: 91.9470\n",
      "Epoch [30/300], Step [120/384], Loss: 69.9266\n",
      "Epoch [30/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [30/300], Step [160/384], Loss: 84.5350\n",
      "Epoch [30/300], Step [180/384], Loss: 145.1059\n",
      "Epoch [30/300], Step [200/384], Loss: 130.3328\n",
      "Epoch [30/300], Step [220/384], Loss: 141.2254\n",
      "Epoch [30/300], Step [240/384], Loss: 155.2219\n",
      "Epoch [30/300], Step [260/384], Loss: 116.8571\n",
      "Epoch [30/300], Step [280/384], Loss: 53.3687\n",
      "Epoch [30/300], Step [300/384], Loss: 25.4981\n",
      "Epoch [30/300], Step [320/384], Loss: 102.6812\n",
      "Epoch [30/300], Step [340/384], Loss: 104.4010\n",
      "Epoch [30/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [30/300], Step [380/384], Loss: 317.2103\n",
      "Epoch [31/300], Step [20/384], Loss: 72.9750\n",
      "Epoch [31/300], Step [40/384], Loss: 150.3014\n",
      "Epoch [31/300], Step [60/384], Loss: 140.0467\n",
      "Epoch [31/300], Step [80/384], Loss: 109.7776\n",
      "Epoch [31/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [31/300], Step [120/384], Loss: 15.8643\n",
      "Epoch [31/300], Step [140/384], Loss: 0.1355\n",
      "Epoch [31/300], Step [160/384], Loss: 102.7873\n",
      "Epoch [31/300], Step [180/384], Loss: 56.4608\n",
      "Epoch [31/300], Step [200/384], Loss: 208.8964\n",
      "Epoch [31/300], Step [220/384], Loss: 267.1677\n",
      "Epoch [31/300], Step [240/384], Loss: 66.0787\n",
      "Epoch [31/300], Step [260/384], Loss: 196.8488\n",
      "Epoch [31/300], Step [280/384], Loss: 17.2387\n",
      "Epoch [31/300], Step [300/384], Loss: 184.6409\n",
      "Epoch [31/300], Step [320/384], Loss: 61.9558\n",
      "Epoch [31/300], Step [340/384], Loss: 156.0003\n",
      "Epoch [31/300], Step [360/384], Loss: 71.6517\n",
      "Epoch [31/300], Step [380/384], Loss: 190.8963\n",
      "Epoch [32/300], Step [20/384], Loss: 88.7198\n",
      "Epoch [32/300], Step [40/384], Loss: 43.5977\n",
      "Epoch [32/300], Step [60/384], Loss: 60.5862\n",
      "Epoch [32/300], Step [80/384], Loss: 4.7817\n",
      "Epoch [32/300], Step [100/384], Loss: 122.0228\n",
      "Epoch [32/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [32/300], Step [140/384], Loss: 31.6111\n",
      "Epoch [32/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [32/300], Step [180/384], Loss: 306.2880\n",
      "Epoch [32/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [32/300], Step [220/384], Loss: 79.5067\n",
      "Epoch [32/300], Step [240/384], Loss: 31.9818\n",
      "Epoch [32/300], Step [260/384], Loss: 179.4103\n",
      "Epoch [32/300], Step [280/384], Loss: 40.4783\n",
      "Epoch [32/300], Step [300/384], Loss: 161.4251\n",
      "Epoch [32/300], Step [320/384], Loss: 5.0346\n",
      "Epoch [32/300], Step [340/384], Loss: 241.0260\n",
      "Epoch [32/300], Step [360/384], Loss: 187.7399\n",
      "Epoch [32/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [33/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [33/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [33/300], Step [60/384], Loss: 8.5562\n",
      "Epoch [33/300], Step [80/384], Loss: 105.8519\n",
      "Epoch [33/300], Step [100/384], Loss: 257.9515\n",
      "Epoch [33/300], Step [120/384], Loss: 6.8573\n",
      "Epoch [33/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [33/300], Step [160/384], Loss: 111.2326\n",
      "Epoch [33/300], Step [180/384], Loss: 186.1254\n",
      "Epoch [33/300], Step [200/384], Loss: 32.0614\n",
      "Epoch [33/300], Step [220/384], Loss: 133.5048\n",
      "Epoch [33/300], Step [240/384], Loss: 113.6908\n",
      "Epoch [33/300], Step [260/384], Loss: 94.3877\n",
      "Epoch [33/300], Step [280/384], Loss: 117.3197\n",
      "Epoch [33/300], Step [300/384], Loss: 105.2714\n",
      "Epoch [33/300], Step [320/384], Loss: 113.1846\n",
      "Epoch [33/300], Step [340/384], Loss: 225.6490\n",
      "Epoch [33/300], Step [360/384], Loss: 42.5283\n",
      "Epoch [33/300], Step [380/384], Loss: 147.1091\n",
      "Epoch [34/300], Step [20/384], Loss: 3.3497\n",
      "Epoch [34/300], Step [40/384], Loss: 10.5878\n",
      "Epoch [34/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [34/300], Step [80/384], Loss: 41.0120\n",
      "Epoch [34/300], Step [100/384], Loss: 69.6530\n",
      "Epoch [34/300], Step [120/384], Loss: 89.6641\n",
      "Epoch [34/300], Step [140/384], Loss: 70.1198\n",
      "Epoch [34/300], Step [160/384], Loss: 38.0585\n",
      "Epoch [34/300], Step [180/384], Loss: 275.2012\n",
      "Epoch [34/300], Step [200/384], Loss: 297.0486\n",
      "Epoch [34/300], Step [220/384], Loss: 25.6077\n",
      "Epoch [34/300], Step [240/384], Loss: 60.7206\n",
      "Epoch [34/300], Step [260/384], Loss: 220.7108\n",
      "Epoch [34/300], Step [280/384], Loss: 73.0816\n",
      "Epoch [34/300], Step [300/384], Loss: 147.9256\n",
      "Epoch [34/300], Step [320/384], Loss: 102.3729\n",
      "Epoch [34/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [34/300], Step [360/384], Loss: 117.9923\n",
      "Epoch [34/300], Step [380/384], Loss: 487.5767\n",
      "Epoch [35/300], Step [20/384], Loss: 138.6015\n",
      "Epoch [35/300], Step [40/384], Loss: 260.3244\n",
      "Epoch [35/300], Step [60/384], Loss: 270.0410\n",
      "Epoch [35/300], Step [80/384], Loss: 159.8192\n",
      "Epoch [35/300], Step [100/384], Loss: 400.3156\n",
      "Epoch [35/300], Step [120/384], Loss: 124.7187\n",
      "Epoch [35/300], Step [140/384], Loss: 29.3846\n",
      "Epoch [35/300], Step [160/384], Loss: 326.3336\n",
      "Epoch [35/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [35/300], Step [200/384], Loss: 187.7767\n",
      "Epoch [35/300], Step [220/384], Loss: 60.1612\n",
      "Epoch [35/300], Step [240/384], Loss: 128.3257\n",
      "Epoch [35/300], Step [260/384], Loss: 127.3537\n",
      "Epoch [35/300], Step [280/384], Loss: 36.7064\n",
      "Epoch [35/300], Step [300/384], Loss: 142.3373\n",
      "Epoch [35/300], Step [320/384], Loss: 15.4772\n",
      "Epoch [35/300], Step [340/384], Loss: 27.2524\n",
      "Epoch [35/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [35/300], Step [380/384], Loss: 202.8041\n",
      "Epoch [36/300], Step [20/384], Loss: 180.4080\n",
      "Epoch [36/300], Step [40/384], Loss: 26.1095\n",
      "Epoch [36/300], Step [60/384], Loss: 5.3419\n",
      "Epoch [36/300], Step [80/384], Loss: 54.3517\n",
      "Epoch [36/300], Step [100/384], Loss: 9.3334\n",
      "Epoch [36/300], Step [120/384], Loss: 151.1285\n",
      "Epoch [36/300], Step [140/384], Loss: 101.2521\n",
      "Epoch [36/300], Step [160/384], Loss: 20.4565\n",
      "Epoch [36/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [36/300], Step [200/384], Loss: 72.1639\n",
      "Epoch [36/300], Step [220/384], Loss: 133.2766\n",
      "Epoch [36/300], Step [240/384], Loss: 202.9572\n",
      "Epoch [36/300], Step [260/384], Loss: 213.7754\n",
      "Epoch [36/300], Step [280/384], Loss: 11.9865\n",
      "Epoch [36/300], Step [300/384], Loss: 19.0259\n",
      "Epoch [36/300], Step [320/384], Loss: 219.0126\n",
      "Epoch [36/300], Step [340/384], Loss: 92.1379\n",
      "Epoch [36/300], Step [360/384], Loss: 195.6929\n",
      "Epoch [36/300], Step [380/384], Loss: 295.5316\n",
      "Epoch [37/300], Step [20/384], Loss: 163.1707\n",
      "Epoch [37/300], Step [40/384], Loss: 86.1668\n",
      "Epoch [37/300], Step [60/384], Loss: 0.0002\n",
      "Epoch [37/300], Step [80/384], Loss: 99.5007\n",
      "Epoch [37/300], Step [100/384], Loss: 66.7284\n",
      "Epoch [37/300], Step [120/384], Loss: 101.6274\n",
      "Epoch [37/300], Step [140/384], Loss: 6.0114\n",
      "Epoch [37/300], Step [160/384], Loss: 318.1346\n",
      "Epoch [37/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [37/300], Step [200/384], Loss: 46.3632\n",
      "Epoch [37/300], Step [220/384], Loss: 44.9009\n",
      "Epoch [37/300], Step [240/384], Loss: 115.2408\n",
      "Epoch [37/300], Step [260/384], Loss: 20.4895\n",
      "Epoch [37/300], Step [280/384], Loss: 57.0389\n",
      "Epoch [37/300], Step [300/384], Loss: 144.8847\n",
      "Epoch [37/300], Step [320/384], Loss: 275.6130\n",
      "Epoch [37/300], Step [340/384], Loss: 50.1443\n",
      "Epoch [37/300], Step [360/384], Loss: 8.6799\n",
      "Epoch [37/300], Step [380/384], Loss: 267.7484\n",
      "Epoch [38/300], Step [20/384], Loss: 91.1939\n",
      "Epoch [38/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [38/300], Step [60/384], Loss: 88.0874\n",
      "Epoch [38/300], Step [80/384], Loss: 293.6462\n",
      "Epoch [38/300], Step [100/384], Loss: 1.6872\n",
      "Epoch [38/300], Step [120/384], Loss: 158.2612\n",
      "Epoch [38/300], Step [140/384], Loss: 304.5048\n",
      "Epoch [38/300], Step [160/384], Loss: 329.3831\n",
      "Epoch [38/300], Step [180/384], Loss: 6.6390\n",
      "Epoch [38/300], Step [200/384], Loss: 44.8620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/300], Step [220/384], Loss: 125.0580\n",
      "Epoch [38/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [38/300], Step [260/384], Loss: 102.9547\n",
      "Epoch [38/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [38/300], Step [300/384], Loss: 11.6156\n",
      "Epoch [38/300], Step [320/384], Loss: 107.9752\n",
      "Epoch [38/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [38/300], Step [360/384], Loss: 628.1061\n",
      "Epoch [38/300], Step [380/384], Loss: 0.0001\n",
      "Epoch [39/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [39/300], Step [40/384], Loss: 98.0942\n",
      "Epoch [39/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [39/300], Step [80/384], Loss: 362.2211\n",
      "Epoch [39/300], Step [100/384], Loss: 109.0098\n",
      "Epoch [39/300], Step [120/384], Loss: 24.3264\n",
      "Epoch [39/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [39/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [39/300], Step [180/384], Loss: 58.9440\n",
      "Epoch [39/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [39/300], Step [220/384], Loss: 8.4491\n",
      "Epoch [39/300], Step [240/384], Loss: 161.5174\n",
      "Epoch [39/300], Step [260/384], Loss: 166.7390\n",
      "Epoch [39/300], Step [280/384], Loss: 446.8716\n",
      "Epoch [39/300], Step [300/384], Loss: 181.9461\n",
      "Epoch [39/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [39/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [39/300], Step [360/384], Loss: 5.7845\n",
      "Epoch [39/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [40/300], Step [20/384], Loss: 307.6023\n",
      "Epoch [40/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [40/300], Step [60/384], Loss: 212.0967\n",
      "Epoch [40/300], Step [80/384], Loss: 115.1640\n",
      "Epoch [40/300], Step [100/384], Loss: 32.2064\n",
      "Epoch [40/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [40/300], Step [140/384], Loss: 77.8825\n",
      "Epoch [40/300], Step [160/384], Loss: 160.0981\n",
      "Epoch [40/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [40/300], Step [200/384], Loss: 138.9784\n",
      "Epoch [40/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [40/300], Step [240/384], Loss: 10.9485\n",
      "Epoch [40/300], Step [260/384], Loss: 0.0109\n",
      "Epoch [40/300], Step [280/384], Loss: 129.6023\n",
      "Epoch [40/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [40/300], Step [320/384], Loss: 161.3838\n",
      "Epoch [40/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [40/300], Step [360/384], Loss: 131.6275\n",
      "Epoch [40/300], Step [380/384], Loss: 147.7845\n",
      "Epoch [41/300], Step [20/384], Loss: 121.6972\n",
      "Epoch [41/300], Step [40/384], Loss: 258.6354\n",
      "Epoch [41/300], Step [60/384], Loss: 207.0736\n",
      "Epoch [41/300], Step [80/384], Loss: 315.5312\n",
      "Epoch [41/300], Step [100/384], Loss: 62.7190\n",
      "Epoch [41/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [41/300], Step [140/384], Loss: 76.9172\n",
      "Epoch [41/300], Step [160/384], Loss: 110.7868\n",
      "Epoch [41/300], Step [180/384], Loss: 53.8324\n",
      "Epoch [41/300], Step [200/384], Loss: 72.4736\n",
      "Epoch [41/300], Step [220/384], Loss: 87.0483\n",
      "Epoch [41/300], Step [240/384], Loss: 55.1106\n",
      "Epoch [41/300], Step [260/384], Loss: 261.5837\n",
      "Epoch [41/300], Step [280/384], Loss: 110.3416\n",
      "Epoch [41/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [41/300], Step [320/384], Loss: 70.0456\n",
      "Epoch [41/300], Step [340/384], Loss: 3.3458\n",
      "Epoch [41/300], Step [360/384], Loss: 42.2118\n",
      "Epoch [41/300], Step [380/384], Loss: 7.2172\n",
      "Epoch [42/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [42/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [42/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [42/300], Step [80/384], Loss: 62.9235\n",
      "Epoch [42/300], Step [100/384], Loss: 103.3883\n",
      "Epoch [42/300], Step [120/384], Loss: 181.9811\n",
      "Epoch [42/300], Step [140/384], Loss: 227.5811\n",
      "Epoch [42/300], Step [160/384], Loss: 72.1510\n",
      "Epoch [42/300], Step [180/384], Loss: 287.2948\n",
      "Epoch [42/300], Step [200/384], Loss: 14.6529\n",
      "Epoch [42/300], Step [220/384], Loss: 57.0674\n",
      "Epoch [42/300], Step [240/384], Loss: 308.4879\n",
      "Epoch [42/300], Step [260/384], Loss: 41.6243\n",
      "Epoch [42/300], Step [280/384], Loss: 5.5431\n",
      "Epoch [42/300], Step [300/384], Loss: 7.9970\n",
      "Epoch [42/300], Step [320/384], Loss: 325.1140\n",
      "Epoch [42/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [42/300], Step [360/384], Loss: 167.2375\n",
      "Epoch [42/300], Step [380/384], Loss: 157.4824\n",
      "Epoch [43/300], Step [20/384], Loss: 95.9203\n",
      "Epoch [43/300], Step [40/384], Loss: 49.0786\n",
      "Epoch [43/300], Step [60/384], Loss: 55.2254\n",
      "Epoch [43/300], Step [80/384], Loss: 73.8141\n",
      "Epoch [43/300], Step [100/384], Loss: 93.8285\n",
      "Epoch [43/300], Step [120/384], Loss: 40.3857\n",
      "Epoch [43/300], Step [140/384], Loss: 0.0006\n",
      "Epoch [43/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [43/300], Step [180/384], Loss: 79.9590\n",
      "Epoch [43/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [43/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [43/300], Step [240/384], Loss: 53.2976\n",
      "Epoch [43/300], Step [260/384], Loss: 75.7803\n",
      "Epoch [43/300], Step [280/384], Loss: 33.5086\n",
      "Epoch [43/300], Step [300/384], Loss: 148.1070\n",
      "Epoch [43/300], Step [320/384], Loss: 50.5333\n",
      "Epoch [43/300], Step [340/384], Loss: 117.2775\n",
      "Epoch [43/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [43/300], Step [380/384], Loss: 74.7501\n",
      "Epoch [44/300], Step [20/384], Loss: 30.0282\n",
      "Epoch [44/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [44/300], Step [60/384], Loss: 65.3431\n",
      "Epoch [44/300], Step [80/384], Loss: 380.3859\n",
      "Epoch [44/300], Step [100/384], Loss: 149.8314\n",
      "Epoch [44/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [44/300], Step [140/384], Loss: 90.0505\n",
      "Epoch [44/300], Step [160/384], Loss: 159.4499\n",
      "Epoch [44/300], Step [180/384], Loss: 5.5686\n",
      "Epoch [44/300], Step [200/384], Loss: 25.9576\n",
      "Epoch [44/300], Step [220/384], Loss: 91.2832\n",
      "Epoch [44/300], Step [240/384], Loss: 28.8238\n",
      "Epoch [44/300], Step [260/384], Loss: 23.8051\n",
      "Epoch [44/300], Step [280/384], Loss: 35.0746\n",
      "Epoch [44/300], Step [300/384], Loss: 27.6379\n",
      "Epoch [44/300], Step [320/384], Loss: 91.7633\n",
      "Epoch [44/300], Step [340/384], Loss: 115.0102\n",
      "Epoch [44/300], Step [360/384], Loss: 153.4454\n",
      "Epoch [44/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [45/300], Step [20/384], Loss: 132.5942\n",
      "Epoch [45/300], Step [40/384], Loss: 30.5013\n",
      "Epoch [45/300], Step [60/384], Loss: 59.3860\n",
      "Epoch [45/300], Step [80/384], Loss: 153.9508\n",
      "Epoch [45/300], Step [100/384], Loss: 124.4641\n",
      "Epoch [45/300], Step [120/384], Loss: 471.2003\n",
      "Epoch [45/300], Step [140/384], Loss: 154.7044\n",
      "Epoch [45/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [45/300], Step [180/384], Loss: 78.9253\n",
      "Epoch [45/300], Step [200/384], Loss: 40.8703\n",
      "Epoch [45/300], Step [220/384], Loss: 186.0222\n",
      "Epoch [45/300], Step [240/384], Loss: 124.6547\n",
      "Epoch [45/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [45/300], Step [280/384], Loss: 86.3708\n",
      "Epoch [45/300], Step [300/384], Loss: 315.8322\n",
      "Epoch [45/300], Step [320/384], Loss: 64.0735\n",
      "Epoch [45/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [45/300], Step [360/384], Loss: 219.9309\n",
      "Epoch [45/300], Step [380/384], Loss: 101.9434\n",
      "Epoch [46/300], Step [20/384], Loss: 429.8867\n",
      "Epoch [46/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [46/300], Step [60/384], Loss: 148.3350\n",
      "Epoch [46/300], Step [80/384], Loss: 141.2552\n",
      "Epoch [46/300], Step [100/384], Loss: 87.6016\n",
      "Epoch [46/300], Step [120/384], Loss: 138.7081\n",
      "Epoch [46/300], Step [140/384], Loss: 64.3568\n",
      "Epoch [46/300], Step [160/384], Loss: 228.2247\n",
      "Epoch [46/300], Step [180/384], Loss: 110.4064\n",
      "Epoch [46/300], Step [200/384], Loss: 316.6724\n",
      "Epoch [46/300], Step [220/384], Loss: 109.1551\n",
      "Epoch [46/300], Step [240/384], Loss: 53.4269\n",
      "Epoch [46/300], Step [260/384], Loss: 142.9789\n",
      "Epoch [46/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [46/300], Step [300/384], Loss: 78.8186\n",
      "Epoch [46/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [46/300], Step [340/384], Loss: 315.3385\n",
      "Epoch [46/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [46/300], Step [380/384], Loss: 76.4834\n",
      "Epoch [47/300], Step [20/384], Loss: 321.2791\n",
      "Epoch [47/300], Step [40/384], Loss: 61.1465\n",
      "Epoch [47/300], Step [60/384], Loss: 152.0431\n",
      "Epoch [47/300], Step [80/384], Loss: 0.0825\n",
      "Epoch [47/300], Step [100/384], Loss: 169.8571\n",
      "Epoch [47/300], Step [120/384], Loss: 16.5633\n",
      "Epoch [47/300], Step [140/384], Loss: 133.3089\n",
      "Epoch [47/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [47/300], Step [180/384], Loss: 148.5605\n",
      "Epoch [47/300], Step [200/384], Loss: 20.5748\n",
      "Epoch [47/300], Step [220/384], Loss: 118.8211\n",
      "Epoch [47/300], Step [240/384], Loss: 144.9861\n",
      "Epoch [47/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [47/300], Step [280/384], Loss: 147.8470\n",
      "Epoch [47/300], Step [300/384], Loss: 41.8039\n",
      "Epoch [47/300], Step [320/384], Loss: 129.3103\n",
      "Epoch [47/300], Step [340/384], Loss: 145.5976\n",
      "Epoch [47/300], Step [360/384], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/300], Step [380/384], Loss: 55.6347\n",
      "Epoch [48/300], Step [20/384], Loss: 30.1338\n",
      "Epoch [48/300], Step [40/384], Loss: 112.0056\n",
      "Epoch [48/300], Step [60/384], Loss: 22.2891\n",
      "Epoch [48/300], Step [80/384], Loss: 34.7957\n",
      "Epoch [48/300], Step [100/384], Loss: 37.1973\n",
      "Epoch [48/300], Step [120/384], Loss: 50.7145\n",
      "Epoch [48/300], Step [140/384], Loss: 75.5331\n",
      "Epoch [48/300], Step [160/384], Loss: 29.3721\n",
      "Epoch [48/300], Step [180/384], Loss: 104.0832\n",
      "Epoch [48/300], Step [200/384], Loss: 450.2816\n",
      "Epoch [48/300], Step [220/384], Loss: 95.4420\n",
      "Epoch [48/300], Step [240/384], Loss: 40.4856\n",
      "Epoch [48/300], Step [260/384], Loss: 124.1520\n",
      "Epoch [48/300], Step [280/384], Loss: 246.4839\n",
      "Epoch [48/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [48/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [48/300], Step [340/384], Loss: 96.2543\n",
      "Epoch [48/300], Step [360/384], Loss: 36.3025\n",
      "Epoch [48/300], Step [380/384], Loss: 63.7787\n",
      "Epoch [49/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [49/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [49/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [49/300], Step [80/384], Loss: 78.8578\n",
      "Epoch [49/300], Step [100/384], Loss: 37.3237\n",
      "Epoch [49/300], Step [120/384], Loss: 32.2312\n",
      "Epoch [49/300], Step [140/384], Loss: 6.4895\n",
      "Epoch [49/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [49/300], Step [180/384], Loss: 2.3127\n",
      "Epoch [49/300], Step [200/384], Loss: 15.8454\n",
      "Epoch [49/300], Step [220/384], Loss: 154.6017\n",
      "Epoch [49/300], Step [240/384], Loss: 3.0957\n",
      "Epoch [49/300], Step [260/384], Loss: 300.2104\n",
      "Epoch [49/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [49/300], Step [300/384], Loss: 109.7878\n",
      "Epoch [49/300], Step [320/384], Loss: 45.3584\n",
      "Epoch [49/300], Step [340/384], Loss: 18.7804\n",
      "Epoch [49/300], Step [360/384], Loss: 24.9458\n",
      "Epoch [49/300], Step [380/384], Loss: 41.1900\n",
      "Epoch [50/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [50/300], Step [40/384], Loss: 82.7377\n",
      "Epoch [50/300], Step [60/384], Loss: 257.8342\n",
      "Epoch [50/300], Step [80/384], Loss: 40.5056\n",
      "Epoch [50/300], Step [100/384], Loss: 84.2535\n",
      "Epoch [50/300], Step [120/384], Loss: 117.9198\n",
      "Epoch [50/300], Step [140/384], Loss: 138.1550\n",
      "Epoch [50/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [50/300], Step [180/384], Loss: 81.2297\n",
      "Epoch [50/300], Step [200/384], Loss: 55.2515\n",
      "Epoch [50/300], Step [220/384], Loss: 65.4224\n",
      "Epoch [50/300], Step [240/384], Loss: 96.3130\n",
      "Epoch [50/300], Step [260/384], Loss: 146.7942\n",
      "Epoch [50/300], Step [280/384], Loss: 22.4308\n",
      "Epoch [50/300], Step [300/384], Loss: 198.9269\n",
      "Epoch [50/300], Step [320/384], Loss: 36.4783\n",
      "Epoch [50/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [50/300], Step [360/384], Loss: 1.7592\n",
      "Epoch [50/300], Step [380/384], Loss: 75.3250\n",
      "Epoch [51/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [51/300], Step [40/384], Loss: 84.1631\n",
      "Epoch [51/300], Step [60/384], Loss: 197.6818\n",
      "Epoch [51/300], Step [80/384], Loss: 58.0499\n",
      "Epoch [51/300], Step [100/384], Loss: 70.8186\n",
      "Epoch [51/300], Step [120/384], Loss: 161.1505\n",
      "Epoch [51/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [51/300], Step [160/384], Loss: 73.0955\n",
      "Epoch [51/300], Step [180/384], Loss: 100.5811\n",
      "Epoch [51/300], Step [200/384], Loss: 183.3816\n",
      "Epoch [51/300], Step [220/384], Loss: 453.9019\n",
      "Epoch [51/300], Step [240/384], Loss: 36.9826\n",
      "Epoch [51/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [51/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [51/300], Step [300/384], Loss: 203.7511\n",
      "Epoch [51/300], Step [320/384], Loss: 9.5759\n",
      "Epoch [51/300], Step [340/384], Loss: 207.8572\n",
      "Epoch [51/300], Step [360/384], Loss: 49.3606\n",
      "Epoch [51/300], Step [380/384], Loss: 0.0016\n",
      "Epoch [52/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [52/300], Step [40/384], Loss: 2.2832\n",
      "Epoch [52/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [52/300], Step [80/384], Loss: 153.9470\n",
      "Epoch [52/300], Step [100/384], Loss: 92.8232\n",
      "Epoch [52/300], Step [120/384], Loss: 270.2037\n",
      "Epoch [52/300], Step [140/384], Loss: 0.0001\n",
      "Epoch [52/300], Step [160/384], Loss: 166.1202\n",
      "Epoch [52/300], Step [180/384], Loss: 43.9027\n",
      "Epoch [52/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [52/300], Step [220/384], Loss: 104.9897\n",
      "Epoch [52/300], Step [240/384], Loss: 22.1745\n",
      "Epoch [52/300], Step [260/384], Loss: 185.8336\n",
      "Epoch [52/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [52/300], Step [300/384], Loss: 166.3181\n",
      "Epoch [52/300], Step [320/384], Loss: 129.0204\n",
      "Epoch [52/300], Step [340/384], Loss: 0.0686\n",
      "Epoch [52/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [52/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [53/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [53/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [53/300], Step [60/384], Loss: 334.9306\n",
      "Epoch [53/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [53/300], Step [100/384], Loss: 102.6333\n",
      "Epoch [53/300], Step [120/384], Loss: 106.9947\n",
      "Epoch [53/300], Step [140/384], Loss: 140.7551\n",
      "Epoch [53/300], Step [160/384], Loss: 38.4290\n",
      "Epoch [53/300], Step [180/384], Loss: 92.2988\n",
      "Epoch [53/300], Step [200/384], Loss: 424.5173\n",
      "Epoch [53/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [53/300], Step [240/384], Loss: 42.4011\n",
      "Epoch [53/300], Step [260/384], Loss: 8.6573\n",
      "Epoch [53/300], Step [280/384], Loss: 99.6300\n",
      "Epoch [53/300], Step [300/384], Loss: 101.5575\n",
      "Epoch [53/300], Step [320/384], Loss: 16.0357\n",
      "Epoch [53/300], Step [340/384], Loss: 49.7614\n",
      "Epoch [53/300], Step [360/384], Loss: 179.6669\n",
      "Epoch [53/300], Step [380/384], Loss: 229.6108\n",
      "Epoch [54/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [54/300], Step [40/384], Loss: 1.5458\n",
      "Epoch [54/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [54/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [54/300], Step [100/384], Loss: 123.0071\n",
      "Epoch [54/300], Step [120/384], Loss: 21.3090\n",
      "Epoch [54/300], Step [140/384], Loss: 75.2058\n",
      "Epoch [54/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [54/300], Step [180/384], Loss: 95.0709\n",
      "Epoch [54/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [54/300], Step [220/384], Loss: 84.0615\n",
      "Epoch [54/300], Step [240/384], Loss: 138.8819\n",
      "Epoch [54/300], Step [260/384], Loss: 94.2608\n",
      "Epoch [54/300], Step [280/384], Loss: 155.3508\n",
      "Epoch [54/300], Step [300/384], Loss: 53.2630\n",
      "Epoch [54/300], Step [320/384], Loss: 84.1573\n",
      "Epoch [54/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [54/300], Step [360/384], Loss: 199.1540\n",
      "Epoch [54/300], Step [380/384], Loss: 75.0099\n",
      "Epoch [55/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [55/300], Step [40/384], Loss: 38.7155\n",
      "Epoch [55/300], Step [60/384], Loss: 101.9861\n",
      "Epoch [55/300], Step [80/384], Loss: 28.6357\n",
      "Epoch [55/300], Step [100/384], Loss: 48.7091\n",
      "Epoch [55/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [55/300], Step [140/384], Loss: 218.0234\n",
      "Epoch [55/300], Step [160/384], Loss: 200.8034\n",
      "Epoch [55/300], Step [180/384], Loss: 23.9385\n",
      "Epoch [55/300], Step [200/384], Loss: 193.5120\n",
      "Epoch [55/300], Step [220/384], Loss: 50.4818\n",
      "Epoch [55/300], Step [240/384], Loss: 21.5660\n",
      "Epoch [55/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [55/300], Step [280/384], Loss: 27.8417\n",
      "Epoch [55/300], Step [300/384], Loss: 54.0682\n",
      "Epoch [55/300], Step [320/384], Loss: 68.5840\n",
      "Epoch [55/300], Step [340/384], Loss: 35.9891\n",
      "Epoch [55/300], Step [360/384], Loss: 7.0874\n",
      "Epoch [55/300], Step [380/384], Loss: 57.8763\n",
      "Epoch [56/300], Step [20/384], Loss: 191.7871\n",
      "Epoch [56/300], Step [40/384], Loss: 28.3759\n",
      "Epoch [56/300], Step [60/384], Loss: 47.3752\n",
      "Epoch [56/300], Step [80/384], Loss: 77.3268\n",
      "Epoch [56/300], Step [100/384], Loss: 42.7876\n",
      "Epoch [56/300], Step [120/384], Loss: 80.5761\n",
      "Epoch [56/300], Step [140/384], Loss: 12.4520\n",
      "Epoch [56/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [56/300], Step [180/384], Loss: 84.8118\n",
      "Epoch [56/300], Step [200/384], Loss: 29.6915\n",
      "Epoch [56/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [56/300], Step [240/384], Loss: 103.0034\n",
      "Epoch [56/300], Step [260/384], Loss: 54.2277\n",
      "Epoch [56/300], Step [280/384], Loss: 49.2766\n",
      "Epoch [56/300], Step [300/384], Loss: 72.2779\n",
      "Epoch [56/300], Step [320/384], Loss: 40.7813\n",
      "Epoch [56/300], Step [340/384], Loss: 82.2994\n",
      "Epoch [56/300], Step [360/384], Loss: 65.6094\n",
      "Epoch [56/300], Step [380/384], Loss: 251.0393\n",
      "Epoch [57/300], Step [20/384], Loss: 2.1984\n",
      "Epoch [57/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [57/300], Step [60/384], Loss: 100.1609\n",
      "Epoch [57/300], Step [80/384], Loss: 72.8382\n",
      "Epoch [57/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [57/300], Step [120/384], Loss: 156.1157\n",
      "Epoch [57/300], Step [140/384], Loss: 12.0667\n",
      "Epoch [57/300], Step [160/384], Loss: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/300], Step [180/384], Loss: 102.4313\n",
      "Epoch [57/300], Step [200/384], Loss: 57.7094\n",
      "Epoch [57/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [57/300], Step [240/384], Loss: 6.2863\n",
      "Epoch [57/300], Step [260/384], Loss: 34.8714\n",
      "Epoch [57/300], Step [280/384], Loss: 77.2534\n",
      "Epoch [57/300], Step [300/384], Loss: 50.1218\n",
      "Epoch [57/300], Step [320/384], Loss: 82.4629\n",
      "Epoch [57/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [57/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [57/300], Step [380/384], Loss: 186.9247\n",
      "Epoch [58/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [58/300], Step [40/384], Loss: 48.5505\n",
      "Epoch [58/300], Step [60/384], Loss: 44.4882\n",
      "Epoch [58/300], Step [80/384], Loss: 11.7859\n",
      "Epoch [58/300], Step [100/384], Loss: 232.9289\n",
      "Epoch [58/300], Step [120/384], Loss: 83.7987\n",
      "Epoch [58/300], Step [140/384], Loss: 246.0247\n",
      "Epoch [58/300], Step [160/384], Loss: 8.1886\n",
      "Epoch [58/300], Step [180/384], Loss: 57.5824\n",
      "Epoch [58/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [58/300], Step [220/384], Loss: 189.4086\n",
      "Epoch [58/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [58/300], Step [260/384], Loss: 218.1467\n",
      "Epoch [58/300], Step [280/384], Loss: 283.4836\n",
      "Epoch [58/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [58/300], Step [320/384], Loss: 17.9073\n",
      "Epoch [58/300], Step [340/384], Loss: 180.1748\n",
      "Epoch [58/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [58/300], Step [380/384], Loss: 114.6713\n",
      "Epoch [59/300], Step [20/384], Loss: 2.4927\n",
      "Epoch [59/300], Step [40/384], Loss: 79.4808\n",
      "Epoch [59/300], Step [60/384], Loss: 9.3752\n",
      "Epoch [59/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [59/300], Step [100/384], Loss: 87.2032\n",
      "Epoch [59/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [59/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [59/300], Step [160/384], Loss: 104.0518\n",
      "Epoch [59/300], Step [180/384], Loss: 78.6653\n",
      "Epoch [59/300], Step [200/384], Loss: 40.4824\n",
      "Epoch [59/300], Step [220/384], Loss: 71.7725\n",
      "Epoch [59/300], Step [240/384], Loss: 46.4555\n",
      "Epoch [59/300], Step [260/384], Loss: 118.5781\n",
      "Epoch [59/300], Step [280/384], Loss: 53.3246\n",
      "Epoch [59/300], Step [300/384], Loss: 47.8846\n",
      "Epoch [59/300], Step [320/384], Loss: 38.7404\n",
      "Epoch [59/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [59/300], Step [360/384], Loss: 81.3120\n",
      "Epoch [59/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [60/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [60/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [60/300], Step [60/384], Loss: 79.1709\n",
      "Epoch [60/300], Step [80/384], Loss: 48.8955\n",
      "Epoch [60/300], Step [100/384], Loss: 47.0113\n",
      "Epoch [60/300], Step [120/384], Loss: 15.4591\n",
      "Epoch [60/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [60/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [60/300], Step [180/384], Loss: 89.0220\n",
      "Epoch [60/300], Step [200/384], Loss: 85.3666\n",
      "Epoch [60/300], Step [220/384], Loss: 47.3842\n",
      "Epoch [60/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [60/300], Step [260/384], Loss: 255.5572\n",
      "Epoch [60/300], Step [280/384], Loss: 228.0692\n",
      "Epoch [60/300], Step [300/384], Loss: 102.2633\n",
      "Epoch [60/300], Step [320/384], Loss: 46.9423\n",
      "Epoch [60/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [60/300], Step [360/384], Loss: 37.5340\n",
      "Epoch [60/300], Step [380/384], Loss: 7.6372\n",
      "Epoch [61/300], Step [20/384], Loss: 53.1964\n",
      "Epoch [61/300], Step [40/384], Loss: 11.3428\n",
      "Epoch [61/300], Step [60/384], Loss: 7.4894\n",
      "Epoch [61/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [61/300], Step [100/384], Loss: 9.6617\n",
      "Epoch [61/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [61/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [61/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [61/300], Step [180/384], Loss: 27.8426\n",
      "Epoch [61/300], Step [200/384], Loss: 76.6244\n",
      "Epoch [61/300], Step [220/384], Loss: 244.4476\n",
      "Epoch [61/300], Step [240/384], Loss: 34.3118\n",
      "Epoch [61/300], Step [260/384], Loss: 53.6960\n",
      "Epoch [61/300], Step [280/384], Loss: 87.5563\n",
      "Epoch [61/300], Step [300/384], Loss: 52.9033\n",
      "Epoch [61/300], Step [320/384], Loss: 7.5564\n",
      "Epoch [61/300], Step [340/384], Loss: 158.1003\n",
      "Epoch [61/300], Step [360/384], Loss: 236.0518\n",
      "Epoch [61/300], Step [380/384], Loss: 179.3501\n",
      "Epoch [62/300], Step [20/384], Loss: 31.0777\n",
      "Epoch [62/300], Step [40/384], Loss: 15.4982\n",
      "Epoch [62/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [62/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [62/300], Step [100/384], Loss: 216.9443\n",
      "Epoch [62/300], Step [120/384], Loss: 62.8801\n",
      "Epoch [62/300], Step [140/384], Loss: 81.3817\n",
      "Epoch [62/300], Step [160/384], Loss: 462.2114\n",
      "Epoch [62/300], Step [180/384], Loss: 23.3870\n",
      "Epoch [62/300], Step [200/384], Loss: 92.1099\n",
      "Epoch [62/300], Step [220/384], Loss: 207.2503\n",
      "Epoch [62/300], Step [240/384], Loss: 71.6001\n",
      "Epoch [62/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [62/300], Step [280/384], Loss: 188.9628\n",
      "Epoch [62/300], Step [300/384], Loss: 95.5296\n",
      "Epoch [62/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [62/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [62/300], Step [360/384], Loss: 17.6896\n",
      "Epoch [62/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [63/300], Step [20/384], Loss: 9.8153\n",
      "Epoch [63/300], Step [40/384], Loss: 62.4496\n",
      "Epoch [63/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [63/300], Step [80/384], Loss: 178.7128\n",
      "Epoch [63/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [63/300], Step [120/384], Loss: 122.1531\n",
      "Epoch [63/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [63/300], Step [160/384], Loss: 121.9469\n",
      "Epoch [63/300], Step [180/384], Loss: 10.9009\n",
      "Epoch [63/300], Step [200/384], Loss: 204.4181\n",
      "Epoch [63/300], Step [220/384], Loss: 54.6643\n",
      "Epoch [63/300], Step [240/384], Loss: 48.0028\n",
      "Epoch [63/300], Step [260/384], Loss: 3.1738\n",
      "Epoch [63/300], Step [280/384], Loss: 51.8823\n",
      "Epoch [63/300], Step [300/384], Loss: 40.6552\n",
      "Epoch [63/300], Step [320/384], Loss: 154.3153\n",
      "Epoch [63/300], Step [340/384], Loss: 104.3571\n",
      "Epoch [63/300], Step [360/384], Loss: 135.7605\n",
      "Epoch [63/300], Step [380/384], Loss: 58.2669\n",
      "Epoch [64/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [64/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [64/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [64/300], Step [80/384], Loss: 119.2351\n",
      "Epoch [64/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [64/300], Step [120/384], Loss: 22.1651\n",
      "Epoch [64/300], Step [140/384], Loss: 0.2478\n",
      "Epoch [64/300], Step [160/384], Loss: 59.9564\n",
      "Epoch [64/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [64/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [64/300], Step [220/384], Loss: 85.3310\n",
      "Epoch [64/300], Step [240/384], Loss: 36.0267\n",
      "Epoch [64/300], Step [260/384], Loss: 72.6099\n",
      "Epoch [64/300], Step [280/384], Loss: 72.3345\n",
      "Epoch [64/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [64/300], Step [320/384], Loss: 111.8250\n",
      "Epoch [64/300], Step [340/384], Loss: 1.2922\n",
      "Epoch [64/300], Step [360/384], Loss: 31.1898\n",
      "Epoch [64/300], Step [380/384], Loss: 169.5939\n",
      "Epoch [65/300], Step [20/384], Loss: 19.4605\n",
      "Epoch [65/300], Step [40/384], Loss: 22.8889\n",
      "Epoch [65/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [65/300], Step [80/384], Loss: 59.5593\n",
      "Epoch [65/300], Step [100/384], Loss: 151.7005\n",
      "Epoch [65/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [65/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [65/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [65/300], Step [180/384], Loss: 31.3905\n",
      "Epoch [65/300], Step [200/384], Loss: 59.6083\n",
      "Epoch [65/300], Step [220/384], Loss: 229.9875\n",
      "Epoch [65/300], Step [240/384], Loss: 126.8123\n",
      "Epoch [65/300], Step [260/384], Loss: 18.4873\n",
      "Epoch [65/300], Step [280/384], Loss: 61.5124\n",
      "Epoch [65/300], Step [300/384], Loss: 22.0437\n",
      "Epoch [65/300], Step [320/384], Loss: 64.6199\n",
      "Epoch [65/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [65/300], Step [360/384], Loss: 21.5718\n",
      "Epoch [65/300], Step [380/384], Loss: 78.7092\n",
      "Epoch [66/300], Step [20/384], Loss: 40.4845\n",
      "Epoch [66/300], Step [40/384], Loss: 43.4753\n",
      "Epoch [66/300], Step [60/384], Loss: 182.5287\n",
      "Epoch [66/300], Step [80/384], Loss: 211.7655\n",
      "Epoch [66/300], Step [100/384], Loss: 0.0006\n",
      "Epoch [66/300], Step [120/384], Loss: 313.3156\n",
      "Epoch [66/300], Step [140/384], Loss: 1.7596\n",
      "Epoch [66/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [66/300], Step [180/384], Loss: 28.2675\n",
      "Epoch [66/300], Step [200/384], Loss: 24.7020\n",
      "Epoch [66/300], Step [220/384], Loss: 137.9791\n",
      "Epoch [66/300], Step [240/384], Loss: 234.0736\n",
      "Epoch [66/300], Step [260/384], Loss: 37.8598\n",
      "Epoch [66/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [66/300], Step [300/384], Loss: 71.1363\n",
      "Epoch [66/300], Step [320/384], Loss: 334.8923\n",
      "Epoch [66/300], Step [340/384], Loss: 101.3881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/300], Step [360/384], Loss: 155.6901\n",
      "Epoch [66/300], Step [380/384], Loss: 68.0506\n",
      "Epoch [67/300], Step [20/384], Loss: 25.4169\n",
      "Epoch [67/300], Step [40/384], Loss: 139.4520\n",
      "Epoch [67/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [67/300], Step [80/384], Loss: 3.3155\n",
      "Epoch [67/300], Step [100/384], Loss: 115.2611\n",
      "Epoch [67/300], Step [120/384], Loss: 25.8489\n",
      "Epoch [67/300], Step [140/384], Loss: 49.5979\n",
      "Epoch [67/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [67/300], Step [180/384], Loss: 56.7750\n",
      "Epoch [67/300], Step [200/384], Loss: 80.5860\n",
      "Epoch [67/300], Step [220/384], Loss: 64.1458\n",
      "Epoch [67/300], Step [240/384], Loss: 183.0991\n",
      "Epoch [67/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [67/300], Step [280/384], Loss: 0.4935\n",
      "Epoch [67/300], Step [300/384], Loss: 93.8463\n",
      "Epoch [67/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [67/300], Step [340/384], Loss: 20.0808\n",
      "Epoch [67/300], Step [360/384], Loss: 69.5208\n",
      "Epoch [67/300], Step [380/384], Loss: 111.2256\n",
      "Epoch [68/300], Step [20/384], Loss: 180.4377\n",
      "Epoch [68/300], Step [40/384], Loss: 14.7224\n",
      "Epoch [68/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [68/300], Step [80/384], Loss: 86.6629\n",
      "Epoch [68/300], Step [100/384], Loss: 82.0386\n",
      "Epoch [68/300], Step [120/384], Loss: 258.0665\n",
      "Epoch [68/300], Step [140/384], Loss: 10.6867\n",
      "Epoch [68/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [68/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [68/300], Step [200/384], Loss: 47.8958\n",
      "Epoch [68/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [68/300], Step [240/384], Loss: 84.1368\n",
      "Epoch [68/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [68/300], Step [280/384], Loss: 33.9462\n",
      "Epoch [68/300], Step [300/384], Loss: 22.9050\n",
      "Epoch [68/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [68/300], Step [340/384], Loss: 218.9568\n",
      "Epoch [68/300], Step [360/384], Loss: 117.6458\n",
      "Epoch [68/300], Step [380/384], Loss: 211.3847\n",
      "Epoch [69/300], Step [20/384], Loss: 217.4095\n",
      "Epoch [69/300], Step [40/384], Loss: 62.9965\n",
      "Epoch [69/300], Step [60/384], Loss: 73.7112\n",
      "Epoch [69/300], Step [80/384], Loss: 206.7039\n",
      "Epoch [69/300], Step [100/384], Loss: 16.9983\n",
      "Epoch [69/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [69/300], Step [140/384], Loss: 362.7579\n",
      "Epoch [69/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [69/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [69/300], Step [200/384], Loss: 14.0285\n",
      "Epoch [69/300], Step [220/384], Loss: 113.7238\n",
      "Epoch [69/300], Step [240/384], Loss: 156.1603\n",
      "Epoch [69/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [69/300], Step [280/384], Loss: 50.3343\n",
      "Epoch [69/300], Step [300/384], Loss: 4.1363\n",
      "Epoch [69/300], Step [320/384], Loss: 1.3688\n",
      "Epoch [69/300], Step [340/384], Loss: 52.7865\n",
      "Epoch [69/300], Step [360/384], Loss: 41.6700\n",
      "Epoch [69/300], Step [380/384], Loss: 21.5326\n",
      "Epoch [70/300], Step [20/384], Loss: 134.8825\n",
      "Epoch [70/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [70/300], Step [60/384], Loss: 166.7563\n",
      "Epoch [70/300], Step [80/384], Loss: 5.5039\n",
      "Epoch [70/300], Step [100/384], Loss: 29.6732\n",
      "Epoch [70/300], Step [120/384], Loss: 143.3180\n",
      "Epoch [70/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [70/300], Step [160/384], Loss: 142.5186\n",
      "Epoch [70/300], Step [180/384], Loss: 36.6342\n",
      "Epoch [70/300], Step [200/384], Loss: 107.5929\n",
      "Epoch [70/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [70/300], Step [240/384], Loss: 146.7318\n",
      "Epoch [70/300], Step [260/384], Loss: 20.5739\n",
      "Epoch [70/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [70/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [70/300], Step [320/384], Loss: 150.6487\n",
      "Epoch [70/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [70/300], Step [360/384], Loss: 246.9895\n",
      "Epoch [70/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [71/300], Step [20/384], Loss: 10.2428\n",
      "Epoch [71/300], Step [40/384], Loss: 265.6096\n",
      "Epoch [71/300], Step [60/384], Loss: 23.5688\n",
      "Epoch [71/300], Step [80/384], Loss: 0.5093\n",
      "Epoch [71/300], Step [100/384], Loss: 91.9024\n",
      "Epoch [71/300], Step [120/384], Loss: 7.9107\n",
      "Epoch [71/300], Step [140/384], Loss: 110.3330\n",
      "Epoch [71/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [71/300], Step [180/384], Loss: 127.6235\n",
      "Epoch [71/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [71/300], Step [220/384], Loss: 127.9988\n",
      "Epoch [71/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [71/300], Step [260/384], Loss: 25.4400\n",
      "Epoch [71/300], Step [280/384], Loss: 51.4226\n",
      "Epoch [71/300], Step [300/384], Loss: 150.3018\n",
      "Epoch [71/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [71/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [71/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [71/300], Step [380/384], Loss: 51.8547\n",
      "Epoch [72/300], Step [20/384], Loss: 85.5165\n",
      "Epoch [72/300], Step [40/384], Loss: 157.3034\n",
      "Epoch [72/300], Step [60/384], Loss: 157.1334\n",
      "Epoch [72/300], Step [80/384], Loss: 169.8214\n",
      "Epoch [72/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [72/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [72/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [72/300], Step [160/384], Loss: 0.0732\n",
      "Epoch [72/300], Step [180/384], Loss: 128.4946\n",
      "Epoch [72/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [72/300], Step [220/384], Loss: 94.0845\n",
      "Epoch [72/300], Step [240/384], Loss: 1.4318\n",
      "Epoch [72/300], Step [260/384], Loss: 145.9271\n",
      "Epoch [72/300], Step [280/384], Loss: 80.7504\n",
      "Epoch [72/300], Step [300/384], Loss: 32.2113\n",
      "Epoch [72/300], Step [320/384], Loss: 93.5387\n",
      "Epoch [72/300], Step [340/384], Loss: 101.0822\n",
      "Epoch [72/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [72/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [73/300], Step [20/384], Loss: 87.2490\n",
      "Epoch [73/300], Step [40/384], Loss: 33.8326\n",
      "Epoch [73/300], Step [60/384], Loss: 34.1119\n",
      "Epoch [73/300], Step [80/384], Loss: 83.8204\n",
      "Epoch [73/300], Step [100/384], Loss: 81.0813\n",
      "Epoch [73/300], Step [120/384], Loss: 150.2840\n",
      "Epoch [73/300], Step [140/384], Loss: 53.8167\n",
      "Epoch [73/300], Step [160/384], Loss: 81.1233\n",
      "Epoch [73/300], Step [180/384], Loss: 32.2716\n",
      "Epoch [73/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [73/300], Step [220/384], Loss: 35.5802\n",
      "Epoch [73/300], Step [240/384], Loss: 151.2717\n",
      "Epoch [73/300], Step [260/384], Loss: 62.6097\n",
      "Epoch [73/300], Step [280/384], Loss: 37.6639\n",
      "Epoch [73/300], Step [300/384], Loss: 49.8131\n",
      "Epoch [73/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [73/300], Step [340/384], Loss: 90.6795\n",
      "Epoch [73/300], Step [360/384], Loss: 66.2374\n",
      "Epoch [73/300], Step [380/384], Loss: 119.7005\n",
      "Epoch [74/300], Step [20/384], Loss: 75.0303\n",
      "Epoch [74/300], Step [40/384], Loss: 75.1447\n",
      "Epoch [74/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [74/300], Step [80/384], Loss: 134.8909\n",
      "Epoch [74/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [74/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [74/300], Step [140/384], Loss: 24.8303\n",
      "Epoch [74/300], Step [160/384], Loss: 4.3941\n",
      "Epoch [74/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [74/300], Step [200/384], Loss: 81.8846\n",
      "Epoch [74/300], Step [220/384], Loss: 474.7761\n",
      "Epoch [74/300], Step [240/384], Loss: 94.7487\n",
      "Epoch [74/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [74/300], Step [280/384], Loss: 71.8345\n",
      "Epoch [74/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [74/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [74/300], Step [340/384], Loss: 1.4965\n",
      "Epoch [74/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [74/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [75/300], Step [20/384], Loss: 167.7974\n",
      "Epoch [75/300], Step [40/384], Loss: 13.0158\n",
      "Epoch [75/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [75/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [75/300], Step [100/384], Loss: 306.3192\n",
      "Epoch [75/300], Step [120/384], Loss: 85.9857\n",
      "Epoch [75/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [75/300], Step [160/384], Loss: 148.5405\n",
      "Epoch [75/300], Step [180/384], Loss: 37.9668\n",
      "Epoch [75/300], Step [200/384], Loss: 12.2244\n",
      "Epoch [75/300], Step [220/384], Loss: 104.1767\n",
      "Epoch [75/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [75/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [75/300], Step [280/384], Loss: 57.5877\n",
      "Epoch [75/300], Step [300/384], Loss: 0.0005\n",
      "Epoch [75/300], Step [320/384], Loss: 6.3309\n",
      "Epoch [75/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [75/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [75/300], Step [380/384], Loss: 43.9740\n",
      "Epoch [76/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [76/300], Step [40/384], Loss: 184.3599\n",
      "Epoch [76/300], Step [60/384], Loss: 45.7226\n",
      "Epoch [76/300], Step [80/384], Loss: 123.1090\n",
      "Epoch [76/300], Step [100/384], Loss: 110.7800\n",
      "Epoch [76/300], Step [120/384], Loss: 85.5767\n",
      "Epoch [76/300], Step [140/384], Loss: 21.7856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/300], Step [160/384], Loss: 6.0358\n",
      "Epoch [76/300], Step [180/384], Loss: 220.8714\n",
      "Epoch [76/300], Step [200/384], Loss: 29.0244\n",
      "Epoch [76/300], Step [220/384], Loss: 297.9139\n",
      "Epoch [76/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [76/300], Step [260/384], Loss: 108.7749\n",
      "Epoch [76/300], Step [280/384], Loss: 78.6718\n",
      "Epoch [76/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [76/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [76/300], Step [340/384], Loss: 49.3459\n",
      "Epoch [76/300], Step [360/384], Loss: 82.8215\n",
      "Epoch [76/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [77/300], Step [20/384], Loss: 15.5923\n",
      "Epoch [77/300], Step [40/384], Loss: 126.2380\n",
      "Epoch [77/300], Step [60/384], Loss: 106.5623\n",
      "Epoch [77/300], Step [80/384], Loss: 43.8139\n",
      "Epoch [77/300], Step [100/384], Loss: 14.3859\n",
      "Epoch [77/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [77/300], Step [140/384], Loss: 0.6506\n",
      "Epoch [77/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [77/300], Step [180/384], Loss: 25.6812\n",
      "Epoch [77/300], Step [200/384], Loss: 80.1728\n",
      "Epoch [77/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [77/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [77/300], Step [260/384], Loss: 124.5174\n",
      "Epoch [77/300], Step [280/384], Loss: 209.1363\n",
      "Epoch [77/300], Step [300/384], Loss: 115.2260\n",
      "Epoch [77/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [77/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [77/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [77/300], Step [380/384], Loss: 40.3294\n",
      "Epoch [78/300], Step [20/384], Loss: 208.6060\n",
      "Epoch [78/300], Step [40/384], Loss: 110.7069\n",
      "Epoch [78/300], Step [60/384], Loss: 59.4815\n",
      "Epoch [78/300], Step [80/384], Loss: 19.1605\n",
      "Epoch [78/300], Step [100/384], Loss: 195.8350\n",
      "Epoch [78/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [78/300], Step [140/384], Loss: 42.2350\n",
      "Epoch [78/300], Step [160/384], Loss: 50.6995\n",
      "Epoch [78/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [78/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [78/300], Step [220/384], Loss: 76.1518\n",
      "Epoch [78/300], Step [240/384], Loss: 37.9023\n",
      "Epoch [78/300], Step [260/384], Loss: 1.6869\n",
      "Epoch [78/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [78/300], Step [300/384], Loss: 71.6096\n",
      "Epoch [78/300], Step [320/384], Loss: 13.8590\n",
      "Epoch [78/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [78/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [78/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [79/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [79/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [79/300], Step [60/384], Loss: 138.4618\n",
      "Epoch [79/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [79/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [79/300], Step [120/384], Loss: 61.2289\n",
      "Epoch [79/300], Step [140/384], Loss: 52.2354\n",
      "Epoch [79/300], Step [160/384], Loss: 28.7673\n",
      "Epoch [79/300], Step [180/384], Loss: 184.3380\n",
      "Epoch [79/300], Step [200/384], Loss: 47.1996\n",
      "Epoch [79/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [79/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [79/300], Step [260/384], Loss: 273.1205\n",
      "Epoch [79/300], Step [280/384], Loss: 83.8701\n",
      "Epoch [79/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [79/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [79/300], Step [340/384], Loss: 55.2104\n",
      "Epoch [79/300], Step [360/384], Loss: 191.6665\n",
      "Epoch [79/300], Step [380/384], Loss: 47.5153\n",
      "Epoch [80/300], Step [20/384], Loss: 30.3866\n",
      "Epoch [80/300], Step [40/384], Loss: 43.6220\n",
      "Epoch [80/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [80/300], Step [80/384], Loss: 52.2690\n",
      "Epoch [80/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [80/300], Step [120/384], Loss: 46.5352\n",
      "Epoch [80/300], Step [140/384], Loss: 30.9134\n",
      "Epoch [80/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [80/300], Step [180/384], Loss: 113.7371\n",
      "Epoch [80/300], Step [200/384], Loss: 126.9483\n",
      "Epoch [80/300], Step [220/384], Loss: 4.5103\n",
      "Epoch [80/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [80/300], Step [260/384], Loss: 38.3503\n",
      "Epoch [80/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [80/300], Step [300/384], Loss: 23.2875\n",
      "Epoch [80/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [80/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [80/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [80/300], Step [380/384], Loss: 75.4802\n",
      "Epoch [81/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [81/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [81/300], Step [60/384], Loss: 11.4852\n",
      "Epoch [81/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [81/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [81/300], Step [120/384], Loss: 42.0537\n",
      "Epoch [81/300], Step [140/384], Loss: 42.8291\n",
      "Epoch [81/300], Step [160/384], Loss: 70.9959\n",
      "Epoch [81/300], Step [180/384], Loss: 191.9485\n",
      "Epoch [81/300], Step [200/384], Loss: 82.3039\n",
      "Epoch [81/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [81/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [81/300], Step [260/384], Loss: 0.0002\n",
      "Epoch [81/300], Step [280/384], Loss: 38.1296\n",
      "Epoch [81/300], Step [300/384], Loss: 72.0371\n",
      "Epoch [81/300], Step [320/384], Loss: 27.7806\n",
      "Epoch [81/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [81/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [81/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [82/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [82/300], Step [40/384], Loss: 74.5952\n",
      "Epoch [82/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [82/300], Step [80/384], Loss: 28.0271\n",
      "Epoch [82/300], Step [100/384], Loss: 9.8496\n",
      "Epoch [82/300], Step [120/384], Loss: 4.3295\n",
      "Epoch [82/300], Step [140/384], Loss: 18.0696\n",
      "Epoch [82/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [82/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [82/300], Step [200/384], Loss: 182.2394\n",
      "Epoch [82/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [82/300], Step [240/384], Loss: 35.2491\n",
      "Epoch [82/300], Step [260/384], Loss: 11.2761\n",
      "Epoch [82/300], Step [280/384], Loss: 107.5044\n",
      "Epoch [82/300], Step [300/384], Loss: 93.4779\n",
      "Epoch [82/300], Step [320/384], Loss: 16.1024\n",
      "Epoch [82/300], Step [340/384], Loss: 85.2377\n",
      "Epoch [82/300], Step [360/384], Loss: 22.2949\n",
      "Epoch [82/300], Step [380/384], Loss: 27.4834\n",
      "Epoch [83/300], Step [20/384], Loss: 85.6634\n",
      "Epoch [83/300], Step [40/384], Loss: 43.0145\n",
      "Epoch [83/300], Step [60/384], Loss: 115.4053\n",
      "Epoch [83/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [83/300], Step [100/384], Loss: 138.9471\n",
      "Epoch [83/300], Step [120/384], Loss: 8.7451\n",
      "Epoch [83/300], Step [140/384], Loss: 55.7103\n",
      "Epoch [83/300], Step [160/384], Loss: 114.8140\n",
      "Epoch [83/300], Step [180/384], Loss: 20.6005\n",
      "Epoch [83/300], Step [200/384], Loss: 29.5717\n",
      "Epoch [83/300], Step [220/384], Loss: 33.5524\n",
      "Epoch [83/300], Step [240/384], Loss: 13.2571\n",
      "Epoch [83/300], Step [260/384], Loss: 25.9022\n",
      "Epoch [83/300], Step [280/384], Loss: 26.5759\n",
      "Epoch [83/300], Step [300/384], Loss: 36.4350\n",
      "Epoch [83/300], Step [320/384], Loss: 28.4690\n",
      "Epoch [83/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [83/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [83/300], Step [380/384], Loss: 29.1847\n",
      "Epoch [84/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [84/300], Step [40/384], Loss: 88.9170\n",
      "Epoch [84/300], Step [60/384], Loss: 100.9569\n",
      "Epoch [84/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [84/300], Step [100/384], Loss: 49.0661\n",
      "Epoch [84/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [84/300], Step [140/384], Loss: 64.0749\n",
      "Epoch [84/300], Step [160/384], Loss: 204.4935\n",
      "Epoch [84/300], Step [180/384], Loss: 0.5359\n",
      "Epoch [84/300], Step [200/384], Loss: 76.9700\n",
      "Epoch [84/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [84/300], Step [240/384], Loss: 22.2050\n",
      "Epoch [84/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [84/300], Step [280/384], Loss: 137.0802\n",
      "Epoch [84/300], Step [300/384], Loss: 51.4224\n",
      "Epoch [84/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [84/300], Step [340/384], Loss: 82.1224\n",
      "Epoch [84/300], Step [360/384], Loss: 79.6013\n",
      "Epoch [84/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [85/300], Step [20/384], Loss: 20.7998\n",
      "Epoch [85/300], Step [40/384], Loss: 33.6197\n",
      "Epoch [85/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [85/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [85/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [85/300], Step [120/384], Loss: 21.9714\n",
      "Epoch [85/300], Step [140/384], Loss: 127.6898\n",
      "Epoch [85/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [85/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [85/300], Step [200/384], Loss: 15.2341\n",
      "Epoch [85/300], Step [220/384], Loss: 64.9477\n",
      "Epoch [85/300], Step [240/384], Loss: 114.5341\n",
      "Epoch [85/300], Step [260/384], Loss: 16.4496\n",
      "Epoch [85/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [85/300], Step [300/384], Loss: 92.3387\n",
      "Epoch [85/300], Step [320/384], Loss: 163.6268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [85/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [85/300], Step [360/384], Loss: 15.0121\n",
      "Epoch [85/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [86/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [86/300], Step [40/384], Loss: 328.7221\n",
      "Epoch [86/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [86/300], Step [80/384], Loss: 16.4029\n",
      "Epoch [86/300], Step [100/384], Loss: 68.8052\n",
      "Epoch [86/300], Step [120/384], Loss: 61.0865\n",
      "Epoch [86/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [86/300], Step [160/384], Loss: 38.1624\n",
      "Epoch [86/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [86/300], Step [200/384], Loss: 61.2875\n",
      "Epoch [86/300], Step [220/384], Loss: 134.2867\n",
      "Epoch [86/300], Step [240/384], Loss: 85.7211\n",
      "Epoch [86/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [86/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [86/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [86/300], Step [320/384], Loss: 43.9081\n",
      "Epoch [86/300], Step [340/384], Loss: 147.1568\n",
      "Epoch [86/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [86/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [87/300], Step [20/384], Loss: 155.1027\n",
      "Epoch [87/300], Step [40/384], Loss: 8.0046\n",
      "Epoch [87/300], Step [60/384], Loss: 54.7684\n",
      "Epoch [87/300], Step [80/384], Loss: 8.7204\n",
      "Epoch [87/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [87/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [87/300], Step [140/384], Loss: 93.9326\n",
      "Epoch [87/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [87/300], Step [180/384], Loss: 16.4525\n",
      "Epoch [87/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [87/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [87/300], Step [240/384], Loss: 14.5532\n",
      "Epoch [87/300], Step [260/384], Loss: 53.7351\n",
      "Epoch [87/300], Step [280/384], Loss: 4.7921\n",
      "Epoch [87/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [87/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [87/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [87/300], Step [360/384], Loss: 34.2409\n",
      "Epoch [87/300], Step [380/384], Loss: 48.0175\n",
      "Epoch [88/300], Step [20/384], Loss: 80.1161\n",
      "Epoch [88/300], Step [40/384], Loss: 35.5883\n",
      "Epoch [88/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [88/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [88/300], Step [100/384], Loss: 26.2363\n",
      "Epoch [88/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [88/300], Step [140/384], Loss: 7.5006\n",
      "Epoch [88/300], Step [160/384], Loss: 34.3296\n",
      "Epoch [88/300], Step [180/384], Loss: 85.3033\n",
      "Epoch [88/300], Step [200/384], Loss: 105.4772\n",
      "Epoch [88/300], Step [220/384], Loss: 65.3649\n",
      "Epoch [88/300], Step [240/384], Loss: 10.7377\n",
      "Epoch [88/300], Step [260/384], Loss: 76.9844\n",
      "Epoch [88/300], Step [280/384], Loss: 5.9519\n",
      "Epoch [88/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [88/300], Step [320/384], Loss: 96.4594\n",
      "Epoch [88/300], Step [340/384], Loss: 69.8595\n",
      "Epoch [88/300], Step [360/384], Loss: 14.8896\n",
      "Epoch [88/300], Step [380/384], Loss: 29.2365\n",
      "Epoch [89/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [89/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [89/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [89/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [89/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [89/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [89/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [89/300], Step [160/384], Loss: 16.9625\n",
      "Epoch [89/300], Step [180/384], Loss: 58.8199\n",
      "Epoch [89/300], Step [200/384], Loss: 39.2365\n",
      "Epoch [89/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [89/300], Step [240/384], Loss: 26.7795\n",
      "Epoch [89/300], Step [260/384], Loss: 56.8482\n",
      "Epoch [89/300], Step [280/384], Loss: 76.7590\n",
      "Epoch [89/300], Step [300/384], Loss: 67.9078\n",
      "Epoch [89/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [89/300], Step [340/384], Loss: 132.1880\n",
      "Epoch [89/300], Step [360/384], Loss: 4.4805\n",
      "Epoch [89/300], Step [380/384], Loss: 154.3117\n",
      "Epoch [90/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [90/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [90/300], Step [60/384], Loss: 0.9931\n",
      "Epoch [90/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [90/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [90/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [90/300], Step [140/384], Loss: 87.8745\n",
      "Epoch [90/300], Step [160/384], Loss: 160.7132\n",
      "Epoch [90/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [90/300], Step [200/384], Loss: 9.1272\n",
      "Epoch [90/300], Step [220/384], Loss: 19.9111\n",
      "Epoch [90/300], Step [240/384], Loss: 62.1893\n",
      "Epoch [90/300], Step [260/384], Loss: 41.5550\n",
      "Epoch [90/300], Step [280/384], Loss: 222.8473\n",
      "Epoch [90/300], Step [300/384], Loss: 105.9073\n",
      "Epoch [90/300], Step [320/384], Loss: 44.6727\n",
      "Epoch [90/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [90/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [90/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [91/300], Step [20/384], Loss: 0.4583\n",
      "Epoch [91/300], Step [40/384], Loss: 127.1471\n",
      "Epoch [91/300], Step [60/384], Loss: 81.4866\n",
      "Epoch [91/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [91/300], Step [100/384], Loss: 84.4382\n",
      "Epoch [91/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [91/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [91/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [91/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [91/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [91/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [91/300], Step [240/384], Loss: 189.9404\n",
      "Epoch [91/300], Step [260/384], Loss: 230.9382\n",
      "Epoch [91/300], Step [280/384], Loss: 395.3034\n",
      "Epoch [91/300], Step [300/384], Loss: 220.2010\n",
      "Epoch [91/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [91/300], Step [340/384], Loss: 32.8086\n",
      "Epoch [91/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [91/300], Step [380/384], Loss: 66.1616\n",
      "Epoch [92/300], Step [20/384], Loss: 50.3976\n",
      "Epoch [92/300], Step [40/384], Loss: 51.4799\n",
      "Epoch [92/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [92/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [92/300], Step [100/384], Loss: 231.6527\n",
      "Epoch [92/300], Step [120/384], Loss: 46.1736\n",
      "Epoch [92/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [92/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [92/300], Step [180/384], Loss: 10.2722\n",
      "Epoch [92/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [92/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [92/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [92/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [92/300], Step [280/384], Loss: 28.9339\n",
      "Epoch [92/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [92/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [92/300], Step [340/384], Loss: 86.5083\n",
      "Epoch [92/300], Step [360/384], Loss: 78.6421\n",
      "Epoch [92/300], Step [380/384], Loss: 236.1492\n",
      "Epoch [93/300], Step [20/384], Loss: 66.7117\n",
      "Epoch [93/300], Step [40/384], Loss: 133.3955\n",
      "Epoch [93/300], Step [60/384], Loss: 45.0988\n",
      "Epoch [93/300], Step [80/384], Loss: 0.0003\n",
      "Epoch [93/300], Step [100/384], Loss: 140.1609\n",
      "Epoch [93/300], Step [120/384], Loss: 165.2259\n",
      "Epoch [93/300], Step [140/384], Loss: 97.6903\n",
      "Epoch [93/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [93/300], Step [180/384], Loss: 51.8618\n",
      "Epoch [93/300], Step [200/384], Loss: 53.3315\n",
      "Epoch [93/300], Step [220/384], Loss: 68.2378\n",
      "Epoch [93/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [93/300], Step [260/384], Loss: 146.5941\n",
      "Epoch [93/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [93/300], Step [300/384], Loss: 111.9665\n",
      "Epoch [93/300], Step [320/384], Loss: 129.4773\n",
      "Epoch [93/300], Step [340/384], Loss: 92.5084\n",
      "Epoch [93/300], Step [360/384], Loss: 35.1119\n",
      "Epoch [93/300], Step [380/384], Loss: 40.2942\n",
      "Epoch [94/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [94/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [94/300], Step [60/384], Loss: 256.0029\n",
      "Epoch [94/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [94/300], Step [100/384], Loss: 47.4605\n",
      "Epoch [94/300], Step [120/384], Loss: 31.7297\n",
      "Epoch [94/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [94/300], Step [160/384], Loss: 90.5780\n",
      "Epoch [94/300], Step [180/384], Loss: 33.1341\n",
      "Epoch [94/300], Step [200/384], Loss: 58.2547\n",
      "Epoch [94/300], Step [220/384], Loss: 83.8669\n",
      "Epoch [94/300], Step [240/384], Loss: 9.0248\n",
      "Epoch [94/300], Step [260/384], Loss: 279.7922\n",
      "Epoch [94/300], Step [280/384], Loss: 126.1589\n",
      "Epoch [94/300], Step [300/384], Loss: 111.4855\n",
      "Epoch [94/300], Step [320/384], Loss: 146.5792\n",
      "Epoch [94/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [94/300], Step [360/384], Loss: 90.2102\n",
      "Epoch [94/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [95/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [95/300], Step [40/384], Loss: 52.6759\n",
      "Epoch [95/300], Step [60/384], Loss: 422.3705\n",
      "Epoch [95/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [95/300], Step [100/384], Loss: 0.0001\n",
      "Epoch [95/300], Step [120/384], Loss: 111.0694\n",
      "Epoch [95/300], Step [140/384], Loss: 27.4825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [95/300], Step [160/384], Loss: 93.2480\n",
      "Epoch [95/300], Step [180/384], Loss: 51.2864\n",
      "Epoch [95/300], Step [200/384], Loss: 50.1717\n",
      "Epoch [95/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [95/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [95/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [95/300], Step [280/384], Loss: 24.4874\n",
      "Epoch [95/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [95/300], Step [320/384], Loss: 72.5661\n",
      "Epoch [95/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [95/300], Step [360/384], Loss: 0.0743\n",
      "Epoch [95/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [96/300], Step [20/384], Loss: 37.7313\n",
      "Epoch [96/300], Step [40/384], Loss: 125.5909\n",
      "Epoch [96/300], Step [60/384], Loss: 85.4690\n",
      "Epoch [96/300], Step [80/384], Loss: 42.0548\n",
      "Epoch [96/300], Step [100/384], Loss: 10.7889\n",
      "Epoch [96/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [96/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [96/300], Step [160/384], Loss: 0.0017\n",
      "Epoch [96/300], Step [180/384], Loss: 262.6348\n",
      "Epoch [96/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [96/300], Step [220/384], Loss: 189.0514\n",
      "Epoch [96/300], Step [240/384], Loss: 9.4994\n",
      "Epoch [96/300], Step [260/384], Loss: 68.8359\n",
      "Epoch [96/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [96/300], Step [300/384], Loss: 88.3265\n",
      "Epoch [96/300], Step [320/384], Loss: 10.7392\n",
      "Epoch [96/300], Step [340/384], Loss: 365.5456\n",
      "Epoch [96/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [96/300], Step [380/384], Loss: 79.4389\n",
      "Epoch [97/300], Step [20/384], Loss: 84.0818\n",
      "Epoch [97/300], Step [40/384], Loss: 219.8388\n",
      "Epoch [97/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [97/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [97/300], Step [100/384], Loss: 202.2066\n",
      "Epoch [97/300], Step [120/384], Loss: 4.0061\n",
      "Epoch [97/300], Step [140/384], Loss: 20.8106\n",
      "Epoch [97/300], Step [160/384], Loss: 23.9030\n",
      "Epoch [97/300], Step [180/384], Loss: 50.9821\n",
      "Epoch [97/300], Step [200/384], Loss: 3.2949\n",
      "Epoch [97/300], Step [220/384], Loss: 193.0567\n",
      "Epoch [97/300], Step [240/384], Loss: 132.6898\n",
      "Epoch [97/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [97/300], Step [280/384], Loss: 40.8051\n",
      "Epoch [97/300], Step [300/384], Loss: 11.1507\n",
      "Epoch [97/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [97/300], Step [340/384], Loss: 124.7134\n",
      "Epoch [97/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [97/300], Step [380/384], Loss: 93.9502\n",
      "Epoch [98/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [98/300], Step [40/384], Loss: 79.9479\n",
      "Epoch [98/300], Step [60/384], Loss: 73.0303\n",
      "Epoch [98/300], Step [80/384], Loss: 22.8031\n",
      "Epoch [98/300], Step [100/384], Loss: 61.6303\n",
      "Epoch [98/300], Step [120/384], Loss: 7.6019\n",
      "Epoch [98/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [98/300], Step [160/384], Loss: 208.4387\n",
      "Epoch [98/300], Step [180/384], Loss: 38.7343\n",
      "Epoch [98/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [98/300], Step [220/384], Loss: 12.2145\n",
      "Epoch [98/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [98/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [98/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [98/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [98/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [98/300], Step [340/384], Loss: 2.7686\n",
      "Epoch [98/300], Step [360/384], Loss: 166.7246\n",
      "Epoch [98/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [99/300], Step [20/384], Loss: 72.5641\n",
      "Epoch [99/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [99/300], Step [60/384], Loss: 0.2305\n",
      "Epoch [99/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [99/300], Step [100/384], Loss: 28.4621\n",
      "Epoch [99/300], Step [120/384], Loss: 24.5948\n",
      "Epoch [99/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [99/300], Step [160/384], Loss: 26.2230\n",
      "Epoch [99/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [99/300], Step [200/384], Loss: 11.1845\n",
      "Epoch [99/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [99/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [99/300], Step [260/384], Loss: 60.6739\n",
      "Epoch [99/300], Step [280/384], Loss: 42.0678\n",
      "Epoch [99/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [99/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [99/300], Step [340/384], Loss: 62.8496\n",
      "Epoch [99/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [99/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [40/384], Loss: 107.9855\n",
      "Epoch [100/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [80/384], Loss: 119.2414\n",
      "Epoch [100/300], Step [100/384], Loss: 53.8131\n",
      "Epoch [100/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [140/384], Loss: 78.8761\n",
      "Epoch [100/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [220/384], Loss: 93.1910\n",
      "Epoch [100/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [340/384], Loss: 175.3764\n",
      "Epoch [100/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [100/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [101/300], Step [20/384], Loss: 194.4340\n",
      "Epoch [101/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [101/300], Step [60/384], Loss: 113.6777\n",
      "Epoch [101/300], Step [80/384], Loss: 42.6122\n",
      "Epoch [101/300], Step [100/384], Loss: 40.7615\n",
      "Epoch [101/300], Step [120/384], Loss: 83.6606\n",
      "Epoch [101/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [101/300], Step [160/384], Loss: 24.1339\n",
      "Epoch [101/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [101/300], Step [200/384], Loss: 68.4329\n",
      "Epoch [101/300], Step [220/384], Loss: 284.9458\n",
      "Epoch [101/300], Step [240/384], Loss: 288.0898\n",
      "Epoch [101/300], Step [260/384], Loss: 38.7319\n",
      "Epoch [101/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [101/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [101/300], Step [320/384], Loss: 147.7413\n",
      "Epoch [101/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [101/300], Step [360/384], Loss: 89.6494\n",
      "Epoch [101/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [102/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [102/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [102/300], Step [60/384], Loss: 97.1346\n",
      "Epoch [102/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [102/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [102/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [102/300], Step [140/384], Loss: 3.2466\n",
      "Epoch [102/300], Step [160/384], Loss: 50.4398\n",
      "Epoch [102/300], Step [180/384], Loss: 38.8910\n",
      "Epoch [102/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [102/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [102/300], Step [240/384], Loss: 33.0115\n",
      "Epoch [102/300], Step [260/384], Loss: 245.9346\n",
      "Epoch [102/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [102/300], Step [300/384], Loss: 9.3002\n",
      "Epoch [102/300], Step [320/384], Loss: 53.8813\n",
      "Epoch [102/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [102/300], Step [360/384], Loss: 70.9953\n",
      "Epoch [102/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [103/300], Step [20/384], Loss: 21.9619\n",
      "Epoch [103/300], Step [40/384], Loss: 70.9856\n",
      "Epoch [103/300], Step [60/384], Loss: 6.0236\n",
      "Epoch [103/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [103/300], Step [100/384], Loss: 63.9678\n",
      "Epoch [103/300], Step [120/384], Loss: 13.9914\n",
      "Epoch [103/300], Step [140/384], Loss: 94.6331\n",
      "Epoch [103/300], Step [160/384], Loss: 274.8376\n",
      "Epoch [103/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [103/300], Step [200/384], Loss: 54.6469\n",
      "Epoch [103/300], Step [220/384], Loss: 28.0808\n",
      "Epoch [103/300], Step [240/384], Loss: 0.9039\n",
      "Epoch [103/300], Step [260/384], Loss: 85.3985\n",
      "Epoch [103/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [103/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [103/300], Step [320/384], Loss: 158.3892\n",
      "Epoch [103/300], Step [340/384], Loss: 129.0583\n",
      "Epoch [103/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [103/300], Step [380/384], Loss: 31.3260\n",
      "Epoch [104/300], Step [20/384], Loss: 24.9287\n",
      "Epoch [104/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [104/300], Step [60/384], Loss: 15.7748\n",
      "Epoch [104/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [104/300], Step [100/384], Loss: 189.4914\n",
      "Epoch [104/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [104/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [104/300], Step [160/384], Loss: 76.7661\n",
      "Epoch [104/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [104/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [104/300], Step [220/384], Loss: 239.0041\n",
      "Epoch [104/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [104/300], Step [260/384], Loss: 136.2757\n",
      "Epoch [104/300], Step [280/384], Loss: 35.8143\n",
      "Epoch [104/300], Step [300/384], Loss: 23.8530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [104/300], Step [320/384], Loss: 93.3487\n",
      "Epoch [104/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [104/300], Step [360/384], Loss: 73.7971\n",
      "Epoch [104/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [105/300], Step [20/384], Loss: 9.1306\n",
      "Epoch [105/300], Step [40/384], Loss: 95.8574\n",
      "Epoch [105/300], Step [60/384], Loss: 59.1402\n",
      "Epoch [105/300], Step [80/384], Loss: 100.2684\n",
      "Epoch [105/300], Step [100/384], Loss: 62.0372\n",
      "Epoch [105/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [105/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [105/300], Step [160/384], Loss: 151.1213\n",
      "Epoch [105/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [105/300], Step [200/384], Loss: 45.9449\n",
      "Epoch [105/300], Step [220/384], Loss: 43.5175\n",
      "Epoch [105/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [105/300], Step [260/384], Loss: 54.6713\n",
      "Epoch [105/300], Step [280/384], Loss: 20.6017\n",
      "Epoch [105/300], Step [300/384], Loss: 132.0236\n",
      "Epoch [105/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [105/300], Step [340/384], Loss: 20.1071\n",
      "Epoch [105/300], Step [360/384], Loss: 21.5073\n",
      "Epoch [105/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [20/384], Loss: 0.0018\n",
      "Epoch [106/300], Step [40/384], Loss: 21.6799\n",
      "Epoch [106/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [80/384], Loss: 116.5103\n",
      "Epoch [106/300], Step [100/384], Loss: 38.7043\n",
      "Epoch [106/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [160/384], Loss: 38.3814\n",
      "Epoch [106/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [220/384], Loss: 44.9524\n",
      "Epoch [106/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [280/384], Loss: 38.5466\n",
      "Epoch [106/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [106/300], Step [380/384], Loss: 208.2040\n",
      "Epoch [107/300], Step [20/384], Loss: 57.3649\n",
      "Epoch [107/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [107/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [107/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [107/300], Step [100/384], Loss: 208.2249\n",
      "Epoch [107/300], Step [120/384], Loss: 106.8203\n",
      "Epoch [107/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [107/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [107/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [107/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [107/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [107/300], Step [240/384], Loss: 56.7473\n",
      "Epoch [107/300], Step [260/384], Loss: 83.3159\n",
      "Epoch [107/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [107/300], Step [300/384], Loss: 35.7117\n",
      "Epoch [107/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [107/300], Step [340/384], Loss: 41.4176\n",
      "Epoch [107/300], Step [360/384], Loss: 26.5051\n",
      "Epoch [107/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [108/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [108/300], Step [40/384], Loss: 167.4801\n",
      "Epoch [108/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [108/300], Step [80/384], Loss: 66.0742\n",
      "Epoch [108/300], Step [100/384], Loss: 84.3531\n",
      "Epoch [108/300], Step [120/384], Loss: 202.1368\n",
      "Epoch [108/300], Step [140/384], Loss: 44.7888\n",
      "Epoch [108/300], Step [160/384], Loss: 121.3555\n",
      "Epoch [108/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [108/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [108/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [108/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [108/300], Step [260/384], Loss: 0.0046\n",
      "Epoch [108/300], Step [280/384], Loss: 36.2759\n",
      "Epoch [108/300], Step [300/384], Loss: 40.1909\n",
      "Epoch [108/300], Step [320/384], Loss: 7.2399\n",
      "Epoch [108/300], Step [340/384], Loss: 60.7139\n",
      "Epoch [108/300], Step [360/384], Loss: 113.0009\n",
      "Epoch [108/300], Step [380/384], Loss: 23.4850\n",
      "Epoch [109/300], Step [20/384], Loss: 230.5442\n",
      "Epoch [109/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [60/384], Loss: 7.2825\n",
      "Epoch [109/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [100/384], Loss: 86.2124\n",
      "Epoch [109/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [140/384], Loss: 12.7431\n",
      "Epoch [109/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [180/384], Loss: 72.3931\n",
      "Epoch [109/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [109/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [110/300], Step [20/384], Loss: 51.5760\n",
      "Epoch [110/300], Step [40/384], Loss: 6.4035\n",
      "Epoch [110/300], Step [60/384], Loss: 65.3054\n",
      "Epoch [110/300], Step [80/384], Loss: 64.3370\n",
      "Epoch [110/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [110/300], Step [120/384], Loss: 103.5014\n",
      "Epoch [110/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [110/300], Step [160/384], Loss: 132.2215\n",
      "Epoch [110/300], Step [180/384], Loss: 74.4772\n",
      "Epoch [110/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [110/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [110/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [110/300], Step [260/384], Loss: 130.7554\n",
      "Epoch [110/300], Step [280/384], Loss: 40.0781\n",
      "Epoch [110/300], Step [300/384], Loss: 30.4938\n",
      "Epoch [110/300], Step [320/384], Loss: 161.0483\n",
      "Epoch [110/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [110/300], Step [360/384], Loss: 208.9572\n",
      "Epoch [110/300], Step [380/384], Loss: 30.1863\n",
      "Epoch [111/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [111/300], Step [40/384], Loss: 21.6380\n",
      "Epoch [111/300], Step [60/384], Loss: 20.4240\n",
      "Epoch [111/300], Step [80/384], Loss: 1.6733\n",
      "Epoch [111/300], Step [100/384], Loss: 58.1576\n",
      "Epoch [111/300], Step [120/384], Loss: 103.3541\n",
      "Epoch [111/300], Step [140/384], Loss: 11.1610\n",
      "Epoch [111/300], Step [160/384], Loss: 38.0438\n",
      "Epoch [111/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [111/300], Step [200/384], Loss: 98.8368\n",
      "Epoch [111/300], Step [220/384], Loss: 32.6681\n",
      "Epoch [111/300], Step [240/384], Loss: 59.2336\n",
      "Epoch [111/300], Step [260/384], Loss: 89.1702\n",
      "Epoch [111/300], Step [280/384], Loss: 6.0721\n",
      "Epoch [111/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [111/300], Step [320/384], Loss: 149.6695\n",
      "Epoch [111/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [111/300], Step [360/384], Loss: 486.0431\n",
      "Epoch [111/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [120/384], Loss: 157.1503\n",
      "Epoch [112/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [200/384], Loss: 30.8615\n",
      "Epoch [112/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [112/300], Step [380/384], Loss: 38.0616\n",
      "Epoch [113/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [113/300], Step [40/384], Loss: 71.5154\n",
      "Epoch [113/300], Step [60/384], Loss: 67.1816\n",
      "Epoch [113/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [113/300], Step [100/384], Loss: 0.0007\n",
      "Epoch [113/300], Step [120/384], Loss: 107.2286\n",
      "Epoch [113/300], Step [140/384], Loss: 90.4477\n",
      "Epoch [113/300], Step [160/384], Loss: 0.5625\n",
      "Epoch [113/300], Step [180/384], Loss: 3.9895\n",
      "Epoch [113/300], Step [200/384], Loss: 24.5607\n",
      "Epoch [113/300], Step [220/384], Loss: 33.4879\n",
      "Epoch [113/300], Step [240/384], Loss: 1.4976\n",
      "Epoch [113/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [113/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [113/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [113/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [113/300], Step [340/384], Loss: 18.5611\n",
      "Epoch [113/300], Step [360/384], Loss: 151.8505\n",
      "Epoch [113/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [114/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [114/300], Step [40/384], Loss: 144.5019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [114/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [114/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [114/300], Step [100/384], Loss: 7.8996\n",
      "Epoch [114/300], Step [120/384], Loss: 124.5019\n",
      "Epoch [114/300], Step [140/384], Loss: 53.5201\n",
      "Epoch [114/300], Step [160/384], Loss: 27.2429\n",
      "Epoch [114/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [114/300], Step [200/384], Loss: 6.4828\n",
      "Epoch [114/300], Step [220/384], Loss: 6.8568\n",
      "Epoch [114/300], Step [240/384], Loss: 123.1290\n",
      "Epoch [114/300], Step [260/384], Loss: 36.2354\n",
      "Epoch [114/300], Step [280/384], Loss: 106.1928\n",
      "Epoch [114/300], Step [300/384], Loss: 130.8138\n",
      "Epoch [114/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [114/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [114/300], Step [360/384], Loss: 6.1537\n",
      "Epoch [114/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [60/384], Loss: 51.0452\n",
      "Epoch [115/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [120/384], Loss: 165.5816\n",
      "Epoch [115/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [200/384], Loss: 52.6739\n",
      "Epoch [115/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [300/384], Loss: 10.8816\n",
      "Epoch [115/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [115/300], Step [360/384], Loss: 36.5668\n",
      "Epoch [115/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [116/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [116/300], Step [40/384], Loss: 189.4986\n",
      "Epoch [116/300], Step [60/384], Loss: 134.4547\n",
      "Epoch [116/300], Step [80/384], Loss: 66.2621\n",
      "Epoch [116/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [116/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [116/300], Step [140/384], Loss: 21.5803\n",
      "Epoch [116/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [116/300], Step [180/384], Loss: 194.3006\n",
      "Epoch [116/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [116/300], Step [220/384], Loss: 57.6045\n",
      "Epoch [116/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [116/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [116/300], Step [280/384], Loss: 134.2526\n",
      "Epoch [116/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [116/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [116/300], Step [340/384], Loss: 0.0002\n",
      "Epoch [116/300], Step [360/384], Loss: 21.9283\n",
      "Epoch [116/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [117/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [117/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [117/300], Step [60/384], Loss: 246.9236\n",
      "Epoch [117/300], Step [80/384], Loss: 247.8909\n",
      "Epoch [117/300], Step [100/384], Loss: 67.6310\n",
      "Epoch [117/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [117/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [117/300], Step [160/384], Loss: 40.5089\n",
      "Epoch [117/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [117/300], Step [200/384], Loss: 294.9139\n",
      "Epoch [117/300], Step [220/384], Loss: 164.0087\n",
      "Epoch [117/300], Step [240/384], Loss: 2.9649\n",
      "Epoch [117/300], Step [260/384], Loss: 18.5657\n",
      "Epoch [117/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [117/300], Step [300/384], Loss: 140.8464\n",
      "Epoch [117/300], Step [320/384], Loss: 7.7690\n",
      "Epoch [117/300], Step [340/384], Loss: 12.6314\n",
      "Epoch [117/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [117/300], Step [380/384], Loss: 48.9585\n",
      "Epoch [118/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [118/300], Step [40/384], Loss: 2.5719\n",
      "Epoch [118/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [118/300], Step [80/384], Loss: 10.6458\n",
      "Epoch [118/300], Step [100/384], Loss: 130.1549\n",
      "Epoch [118/300], Step [120/384], Loss: 27.1474\n",
      "Epoch [118/300], Step [140/384], Loss: 278.4137\n",
      "Epoch [118/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [118/300], Step [180/384], Loss: 92.9162\n",
      "Epoch [118/300], Step [200/384], Loss: 63.3459\n",
      "Epoch [118/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [118/300], Step [240/384], Loss: 58.4540\n",
      "Epoch [118/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [118/300], Step [280/384], Loss: 45.0230\n",
      "Epoch [118/300], Step [300/384], Loss: 102.4636\n",
      "Epoch [118/300], Step [320/384], Loss: 107.3240\n",
      "Epoch [118/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [118/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [118/300], Step [380/384], Loss: 5.8417\n",
      "Epoch [119/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [119/300], Step [40/384], Loss: 14.9877\n",
      "Epoch [119/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [119/300], Step [80/384], Loss: 72.4324\n",
      "Epoch [119/300], Step [100/384], Loss: 47.9985\n",
      "Epoch [119/300], Step [120/384], Loss: 85.1695\n",
      "Epoch [119/300], Step [140/384], Loss: 76.0405\n",
      "Epoch [119/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [119/300], Step [180/384], Loss: 3.7541\n",
      "Epoch [119/300], Step [200/384], Loss: 59.2529\n",
      "Epoch [119/300], Step [220/384], Loss: 65.0061\n",
      "Epoch [119/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [119/300], Step [260/384], Loss: 25.3572\n",
      "Epoch [119/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [119/300], Step [300/384], Loss: 90.0544\n",
      "Epoch [119/300], Step [320/384], Loss: 151.1009\n",
      "Epoch [119/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [119/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [119/300], Step [380/384], Loss: 70.2504\n",
      "Epoch [120/300], Step [20/384], Loss: 15.0913\n",
      "Epoch [120/300], Step [40/384], Loss: 62.2257\n",
      "Epoch [120/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [80/384], Loss: 32.0639\n",
      "Epoch [120/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [140/384], Loss: 97.4911\n",
      "Epoch [120/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [200/384], Loss: 0.3719\n",
      "Epoch [120/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [120/300], Step [380/384], Loss: 33.5153\n",
      "Epoch [121/300], Step [20/384], Loss: 132.2179\n",
      "Epoch [121/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [121/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [121/300], Step [80/384], Loss: 29.9547\n",
      "Epoch [121/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [121/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [121/300], Step [140/384], Loss: 109.4375\n",
      "Epoch [121/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [121/300], Step [180/384], Loss: 66.4779\n",
      "Epoch [121/300], Step [200/384], Loss: 58.6870\n",
      "Epoch [121/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [121/300], Step [240/384], Loss: 31.8073\n",
      "Epoch [121/300], Step [260/384], Loss: 40.6633\n",
      "Epoch [121/300], Step [280/384], Loss: 119.2653\n",
      "Epoch [121/300], Step [300/384], Loss: 0.0026\n",
      "Epoch [121/300], Step [320/384], Loss: 220.5701\n",
      "Epoch [121/300], Step [340/384], Loss: 14.6218\n",
      "Epoch [121/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [121/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [20/384], Loss: 94.0253\n",
      "Epoch [122/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [140/384], Loss: 11.1603\n",
      "Epoch [122/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [180/384], Loss: 31.1244\n",
      "Epoch [122/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [280/384], Loss: 92.3503\n",
      "Epoch [122/300], Step [300/384], Loss: 100.5631\n",
      "Epoch [122/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [340/384], Loss: 104.8171\n",
      "Epoch [122/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [122/300], Step [380/384], Loss: 97.4870\n",
      "Epoch [123/300], Step [20/384], Loss: 91.4769\n",
      "Epoch [123/300], Step [40/384], Loss: 144.5107\n",
      "Epoch [123/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [123/300], Step [80/384], Loss: 44.4655\n",
      "Epoch [123/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [123/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [123/300], Step [140/384], Loss: 88.3906\n",
      "Epoch [123/300], Step [160/384], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [123/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [123/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [123/300], Step [220/384], Loss: 88.3662\n",
      "Epoch [123/300], Step [240/384], Loss: 28.3629\n",
      "Epoch [123/300], Step [260/384], Loss: 152.9207\n",
      "Epoch [123/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [123/300], Step [300/384], Loss: 16.3721\n",
      "Epoch [123/300], Step [320/384], Loss: 78.1230\n",
      "Epoch [123/300], Step [340/384], Loss: 13.6447\n",
      "Epoch [123/300], Step [360/384], Loss: 120.4252\n",
      "Epoch [123/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [20/384], Loss: 64.8374\n",
      "Epoch [124/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [60/384], Loss: 115.9414\n",
      "Epoch [124/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [160/384], Loss: 61.7797\n",
      "Epoch [124/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [240/384], Loss: 0.0057\n",
      "Epoch [124/300], Step [260/384], Loss: 3.6459\n",
      "Epoch [124/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [300/384], Loss: 240.3924\n",
      "Epoch [124/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [124/300], Step [380/384], Loss: 117.9233\n",
      "Epoch [125/300], Step [20/384], Loss: 236.4170\n",
      "Epoch [125/300], Step [40/384], Loss: 168.2220\n",
      "Epoch [125/300], Step [60/384], Loss: 40.9100\n",
      "Epoch [125/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [125/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [125/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [125/300], Step [140/384], Loss: 254.8585\n",
      "Epoch [125/300], Step [160/384], Loss: 41.3471\n",
      "Epoch [125/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [125/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [125/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [125/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [125/300], Step [260/384], Loss: 5.4270\n",
      "Epoch [125/300], Step [280/384], Loss: 26.9302\n",
      "Epoch [125/300], Step [300/384], Loss: 70.9323\n",
      "Epoch [125/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [125/300], Step [340/384], Loss: 161.2856\n",
      "Epoch [125/300], Step [360/384], Loss: 67.7391\n",
      "Epoch [125/300], Step [380/384], Loss: 25.5786\n",
      "Epoch [126/300], Step [20/384], Loss: 50.8153\n",
      "Epoch [126/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [126/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [126/300], Step [80/384], Loss: 31.2662\n",
      "Epoch [126/300], Step [100/384], Loss: 81.7713\n",
      "Epoch [126/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [126/300], Step [140/384], Loss: 2.5369\n",
      "Epoch [126/300], Step [160/384], Loss: 17.6021\n",
      "Epoch [126/300], Step [180/384], Loss: 14.4618\n",
      "Epoch [126/300], Step [200/384], Loss: 32.4454\n",
      "Epoch [126/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [126/300], Step [240/384], Loss: 13.7615\n",
      "Epoch [126/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [126/300], Step [280/384], Loss: 78.7735\n",
      "Epoch [126/300], Step [300/384], Loss: 191.3715\n",
      "Epoch [126/300], Step [320/384], Loss: 16.8699\n",
      "Epoch [126/300], Step [340/384], Loss: 1.5629\n",
      "Epoch [126/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [126/300], Step [380/384], Loss: 28.2952\n",
      "Epoch [127/300], Step [20/384], Loss: 46.6640\n",
      "Epoch [127/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [127/300], Step [60/384], Loss: 2.4377\n",
      "Epoch [127/300], Step [80/384], Loss: 85.6883\n",
      "Epoch [127/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [127/300], Step [120/384], Loss: 84.8845\n",
      "Epoch [127/300], Step [140/384], Loss: 111.1540\n",
      "Epoch [127/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [127/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [127/300], Step [200/384], Loss: 275.5909\n",
      "Epoch [127/300], Step [220/384], Loss: 152.1347\n",
      "Epoch [127/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [127/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [127/300], Step [280/384], Loss: 92.3588\n",
      "Epoch [127/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [127/300], Step [320/384], Loss: 377.7129\n",
      "Epoch [127/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [127/300], Step [360/384], Loss: 237.5261\n",
      "Epoch [127/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [128/300], Step [20/384], Loss: 68.9289\n",
      "Epoch [128/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [128/300], Step [60/384], Loss: 11.9622\n",
      "Epoch [128/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [128/300], Step [100/384], Loss: 148.4144\n",
      "Epoch [128/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [128/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [128/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [128/300], Step [180/384], Loss: 68.1163\n",
      "Epoch [128/300], Step [200/384], Loss: 91.5340\n",
      "Epoch [128/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [128/300], Step [240/384], Loss: 90.5221\n",
      "Epoch [128/300], Step [260/384], Loss: 126.7219\n",
      "Epoch [128/300], Step [280/384], Loss: 34.8715\n",
      "Epoch [128/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [128/300], Step [320/384], Loss: 134.3930\n",
      "Epoch [128/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [128/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [128/300], Step [380/384], Loss: 34.5776\n",
      "Epoch [129/300], Step [20/384], Loss: 35.2360\n",
      "Epoch [129/300], Step [40/384], Loss: 57.1519\n",
      "Epoch [129/300], Step [60/384], Loss: 130.1262\n",
      "Epoch [129/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [129/300], Step [100/384], Loss: 312.0207\n",
      "Epoch [129/300], Step [120/384], Loss: 40.3370\n",
      "Epoch [129/300], Step [140/384], Loss: 22.8289\n",
      "Epoch [129/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [129/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [129/300], Step [200/384], Loss: 115.9660\n",
      "Epoch [129/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [129/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [129/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [129/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [129/300], Step [300/384], Loss: 1.1724\n",
      "Epoch [129/300], Step [320/384], Loss: 97.7551\n",
      "Epoch [129/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [129/300], Step [360/384], Loss: 48.5521\n",
      "Epoch [129/300], Step [380/384], Loss: 162.4081\n",
      "Epoch [130/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [130/300], Step [40/384], Loss: 92.9364\n",
      "Epoch [130/300], Step [60/384], Loss: 71.2252\n",
      "Epoch [130/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [130/300], Step [100/384], Loss: 78.3253\n",
      "Epoch [130/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [130/300], Step [140/384], Loss: 53.7846\n",
      "Epoch [130/300], Step [160/384], Loss: 31.5455\n",
      "Epoch [130/300], Step [180/384], Loss: 64.2886\n",
      "Epoch [130/300], Step [200/384], Loss: 0.0001\n",
      "Epoch [130/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [130/300], Step [240/384], Loss: 19.2746\n",
      "Epoch [130/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [130/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [130/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [130/300], Step [320/384], Loss: 30.3863\n",
      "Epoch [130/300], Step [340/384], Loss: 18.0411\n",
      "Epoch [130/300], Step [360/384], Loss: 58.7062\n",
      "Epoch [130/300], Step [380/384], Loss: 88.4442\n",
      "Epoch [131/300], Step [20/384], Loss: 125.5162\n",
      "Epoch [131/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [60/384], Loss: 8.6907\n",
      "Epoch [131/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [220/384], Loss: 58.7414\n",
      "Epoch [131/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [260/384], Loss: 93.3607\n",
      "Epoch [131/300], Step [280/384], Loss: 207.5687\n",
      "Epoch [131/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [131/300], Step [380/384], Loss: 1.0572\n",
      "Epoch [132/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [132/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [132/300], Step [60/384], Loss: 69.3723\n",
      "Epoch [132/300], Step [80/384], Loss: 87.2521\n",
      "Epoch [132/300], Step [100/384], Loss: 83.5445\n",
      "Epoch [132/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [132/300], Step [140/384], Loss: 37.8810\n",
      "Epoch [132/300], Step [160/384], Loss: 2.1558\n",
      "Epoch [132/300], Step [180/384], Loss: 237.9416\n",
      "Epoch [132/300], Step [200/384], Loss: 56.6283\n",
      "Epoch [132/300], Step [220/384], Loss: 16.1208\n",
      "Epoch [132/300], Step [240/384], Loss: 19.0956\n",
      "Epoch [132/300], Step [260/384], Loss: 103.3876\n",
      "Epoch [132/300], Step [280/384], Loss: 82.7089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [132/300], Step [300/384], Loss: 17.1039\n",
      "Epoch [132/300], Step [320/384], Loss: 92.0448\n",
      "Epoch [132/300], Step [340/384], Loss: 80.7104\n",
      "Epoch [132/300], Step [360/384], Loss: 62.2505\n",
      "Epoch [132/300], Step [380/384], Loss: 6.1980\n",
      "Epoch [133/300], Step [20/384], Loss: 22.1086\n",
      "Epoch [133/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [133/300], Step [60/384], Loss: 1.3628\n",
      "Epoch [133/300], Step [80/384], Loss: 13.2318\n",
      "Epoch [133/300], Step [100/384], Loss: 35.4767\n",
      "Epoch [133/300], Step [120/384], Loss: 43.0004\n",
      "Epoch [133/300], Step [140/384], Loss: 95.8229\n",
      "Epoch [133/300], Step [160/384], Loss: 16.4707\n",
      "Epoch [133/300], Step [180/384], Loss: 9.0379\n",
      "Epoch [133/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [133/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [133/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [133/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [133/300], Step [280/384], Loss: 176.4031\n",
      "Epoch [133/300], Step [300/384], Loss: 26.9045\n",
      "Epoch [133/300], Step [320/384], Loss: 14.1849\n",
      "Epoch [133/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [133/300], Step [360/384], Loss: 12.1197\n",
      "Epoch [133/300], Step [380/384], Loss: 32.8970\n",
      "Epoch [134/300], Step [20/384], Loss: 7.2696\n",
      "Epoch [134/300], Step [40/384], Loss: 0.0861\n",
      "Epoch [134/300], Step [60/384], Loss: 30.0358\n",
      "Epoch [134/300], Step [80/384], Loss: 98.0230\n",
      "Epoch [134/300], Step [100/384], Loss: 63.5146\n",
      "Epoch [134/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [134/300], Step [140/384], Loss: 62.9989\n",
      "Epoch [134/300], Step [160/384], Loss: 92.7377\n",
      "Epoch [134/300], Step [180/384], Loss: 135.9933\n",
      "Epoch [134/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [134/300], Step [220/384], Loss: 52.9033\n",
      "Epoch [134/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [134/300], Step [260/384], Loss: 99.6300\n",
      "Epoch [134/300], Step [280/384], Loss: 39.3594\n",
      "Epoch [134/300], Step [300/384], Loss: 28.5861\n",
      "Epoch [134/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [134/300], Step [340/384], Loss: 2.6210\n",
      "Epoch [134/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [134/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [135/300], Step [20/384], Loss: 46.9901\n",
      "Epoch [135/300], Step [40/384], Loss: 44.8953\n",
      "Epoch [135/300], Step [60/384], Loss: 21.1280\n",
      "Epoch [135/300], Step [80/384], Loss: 66.9142\n",
      "Epoch [135/300], Step [100/384], Loss: 155.6039\n",
      "Epoch [135/300], Step [120/384], Loss: 269.4358\n",
      "Epoch [135/300], Step [140/384], Loss: 81.8875\n",
      "Epoch [135/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [135/300], Step [180/384], Loss: 69.9670\n",
      "Epoch [135/300], Step [200/384], Loss: 102.7989\n",
      "Epoch [135/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [135/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [135/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [135/300], Step [280/384], Loss: 360.6921\n",
      "Epoch [135/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [135/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [135/300], Step [340/384], Loss: 10.5510\n",
      "Epoch [135/300], Step [360/384], Loss: 56.5536\n",
      "Epoch [135/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [40/384], Loss: 76.3566\n",
      "Epoch [136/300], Step [60/384], Loss: 14.8083\n",
      "Epoch [136/300], Step [80/384], Loss: 70.4821\n",
      "Epoch [136/300], Step [100/384], Loss: 100.3015\n",
      "Epoch [136/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [220/384], Loss: 61.0855\n",
      "Epoch [136/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [300/384], Loss: 88.6672\n",
      "Epoch [136/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [136/300], Step [360/384], Loss: 130.8455\n",
      "Epoch [136/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [20/384], Loss: 0.7004\n",
      "Epoch [137/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [80/384], Loss: 97.5442\n",
      "Epoch [137/300], Step [100/384], Loss: 42.9693\n",
      "Epoch [137/300], Step [120/384], Loss: 41.3597\n",
      "Epoch [137/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [200/384], Loss: 20.4618\n",
      "Epoch [137/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [240/384], Loss: 95.0204\n",
      "Epoch [137/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [320/384], Loss: 2.4265\n",
      "Epoch [137/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [137/300], Step [380/384], Loss: 59.4110\n",
      "Epoch [138/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [138/300], Step [40/384], Loss: 74.7115\n",
      "Epoch [138/300], Step [60/384], Loss: 93.1198\n",
      "Epoch [138/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [138/300], Step [100/384], Loss: 19.6151\n",
      "Epoch [138/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [138/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [138/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [138/300], Step [180/384], Loss: 21.6987\n",
      "Epoch [138/300], Step [200/384], Loss: 95.8628\n",
      "Epoch [138/300], Step [220/384], Loss: 94.7796\n",
      "Epoch [138/300], Step [240/384], Loss: 30.2115\n",
      "Epoch [138/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [138/300], Step [280/384], Loss: 118.7533\n",
      "Epoch [138/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [138/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [138/300], Step [340/384], Loss: 143.2957\n",
      "Epoch [138/300], Step [360/384], Loss: 39.0904\n",
      "Epoch [138/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [139/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [139/300], Step [40/384], Loss: 134.0055\n",
      "Epoch [139/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [139/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [139/300], Step [100/384], Loss: 34.4969\n",
      "Epoch [139/300], Step [120/384], Loss: 130.4041\n",
      "Epoch [139/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [139/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [139/300], Step [180/384], Loss: 127.7957\n",
      "Epoch [139/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [139/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [139/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [139/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [139/300], Step [280/384], Loss: 83.4455\n",
      "Epoch [139/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [139/300], Step [320/384], Loss: 174.4127\n",
      "Epoch [139/300], Step [340/384], Loss: 21.4917\n",
      "Epoch [139/300], Step [360/384], Loss: 19.4958\n",
      "Epoch [139/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [140/300], Step [20/384], Loss: 71.7616\n",
      "Epoch [140/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [140/300], Step [60/384], Loss: 43.3677\n",
      "Epoch [140/300], Step [80/384], Loss: 22.6287\n",
      "Epoch [140/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [140/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [140/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [140/300], Step [160/384], Loss: 103.3448\n",
      "Epoch [140/300], Step [180/384], Loss: 14.3964\n",
      "Epoch [140/300], Step [200/384], Loss: 31.9149\n",
      "Epoch [140/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [140/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [140/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [140/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [140/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [140/300], Step [320/384], Loss: 50.5277\n",
      "Epoch [140/300], Step [340/384], Loss: 68.0798\n",
      "Epoch [140/300], Step [360/384], Loss: 55.2573\n",
      "Epoch [140/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [141/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [141/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [141/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [141/300], Step [80/384], Loss: 103.9211\n",
      "Epoch [141/300], Step [100/384], Loss: 171.6582\n",
      "Epoch [141/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [141/300], Step [140/384], Loss: 49.6269\n",
      "Epoch [141/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [141/300], Step [180/384], Loss: 162.7660\n",
      "Epoch [141/300], Step [200/384], Loss: 19.2747\n",
      "Epoch [141/300], Step [220/384], Loss: 86.4363\n",
      "Epoch [141/300], Step [240/384], Loss: 112.7025\n",
      "Epoch [141/300], Step [260/384], Loss: 86.9685\n",
      "Epoch [141/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [141/300], Step [300/384], Loss: 28.6568\n",
      "Epoch [141/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [141/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [141/300], Step [360/384], Loss: 65.0685\n",
      "Epoch [141/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [20/384], Loss: 64.5537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [142/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [60/384], Loss: 12.5449\n",
      "Epoch [142/300], Step [80/384], Loss: 192.1929\n",
      "Epoch [142/300], Step [100/384], Loss: 31.3975\n",
      "Epoch [142/300], Step [120/384], Loss: 79.5724\n",
      "Epoch [142/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [160/384], Loss: 6.7179\n",
      "Epoch [142/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [320/384], Loss: 0.3007\n",
      "Epoch [142/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [142/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [143/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [143/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [143/300], Step [60/384], Loss: 35.1085\n",
      "Epoch [143/300], Step [80/384], Loss: 21.4054\n",
      "Epoch [143/300], Step [100/384], Loss: 87.6762\n",
      "Epoch [143/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [143/300], Step [140/384], Loss: 256.5464\n",
      "Epoch [143/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [143/300], Step [180/384], Loss: 122.4275\n",
      "Epoch [143/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [143/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [143/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [143/300], Step [260/384], Loss: 153.5206\n",
      "Epoch [143/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [143/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [143/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [143/300], Step [340/384], Loss: 98.5045\n",
      "Epoch [143/300], Step [360/384], Loss: 2.1332\n",
      "Epoch [143/300], Step [380/384], Loss: 11.2849\n",
      "Epoch [144/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [144/300], Step [40/384], Loss: 17.2068\n",
      "Epoch [144/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [144/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [144/300], Step [100/384], Loss: 23.0742\n",
      "Epoch [144/300], Step [120/384], Loss: 46.5077\n",
      "Epoch [144/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [144/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [144/300], Step [180/384], Loss: 101.2035\n",
      "Epoch [144/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [144/300], Step [220/384], Loss: 84.7562\n",
      "Epoch [144/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [144/300], Step [260/384], Loss: 134.3900\n",
      "Epoch [144/300], Step [280/384], Loss: 15.7234\n",
      "Epoch [144/300], Step [300/384], Loss: 47.9924\n",
      "Epoch [144/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [144/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [144/300], Step [360/384], Loss: 9.2570\n",
      "Epoch [144/300], Step [380/384], Loss: 146.1013\n",
      "Epoch [145/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [120/384], Loss: 7.4511\n",
      "Epoch [145/300], Step [140/384], Loss: 102.3122\n",
      "Epoch [145/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [180/384], Loss: 82.2172\n",
      "Epoch [145/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [220/384], Loss: 81.7306\n",
      "Epoch [145/300], Step [240/384], Loss: 32.8217\n",
      "Epoch [145/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [280/384], Loss: 97.0748\n",
      "Epoch [145/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [145/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [40/384], Loss: 31.5609\n",
      "Epoch [146/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [120/384], Loss: 104.7340\n",
      "Epoch [146/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [200/384], Loss: 78.7272\n",
      "Epoch [146/300], Step [220/384], Loss: 87.6396\n",
      "Epoch [146/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [260/384], Loss: 115.7288\n",
      "Epoch [146/300], Step [280/384], Loss: 13.1013\n",
      "Epoch [146/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [146/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [180/384], Loss: 72.7958\n",
      "Epoch [147/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [220/384], Loss: 142.4364\n",
      "Epoch [147/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [280/384], Loss: 23.0325\n",
      "Epoch [147/300], Step [300/384], Loss: 82.2201\n",
      "Epoch [147/300], Step [320/384], Loss: 9.6200\n",
      "Epoch [147/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [147/300], Step [360/384], Loss: 13.5532\n",
      "Epoch [147/300], Step [380/384], Loss: 30.9145\n",
      "Epoch [148/300], Step [20/384], Loss: 47.2047\n",
      "Epoch [148/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [260/384], Loss: 4.5318\n",
      "Epoch [148/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [340/384], Loss: 10.6583\n",
      "Epoch [148/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [148/300], Step [380/384], Loss: 65.1032\n",
      "Epoch [149/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [149/300], Step [40/384], Loss: 68.5076\n",
      "Epoch [149/300], Step [60/384], Loss: 199.6782\n",
      "Epoch [149/300], Step [80/384], Loss: 335.9071\n",
      "Epoch [149/300], Step [100/384], Loss: 70.7929\n",
      "Epoch [149/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [149/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [149/300], Step [160/384], Loss: 62.3714\n",
      "Epoch [149/300], Step [180/384], Loss: 141.2962\n",
      "Epoch [149/300], Step [200/384], Loss: 58.0084\n",
      "Epoch [149/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [149/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [149/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [149/300], Step [280/384], Loss: 156.7125\n",
      "Epoch [149/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [149/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [149/300], Step [340/384], Loss: 47.2918\n",
      "Epoch [149/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [149/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [150/300], Step [20/384], Loss: 40.2254\n",
      "Epoch [150/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [150/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [150/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [150/300], Step [100/384], Loss: 118.8955\n",
      "Epoch [150/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [150/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [150/300], Step [160/384], Loss: 54.4890\n",
      "Epoch [150/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [150/300], Step [200/384], Loss: 16.0335\n",
      "Epoch [150/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [150/300], Step [240/384], Loss: 94.2004\n",
      "Epoch [150/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [150/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [150/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [150/300], Step [320/384], Loss: 2.1679\n",
      "Epoch [150/300], Step [340/384], Loss: 55.1414\n",
      "Epoch [150/300], Step [360/384], Loss: 40.7761\n",
      "Epoch [150/300], Step [380/384], Loss: 41.9004\n",
      "Epoch [151/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [151/300], Step [40/384], Loss: 277.7157\n",
      "Epoch [151/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [151/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [151/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [151/300], Step [120/384], Loss: 62.2382\n",
      "Epoch [151/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [151/300], Step [160/384], Loss: 9.7044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [151/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [151/300], Step [200/384], Loss: 76.6071\n",
      "Epoch [151/300], Step [220/384], Loss: 14.0842\n",
      "Epoch [151/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [151/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [151/300], Step [280/384], Loss: 68.9109\n",
      "Epoch [151/300], Step [300/384], Loss: 19.7474\n",
      "Epoch [151/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [151/300], Step [340/384], Loss: 212.8303\n",
      "Epoch [151/300], Step [360/384], Loss: 77.5794\n",
      "Epoch [151/300], Step [380/384], Loss: 49.7297\n",
      "Epoch [152/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [152/300], Step [40/384], Loss: 138.0891\n",
      "Epoch [152/300], Step [60/384], Loss: 210.0233\n",
      "Epoch [152/300], Step [80/384], Loss: 48.9976\n",
      "Epoch [152/300], Step [100/384], Loss: 98.3217\n",
      "Epoch [152/300], Step [120/384], Loss: 144.9498\n",
      "Epoch [152/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [152/300], Step [160/384], Loss: 122.2548\n",
      "Epoch [152/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [152/300], Step [200/384], Loss: 1.8128\n",
      "Epoch [152/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [152/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [152/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [152/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [152/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [152/300], Step [320/384], Loss: 99.9719\n",
      "Epoch [152/300], Step [340/384], Loss: 19.2558\n",
      "Epoch [152/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [152/300], Step [380/384], Loss: 85.7670\n",
      "Epoch [153/300], Step [20/384], Loss: 93.2559\n",
      "Epoch [153/300], Step [40/384], Loss: 111.9173\n",
      "Epoch [153/300], Step [60/384], Loss: 10.3465\n",
      "Epoch [153/300], Step [80/384], Loss: 18.3962\n",
      "Epoch [153/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [153/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [153/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [153/300], Step [160/384], Loss: 19.6624\n",
      "Epoch [153/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [153/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [153/300], Step [220/384], Loss: 79.0462\n",
      "Epoch [153/300], Step [240/384], Loss: 124.1085\n",
      "Epoch [153/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [153/300], Step [280/384], Loss: 109.8317\n",
      "Epoch [153/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [153/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [153/300], Step [340/384], Loss: 290.5719\n",
      "Epoch [153/300], Step [360/384], Loss: 10.2883\n",
      "Epoch [153/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [154/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [154/300], Step [40/384], Loss: 39.3566\n",
      "Epoch [154/300], Step [60/384], Loss: 2.7412\n",
      "Epoch [154/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [154/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [154/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [154/300], Step [140/384], Loss: 125.0055\n",
      "Epoch [154/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [154/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [154/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [154/300], Step [220/384], Loss: 208.8940\n",
      "Epoch [154/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [154/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [154/300], Step [280/384], Loss: 148.5391\n",
      "Epoch [154/300], Step [300/384], Loss: 127.3157\n",
      "Epoch [154/300], Step [320/384], Loss: 138.8237\n",
      "Epoch [154/300], Step [340/384], Loss: 39.9758\n",
      "Epoch [154/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [154/300], Step [380/384], Loss: 84.4258\n",
      "Epoch [155/300], Step [20/384], Loss: 10.2348\n",
      "Epoch [155/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [60/384], Loss: 4.1403\n",
      "Epoch [155/300], Step [80/384], Loss: 34.5804\n",
      "Epoch [155/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [120/384], Loss: 145.2737\n",
      "Epoch [155/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [160/384], Loss: 86.5055\n",
      "Epoch [155/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [340/384], Loss: 32.5454\n",
      "Epoch [155/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [155/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [156/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [156/300], Step [40/384], Loss: 44.3065\n",
      "Epoch [156/300], Step [60/384], Loss: 89.8918\n",
      "Epoch [156/300], Step [80/384], Loss: 63.2362\n",
      "Epoch [156/300], Step [100/384], Loss: 74.8260\n",
      "Epoch [156/300], Step [120/384], Loss: 300.3027\n",
      "Epoch [156/300], Step [140/384], Loss: 21.3530\n",
      "Epoch [156/300], Step [160/384], Loss: 57.2164\n",
      "Epoch [156/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [156/300], Step [200/384], Loss: 68.5259\n",
      "Epoch [156/300], Step [220/384], Loss: 12.4116\n",
      "Epoch [156/300], Step [240/384], Loss: 145.8539\n",
      "Epoch [156/300], Step [260/384], Loss: 23.8395\n",
      "Epoch [156/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [156/300], Step [300/384], Loss: 93.8936\n",
      "Epoch [156/300], Step [320/384], Loss: 32.4656\n",
      "Epoch [156/300], Step [340/384], Loss: 5.8254\n",
      "Epoch [156/300], Step [360/384], Loss: 44.5769\n",
      "Epoch [156/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [157/300], Step [20/384], Loss: 48.9246\n",
      "Epoch [157/300], Step [40/384], Loss: 0.6296\n",
      "Epoch [157/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [157/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [157/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [157/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [157/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [157/300], Step [160/384], Loss: 117.2006\n",
      "Epoch [157/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [157/300], Step [200/384], Loss: 27.6163\n",
      "Epoch [157/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [157/300], Step [240/384], Loss: 0.2012\n",
      "Epoch [157/300], Step [260/384], Loss: 64.2891\n",
      "Epoch [157/300], Step [280/384], Loss: 49.4550\n",
      "Epoch [157/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [157/300], Step [320/384], Loss: 27.0960\n",
      "Epoch [157/300], Step [340/384], Loss: 16.0205\n",
      "Epoch [157/300], Step [360/384], Loss: 76.6633\n",
      "Epoch [157/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [60/384], Loss: 133.1592\n",
      "Epoch [158/300], Step [80/384], Loss: 112.0204\n",
      "Epoch [158/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [180/384], Loss: 25.5817\n",
      "Epoch [158/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [240/384], Loss: 76.3164\n",
      "Epoch [158/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [300/384], Loss: 78.2935\n",
      "Epoch [158/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [340/384], Loss: 28.1006\n",
      "Epoch [158/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [158/300], Step [380/384], Loss: 21.2680\n",
      "Epoch [159/300], Step [20/384], Loss: 32.6972\n",
      "Epoch [159/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [159/300], Step [60/384], Loss: 60.8468\n",
      "Epoch [159/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [159/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [159/300], Step [120/384], Loss: 53.7954\n",
      "Epoch [159/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [159/300], Step [160/384], Loss: 130.6855\n",
      "Epoch [159/300], Step [180/384], Loss: 63.6692\n",
      "Epoch [159/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [159/300], Step [220/384], Loss: 53.0741\n",
      "Epoch [159/300], Step [240/384], Loss: 115.4742\n",
      "Epoch [159/300], Step [260/384], Loss: 229.9058\n",
      "Epoch [159/300], Step [280/384], Loss: 45.2109\n",
      "Epoch [159/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [159/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [159/300], Step [340/384], Loss: 115.4946\n",
      "Epoch [159/300], Step [360/384], Loss: 45.7264\n",
      "Epoch [159/300], Step [380/384], Loss: 51.5060\n",
      "Epoch [160/300], Step [20/384], Loss: 50.1427\n",
      "Epoch [160/300], Step [40/384], Loss: 244.2474\n",
      "Epoch [160/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [160/300], Step [80/384], Loss: 84.5985\n",
      "Epoch [160/300], Step [100/384], Loss: 152.8909\n",
      "Epoch [160/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [160/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [160/300], Step [160/384], Loss: 87.7717\n",
      "Epoch [160/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [160/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [160/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [160/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [160/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [160/300], Step [280/384], Loss: 95.1757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/300], Step [300/384], Loss: 62.6444\n",
      "Epoch [160/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [160/300], Step [340/384], Loss: 176.6499\n",
      "Epoch [160/300], Step [360/384], Loss: 80.1535\n",
      "Epoch [160/300], Step [380/384], Loss: 35.0909\n",
      "Epoch [161/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [161/300], Step [40/384], Loss: 70.8861\n",
      "Epoch [161/300], Step [60/384], Loss: 34.3290\n",
      "Epoch [161/300], Step [80/384], Loss: 69.3589\n",
      "Epoch [161/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [161/300], Step [120/384], Loss: 60.4135\n",
      "Epoch [161/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [161/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [161/300], Step [180/384], Loss: 24.4164\n",
      "Epoch [161/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [161/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [161/300], Step [240/384], Loss: 189.5427\n",
      "Epoch [161/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [161/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [161/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [161/300], Step [320/384], Loss: 7.6498\n",
      "Epoch [161/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [161/300], Step [360/384], Loss: 62.6016\n",
      "Epoch [161/300], Step [380/384], Loss: 104.1482\n",
      "Epoch [162/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [162/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [162/300], Step [60/384], Loss: 14.8711\n",
      "Epoch [162/300], Step [80/384], Loss: 54.8937\n",
      "Epoch [162/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [162/300], Step [120/384], Loss: 31.7532\n",
      "Epoch [162/300], Step [140/384], Loss: 17.2883\n",
      "Epoch [162/300], Step [160/384], Loss: 14.2525\n",
      "Epoch [162/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [162/300], Step [200/384], Loss: 77.3977\n",
      "Epoch [162/300], Step [220/384], Loss: 3.5144\n",
      "Epoch [162/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [162/300], Step [260/384], Loss: 40.3006\n",
      "Epoch [162/300], Step [280/384], Loss: 42.5911\n",
      "Epoch [162/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [162/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [162/300], Step [340/384], Loss: 62.5730\n",
      "Epoch [162/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [162/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [40/384], Loss: 175.8138\n",
      "Epoch [163/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [80/384], Loss: 168.6170\n",
      "Epoch [163/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [160/384], Loss: 81.1200\n",
      "Epoch [163/300], Step [180/384], Loss: 117.4156\n",
      "Epoch [163/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [240/384], Loss: 326.7500\n",
      "Epoch [163/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [300/384], Loss: 34.1677\n",
      "Epoch [163/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [163/300], Step [360/384], Loss: 13.9432\n",
      "Epoch [163/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [164/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [164/300], Step [40/384], Loss: 85.7935\n",
      "Epoch [164/300], Step [60/384], Loss: 31.2702\n",
      "Epoch [164/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [164/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [164/300], Step [120/384], Loss: 25.7433\n",
      "Epoch [164/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [164/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [164/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [164/300], Step [200/384], Loss: 74.0078\n",
      "Epoch [164/300], Step [220/384], Loss: 126.7151\n",
      "Epoch [164/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [164/300], Step [260/384], Loss: 38.1629\n",
      "Epoch [164/300], Step [280/384], Loss: 65.7379\n",
      "Epoch [164/300], Step [300/384], Loss: 120.7165\n",
      "Epoch [164/300], Step [320/384], Loss: 3.5030\n",
      "Epoch [164/300], Step [340/384], Loss: 108.5022\n",
      "Epoch [164/300], Step [360/384], Loss: 100.3351\n",
      "Epoch [164/300], Step [380/384], Loss: 83.7333\n",
      "Epoch [165/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [60/384], Loss: 26.3569\n",
      "Epoch [165/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [140/384], Loss: 0.0046\n",
      "Epoch [165/300], Step [160/384], Loss: 62.5051\n",
      "Epoch [165/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [240/384], Loss: 279.3412\n",
      "Epoch [165/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [300/384], Loss: 62.5405\n",
      "Epoch [165/300], Step [320/384], Loss: 106.3454\n",
      "Epoch [165/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [165/300], Step [380/384], Loss: 8.7459\n",
      "Epoch [166/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [200/384], Loss: 50.5860\n",
      "Epoch [166/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [280/384], Loss: 33.3041\n",
      "Epoch [166/300], Step [300/384], Loss: 133.5946\n",
      "Epoch [166/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [166/300], Step [360/384], Loss: 16.9448\n",
      "Epoch [166/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [167/300], Step [20/384], Loss: 0.0004\n",
      "Epoch [167/300], Step [40/384], Loss: 102.7996\n",
      "Epoch [167/300], Step [60/384], Loss: 30.2087\n",
      "Epoch [167/300], Step [80/384], Loss: 37.0848\n",
      "Epoch [167/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [167/300], Step [120/384], Loss: 46.4092\n",
      "Epoch [167/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [167/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [167/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [167/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [167/300], Step [220/384], Loss: 51.9637\n",
      "Epoch [167/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [167/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [167/300], Step [280/384], Loss: 69.8141\n",
      "Epoch [167/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [167/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [167/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [167/300], Step [360/384], Loss: 178.6585\n",
      "Epoch [167/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [100/384], Loss: 110.3217\n",
      "Epoch [168/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [140/384], Loss: 127.5203\n",
      "Epoch [168/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [220/384], Loss: 202.6714\n",
      "Epoch [168/300], Step [240/384], Loss: 100.2216\n",
      "Epoch [168/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [340/384], Loss: 80.0264\n",
      "Epoch [168/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [168/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [169/300], Step [20/384], Loss: 6.4382\n",
      "Epoch [169/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [169/300], Step [60/384], Loss: 21.6040\n",
      "Epoch [169/300], Step [80/384], Loss: 63.3695\n",
      "Epoch [169/300], Step [100/384], Loss: 77.2961\n",
      "Epoch [169/300], Step [120/384], Loss: 51.4139\n",
      "Epoch [169/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [169/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [169/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [169/300], Step [200/384], Loss: 63.6447\n",
      "Epoch [169/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [169/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [169/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [169/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [169/300], Step [300/384], Loss: 80.9715\n",
      "Epoch [169/300], Step [320/384], Loss: 50.0839\n",
      "Epoch [169/300], Step [340/384], Loss: 55.4441\n",
      "Epoch [169/300], Step [360/384], Loss: 75.5243\n",
      "Epoch [169/300], Step [380/384], Loss: 2.0761\n",
      "Epoch [170/300], Step [20/384], Loss: 118.0931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [170/300], Step [60/384], Loss: 21.4192\n",
      "Epoch [170/300], Step [80/384], Loss: 55.0193\n",
      "Epoch [170/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [170/300], Step [120/384], Loss: 20.9745\n",
      "Epoch [170/300], Step [140/384], Loss: 110.0933\n",
      "Epoch [170/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [170/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [170/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [170/300], Step [220/384], Loss: 13.5239\n",
      "Epoch [170/300], Step [240/384], Loss: 58.0659\n",
      "Epoch [170/300], Step [260/384], Loss: 69.5065\n",
      "Epoch [170/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [170/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [170/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [170/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [170/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [170/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [171/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [171/300], Step [40/384], Loss: 61.7528\n",
      "Epoch [171/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [171/300], Step [80/384], Loss: 32.2125\n",
      "Epoch [171/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [171/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [171/300], Step [140/384], Loss: 241.3755\n",
      "Epoch [171/300], Step [160/384], Loss: 210.2234\n",
      "Epoch [171/300], Step [180/384], Loss: 121.1546\n",
      "Epoch [171/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [171/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [171/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [171/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [171/300], Step [280/384], Loss: 40.3313\n",
      "Epoch [171/300], Step [300/384], Loss: 82.6238\n",
      "Epoch [171/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [171/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [171/300], Step [360/384], Loss: 67.7184\n",
      "Epoch [171/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [172/300], Step [20/384], Loss: 61.6753\n",
      "Epoch [172/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [172/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [172/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [172/300], Step [100/384], Loss: 48.2366\n",
      "Epoch [172/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [172/300], Step [140/384], Loss: 116.8444\n",
      "Epoch [172/300], Step [160/384], Loss: 8.1600\n",
      "Epoch [172/300], Step [180/384], Loss: 17.7960\n",
      "Epoch [172/300], Step [200/384], Loss: 31.9575\n",
      "Epoch [172/300], Step [220/384], Loss: 190.0090\n",
      "Epoch [172/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [172/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [172/300], Step [280/384], Loss: 64.2096\n",
      "Epoch [172/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [172/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [172/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [172/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [172/300], Step [380/384], Loss: 126.7630\n",
      "Epoch [173/300], Step [20/384], Loss: 32.6679\n",
      "Epoch [173/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [173/300], Step [60/384], Loss: 44.3184\n",
      "Epoch [173/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [173/300], Step [100/384], Loss: 91.5999\n",
      "Epoch [173/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [173/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [173/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [173/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [173/300], Step [200/384], Loss: 208.9897\n",
      "Epoch [173/300], Step [220/384], Loss: 16.8207\n",
      "Epoch [173/300], Step [240/384], Loss: 63.9033\n",
      "Epoch [173/300], Step [260/384], Loss: 126.5014\n",
      "Epoch [173/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [173/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [173/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [173/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [173/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [173/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [60/384], Loss: 170.2302\n",
      "Epoch [174/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [120/384], Loss: 48.8527\n",
      "Epoch [174/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [200/384], Loss: 83.1549\n",
      "Epoch [174/300], Step [220/384], Loss: 56.4389\n",
      "Epoch [174/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [260/384], Loss: 97.0699\n",
      "Epoch [174/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [300/384], Loss: 194.1529\n",
      "Epoch [174/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [174/300], Step [380/384], Loss: 297.0144\n",
      "Epoch [175/300], Step [20/384], Loss: 35.2113\n",
      "Epoch [175/300], Step [40/384], Loss: 140.0403\n",
      "Epoch [175/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [175/300], Step [80/384], Loss: 193.8269\n",
      "Epoch [175/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [175/300], Step [120/384], Loss: 123.6791\n",
      "Epoch [175/300], Step [140/384], Loss: 9.4927\n",
      "Epoch [175/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [175/300], Step [180/384], Loss: 19.0139\n",
      "Epoch [175/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [175/300], Step [220/384], Loss: 48.7050\n",
      "Epoch [175/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [175/300], Step [260/384], Loss: 239.4091\n",
      "Epoch [175/300], Step [280/384], Loss: 79.0222\n",
      "Epoch [175/300], Step [300/384], Loss: 294.3418\n",
      "Epoch [175/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [175/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [175/300], Step [360/384], Loss: 67.9564\n",
      "Epoch [175/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [176/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [176/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [176/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [176/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [176/300], Step [100/384], Loss: 0.0008\n",
      "Epoch [176/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [176/300], Step [140/384], Loss: 49.9360\n",
      "Epoch [176/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [176/300], Step [180/384], Loss: 47.8242\n",
      "Epoch [176/300], Step [200/384], Loss: 187.9336\n",
      "Epoch [176/300], Step [220/384], Loss: 12.8388\n",
      "Epoch [176/300], Step [240/384], Loss: 52.1516\n",
      "Epoch [176/300], Step [260/384], Loss: 9.7622\n",
      "Epoch [176/300], Step [280/384], Loss: 35.5260\n",
      "Epoch [176/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [176/300], Step [320/384], Loss: 78.5253\n",
      "Epoch [176/300], Step [340/384], Loss: 100.4362\n",
      "Epoch [176/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [176/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [60/384], Loss: 131.3769\n",
      "Epoch [177/300], Step [80/384], Loss: 125.3434\n",
      "Epoch [177/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [180/384], Loss: 75.3459\n",
      "Epoch [177/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [300/384], Loss: 24.8569\n",
      "Epoch [177/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [177/300], Step [380/384], Loss: 195.1969\n",
      "Epoch [178/300], Step [20/384], Loss: 130.4304\n",
      "Epoch [178/300], Step [40/384], Loss: 15.4773\n",
      "Epoch [178/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [80/384], Loss: 6.9857\n",
      "Epoch [178/300], Step [100/384], Loss: 52.1287\n",
      "Epoch [178/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [280/384], Loss: 73.5894\n",
      "Epoch [178/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [340/384], Loss: 171.2668\n",
      "Epoch [178/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [178/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [179/300], Step [20/384], Loss: 72.8179\n",
      "Epoch [179/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [179/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [179/300], Step [80/384], Loss: 12.0149\n",
      "Epoch [179/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [179/300], Step [120/384], Loss: 324.1261\n",
      "Epoch [179/300], Step [140/384], Loss: 27.0471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [179/300], Step [160/384], Loss: 7.4584\n",
      "Epoch [179/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [179/300], Step [200/384], Loss: 117.4469\n",
      "Epoch [179/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [179/300], Step [240/384], Loss: 105.0629\n",
      "Epoch [179/300], Step [260/384], Loss: 39.7745\n",
      "Epoch [179/300], Step [280/384], Loss: 19.2070\n",
      "Epoch [179/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [179/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [179/300], Step [340/384], Loss: 114.7139\n",
      "Epoch [179/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [179/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [40/384], Loss: 154.3595\n",
      "Epoch [180/300], Step [60/384], Loss: 38.2705\n",
      "Epoch [180/300], Step [80/384], Loss: 0.1058\n",
      "Epoch [180/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [140/384], Loss: 23.2117\n",
      "Epoch [180/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [180/384], Loss: 69.9399\n",
      "Epoch [180/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [220/384], Loss: 75.5549\n",
      "Epoch [180/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [340/384], Loss: 29.8522\n",
      "Epoch [180/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [180/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [181/300], Step [20/384], Loss: 0.9023\n",
      "Epoch [181/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [181/300], Step [60/384], Loss: 19.1162\n",
      "Epoch [181/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [181/300], Step [100/384], Loss: 49.6537\n",
      "Epoch [181/300], Step [120/384], Loss: 44.3548\n",
      "Epoch [181/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [181/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [181/300], Step [180/384], Loss: 81.7153\n",
      "Epoch [181/300], Step [200/384], Loss: 118.7766\n",
      "Epoch [181/300], Step [220/384], Loss: 23.9243\n",
      "Epoch [181/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [181/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [181/300], Step [280/384], Loss: 188.1190\n",
      "Epoch [181/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [181/300], Step [320/384], Loss: 5.0536\n",
      "Epoch [181/300], Step [340/384], Loss: 90.3514\n",
      "Epoch [181/300], Step [360/384], Loss: 68.9295\n",
      "Epoch [181/300], Step [380/384], Loss: 77.2979\n",
      "Epoch [182/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [40/384], Loss: 33.7381\n",
      "Epoch [182/300], Step [60/384], Loss: 24.1916\n",
      "Epoch [182/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [140/384], Loss: 55.3986\n",
      "Epoch [182/300], Step [160/384], Loss: 29.9794\n",
      "Epoch [182/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [220/384], Loss: 19.2690\n",
      "Epoch [182/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [182/300], Step [380/384], Loss: 21.8768\n",
      "Epoch [183/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [183/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [183/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [183/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [183/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [183/300], Step [120/384], Loss: 13.3943\n",
      "Epoch [183/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [183/300], Step [160/384], Loss: 81.2613\n",
      "Epoch [183/300], Step [180/384], Loss: 32.3930\n",
      "Epoch [183/300], Step [200/384], Loss: 62.9044\n",
      "Epoch [183/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [183/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [183/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [183/300], Step [280/384], Loss: 65.5033\n",
      "Epoch [183/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [183/300], Step [320/384], Loss: 106.7370\n",
      "Epoch [183/300], Step [340/384], Loss: 5.6373\n",
      "Epoch [183/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [183/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [120/384], Loss: 47.4525\n",
      "Epoch [184/300], Step [140/384], Loss: 7.5628\n",
      "Epoch [184/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [200/384], Loss: 4.5851\n",
      "Epoch [184/300], Step [220/384], Loss: 89.7015\n",
      "Epoch [184/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [280/384], Loss: 7.9165\n",
      "Epoch [184/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [320/384], Loss: 115.1611\n",
      "Epoch [184/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [184/300], Step [380/384], Loss: 67.6461\n",
      "Epoch [185/300], Step [20/384], Loss: 252.8656\n",
      "Epoch [185/300], Step [40/384], Loss: 26.9697\n",
      "Epoch [185/300], Step [60/384], Loss: 0.0010\n",
      "Epoch [185/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [185/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [185/300], Step [120/384], Loss: 27.9600\n",
      "Epoch [185/300], Step [140/384], Loss: 65.5279\n",
      "Epoch [185/300], Step [160/384], Loss: 104.0268\n",
      "Epoch [185/300], Step [180/384], Loss: 44.6305\n",
      "Epoch [185/300], Step [200/384], Loss: 27.5389\n",
      "Epoch [185/300], Step [220/384], Loss: 74.3154\n",
      "Epoch [185/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [185/300], Step [260/384], Loss: 84.8058\n",
      "Epoch [185/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [185/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [185/300], Step [320/384], Loss: 16.0421\n",
      "Epoch [185/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [185/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [185/300], Step [380/384], Loss: 72.0664\n",
      "Epoch [186/300], Step [20/384], Loss: 83.0976\n",
      "Epoch [186/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [80/384], Loss: 22.4439\n",
      "Epoch [186/300], Step [100/384], Loss: 152.7017\n",
      "Epoch [186/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [160/384], Loss: 91.6865\n",
      "Epoch [186/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [186/300], Step [360/384], Loss: 0.5054\n",
      "Epoch [186/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [40/384], Loss: 17.8157\n",
      "Epoch [187/300], Step [60/384], Loss: 25.3595\n",
      "Epoch [187/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [120/384], Loss: 8.8017\n",
      "Epoch [187/300], Step [140/384], Loss: 118.8915\n",
      "Epoch [187/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [220/384], Loss: 1.2975\n",
      "Epoch [187/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [280/384], Loss: 53.1639\n",
      "Epoch [187/300], Step [300/384], Loss: 12.8603\n",
      "Epoch [187/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [187/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [20/384], Loss: 21.5700\n",
      "Epoch [188/300], Step [40/384], Loss: 47.7890\n",
      "Epoch [188/300], Step [60/384], Loss: 41.6121\n",
      "Epoch [188/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [280/384], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [188/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [320/384], Loss: 50.1470\n",
      "Epoch [188/300], Step [340/384], Loss: 58.2402\n",
      "Epoch [188/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [188/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [189/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [189/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [189/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [189/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [189/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [189/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [189/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [189/300], Step [160/384], Loss: 19.2767\n",
      "Epoch [189/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [189/300], Step [200/384], Loss: 56.8644\n",
      "Epoch [189/300], Step [220/384], Loss: 0.8616\n",
      "Epoch [189/300], Step [240/384], Loss: 113.1449\n",
      "Epoch [189/300], Step [260/384], Loss: 137.7244\n",
      "Epoch [189/300], Step [280/384], Loss: 0.0035\n",
      "Epoch [189/300], Step [300/384], Loss: 1.7327\n",
      "Epoch [189/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [189/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [189/300], Step [360/384], Loss: 30.9251\n",
      "Epoch [189/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [80/384], Loss: 43.2862\n",
      "Epoch [190/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [140/384], Loss: 3.7333\n",
      "Epoch [190/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [220/384], Loss: 53.4685\n",
      "Epoch [190/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [190/300], Step [360/384], Loss: 4.0996\n",
      "Epoch [190/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [60/384], Loss: 110.6926\n",
      "Epoch [191/300], Step [80/384], Loss: 139.0196\n",
      "Epoch [191/300], Step [100/384], Loss: 51.7465\n",
      "Epoch [191/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [220/384], Loss: 4.8637\n",
      "Epoch [191/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [260/384], Loss: 11.8131\n",
      "Epoch [191/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [320/384], Loss: 67.8462\n",
      "Epoch [191/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [191/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [140/384], Loss: 61.7199\n",
      "Epoch [192/300], Step [160/384], Loss: 6.0502\n",
      "Epoch [192/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [200/384], Loss: 5.2964\n",
      "Epoch [192/300], Step [220/384], Loss: 119.2469\n",
      "Epoch [192/300], Step [240/384], Loss: 56.5862\n",
      "Epoch [192/300], Step [260/384], Loss: 11.4100\n",
      "Epoch [192/300], Step [280/384], Loss: 16.8747\n",
      "Epoch [192/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [192/300], Step [380/384], Loss: 1.7323\n",
      "Epoch [193/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [193/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [193/300], Step [60/384], Loss: 16.6019\n",
      "Epoch [193/300], Step [80/384], Loss: 73.5092\n",
      "Epoch [193/300], Step [100/384], Loss: 29.9294\n",
      "Epoch [193/300], Step [120/384], Loss: 162.0404\n",
      "Epoch [193/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [193/300], Step [160/384], Loss: 26.6098\n",
      "Epoch [193/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [193/300], Step [200/384], Loss: 59.2473\n",
      "Epoch [193/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [193/300], Step [240/384], Loss: 42.3694\n",
      "Epoch [193/300], Step [260/384], Loss: 210.0524\n",
      "Epoch [193/300], Step [280/384], Loss: 138.1365\n",
      "Epoch [193/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [193/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [193/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [193/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [193/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [60/384], Loss: 31.4421\n",
      "Epoch [194/300], Step [80/384], Loss: 77.6345\n",
      "Epoch [194/300], Step [100/384], Loss: 38.1752\n",
      "Epoch [194/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [140/384], Loss: 80.0974\n",
      "Epoch [194/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [180/384], Loss: 51.5817\n",
      "Epoch [194/300], Step [200/384], Loss: 26.9699\n",
      "Epoch [194/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [320/384], Loss: 28.2718\n",
      "Epoch [194/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [194/300], Step [380/384], Loss: 0.0503\n",
      "Epoch [195/300], Step [20/384], Loss: 99.2916\n",
      "Epoch [195/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [195/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [195/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [195/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [195/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [195/300], Step [140/384], Loss: 24.3566\n",
      "Epoch [195/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [195/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [195/300], Step [200/384], Loss: 0.0006\n",
      "Epoch [195/300], Step [220/384], Loss: 138.7916\n",
      "Epoch [195/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [195/300], Step [260/384], Loss: 59.9968\n",
      "Epoch [195/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [195/300], Step [300/384], Loss: 174.3114\n",
      "Epoch [195/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [195/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [195/300], Step [360/384], Loss: 30.3694\n",
      "Epoch [195/300], Step [380/384], Loss: 75.5691\n",
      "Epoch [196/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [100/384], Loss: 15.7302\n",
      "Epoch [196/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [140/384], Loss: 105.3823\n",
      "Epoch [196/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [180/384], Loss: 57.1000\n",
      "Epoch [196/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [320/384], Loss: 24.5765\n",
      "Epoch [196/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [196/300], Step [360/384], Loss: 17.1991\n",
      "Epoch [196/300], Step [380/384], Loss: 12.4742\n",
      "Epoch [197/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [197/300], Step [40/384], Loss: 62.7858\n",
      "Epoch [197/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [197/300], Step [80/384], Loss: 82.8132\n",
      "Epoch [197/300], Step [100/384], Loss: 3.1711\n",
      "Epoch [197/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [197/300], Step [140/384], Loss: 26.2115\n",
      "Epoch [197/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [197/300], Step [180/384], Loss: 91.6276\n",
      "Epoch [197/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [197/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [197/300], Step [240/384], Loss: 25.8250\n",
      "Epoch [197/300], Step [260/384], Loss: 40.6980\n",
      "Epoch [197/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [197/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [197/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [197/300], Step [340/384], Loss: 54.5870\n",
      "Epoch [197/300], Step [360/384], Loss: 9.5129\n",
      "Epoch [197/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [198/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [198/300], Step [40/384], Loss: 39.1593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [198/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [198/300], Step [80/384], Loss: 0.6024\n",
      "Epoch [198/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [198/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [198/300], Step [140/384], Loss: 127.7679\n",
      "Epoch [198/300], Step [160/384], Loss: 85.3749\n",
      "Epoch [198/300], Step [180/384], Loss: 50.9202\n",
      "Epoch [198/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [198/300], Step [220/384], Loss: 101.1535\n",
      "Epoch [198/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [198/300], Step [260/384], Loss: 274.3773\n",
      "Epoch [198/300], Step [280/384], Loss: 73.2569\n",
      "Epoch [198/300], Step [300/384], Loss: 40.3435\n",
      "Epoch [198/300], Step [320/384], Loss: 5.5349\n",
      "Epoch [198/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [198/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [198/300], Step [380/384], Loss: 50.5976\n",
      "Epoch [199/300], Step [20/384], Loss: 19.8670\n",
      "Epoch [199/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [199/300], Step [60/384], Loss: 100.7478\n",
      "Epoch [199/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [199/300], Step [100/384], Loss: 3.2660\n",
      "Epoch [199/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [199/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [199/300], Step [160/384], Loss: 3.3356\n",
      "Epoch [199/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [199/300], Step [200/384], Loss: 40.1672\n",
      "Epoch [199/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [199/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [199/300], Step [260/384], Loss: 50.6148\n",
      "Epoch [199/300], Step [280/384], Loss: 22.4106\n",
      "Epoch [199/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [199/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [199/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [199/300], Step [360/384], Loss: 76.5426\n",
      "Epoch [199/300], Step [380/384], Loss: 19.9726\n",
      "Epoch [200/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [200/300], Step [40/384], Loss: 164.7972\n",
      "Epoch [200/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [200/300], Step [80/384], Loss: 0.0004\n",
      "Epoch [200/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [200/300], Step [120/384], Loss: 38.7555\n",
      "Epoch [200/300], Step [140/384], Loss: 16.1026\n",
      "Epoch [200/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [200/300], Step [180/384], Loss: 110.3258\n",
      "Epoch [200/300], Step [200/384], Loss: 246.2244\n",
      "Epoch [200/300], Step [220/384], Loss: 37.7403\n",
      "Epoch [200/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [200/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [200/300], Step [280/384], Loss: 37.3377\n",
      "Epoch [200/300], Step [300/384], Loss: 6.2006\n",
      "Epoch [200/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [200/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [200/300], Step [360/384], Loss: 28.0637\n",
      "Epoch [200/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [60/384], Loss: 18.3805\n",
      "Epoch [201/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [100/384], Loss: 31.8595\n",
      "Epoch [201/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [220/384], Loss: 114.2906\n",
      "Epoch [201/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [260/384], Loss: 534.7391\n",
      "Epoch [201/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [320/384], Loss: 124.9716\n",
      "Epoch [201/300], Step [340/384], Loss: 9.4585\n",
      "Epoch [201/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [201/300], Step [380/384], Loss: 64.4773\n",
      "Epoch [202/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [202/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [202/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [202/300], Step [80/384], Loss: 49.1414\n",
      "Epoch [202/300], Step [100/384], Loss: 2.2603\n",
      "Epoch [202/300], Step [120/384], Loss: 15.5655\n",
      "Epoch [202/300], Step [140/384], Loss: 18.4240\n",
      "Epoch [202/300], Step [160/384], Loss: 152.6400\n",
      "Epoch [202/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [202/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [202/300], Step [220/384], Loss: 2.4251\n",
      "Epoch [202/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [202/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [202/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [202/300], Step [300/384], Loss: 34.1646\n",
      "Epoch [202/300], Step [320/384], Loss: 3.4204\n",
      "Epoch [202/300], Step [340/384], Loss: 116.9475\n",
      "Epoch [202/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [202/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [203/300], Step [20/384], Loss: 64.0681\n",
      "Epoch [203/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [203/300], Step [60/384], Loss: 22.6499\n",
      "Epoch [203/300], Step [80/384], Loss: 19.7324\n",
      "Epoch [203/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [203/300], Step [120/384], Loss: 244.1254\n",
      "Epoch [203/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [203/300], Step [160/384], Loss: 40.2007\n",
      "Epoch [203/300], Step [180/384], Loss: 73.9555\n",
      "Epoch [203/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [203/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [203/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [203/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [203/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [203/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [203/300], Step [320/384], Loss: 103.1658\n",
      "Epoch [203/300], Step [340/384], Loss: 95.9578\n",
      "Epoch [203/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [203/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [204/300], Step [20/384], Loss: 14.3674\n",
      "Epoch [204/300], Step [40/384], Loss: 89.5556\n",
      "Epoch [204/300], Step [60/384], Loss: 98.7958\n",
      "Epoch [204/300], Step [80/384], Loss: 0.2548\n",
      "Epoch [204/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [204/300], Step [120/384], Loss: 88.2233\n",
      "Epoch [204/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [204/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [204/300], Step [180/384], Loss: 6.6094\n",
      "Epoch [204/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [204/300], Step [220/384], Loss: 3.3286\n",
      "Epoch [204/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [204/300], Step [260/384], Loss: 60.3443\n",
      "Epoch [204/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [204/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [204/300], Step [320/384], Loss: 301.2232\n",
      "Epoch [204/300], Step [340/384], Loss: 51.3971\n",
      "Epoch [204/300], Step [360/384], Loss: 9.3469\n",
      "Epoch [204/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [205/300], Step [20/384], Loss: 4.0133\n",
      "Epoch [205/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [205/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [205/300], Step [80/384], Loss: 39.2706\n",
      "Epoch [205/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [205/300], Step [120/384], Loss: 56.2265\n",
      "Epoch [205/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [205/300], Step [160/384], Loss: 22.1097\n",
      "Epoch [205/300], Step [180/384], Loss: 20.1969\n",
      "Epoch [205/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [205/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [205/300], Step [240/384], Loss: 15.0172\n",
      "Epoch [205/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [205/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [205/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [205/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [205/300], Step [340/384], Loss: 13.6690\n",
      "Epoch [205/300], Step [360/384], Loss: 61.3548\n",
      "Epoch [205/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [60/384], Loss: 21.9003\n",
      "Epoch [206/300], Step [80/384], Loss: 35.3097\n",
      "Epoch [206/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [220/384], Loss: 42.4454\n",
      "Epoch [206/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [280/384], Loss: 114.3040\n",
      "Epoch [206/300], Step [300/384], Loss: 62.0970\n",
      "Epoch [206/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [206/300], Step [340/384], Loss: 28.8759\n",
      "Epoch [206/300], Step [360/384], Loss: 43.3418\n",
      "Epoch [206/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [80/384], Loss: 4.2810\n",
      "Epoch [207/300], Step [100/384], Loss: 20.7713\n",
      "Epoch [207/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [180/384], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [207/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [280/384], Loss: 25.6993\n",
      "Epoch [207/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [340/384], Loss: 57.1590\n",
      "Epoch [207/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [207/300], Step [380/384], Loss: 67.8598\n",
      "Epoch [208/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [208/300], Step [40/384], Loss: 34.9832\n",
      "Epoch [208/300], Step [60/384], Loss: 0.3181\n",
      "Epoch [208/300], Step [80/384], Loss: 282.7851\n",
      "Epoch [208/300], Step [100/384], Loss: 4.6896\n",
      "Epoch [208/300], Step [120/384], Loss: 17.7778\n",
      "Epoch [208/300], Step [140/384], Loss: 32.5707\n",
      "Epoch [208/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [208/300], Step [180/384], Loss: 85.4976\n",
      "Epoch [208/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [208/300], Step [220/384], Loss: 160.2449\n",
      "Epoch [208/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [208/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [208/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [208/300], Step [300/384], Loss: 144.4023\n",
      "Epoch [208/300], Step [320/384], Loss: 71.3742\n",
      "Epoch [208/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [208/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [208/300], Step [380/384], Loss: 6.5959\n",
      "Epoch [209/300], Step [20/384], Loss: 66.4972\n",
      "Epoch [209/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [209/300], Step [60/384], Loss: 166.4146\n",
      "Epoch [209/300], Step [80/384], Loss: 129.1721\n",
      "Epoch [209/300], Step [100/384], Loss: 37.1055\n",
      "Epoch [209/300], Step [120/384], Loss: 97.4545\n",
      "Epoch [209/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [209/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [209/300], Step [180/384], Loss: 350.6536\n",
      "Epoch [209/300], Step [200/384], Loss: 88.0962\n",
      "Epoch [209/300], Step [220/384], Loss: 42.4935\n",
      "Epoch [209/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [209/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [209/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [209/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [209/300], Step [320/384], Loss: 24.3143\n",
      "Epoch [209/300], Step [340/384], Loss: 111.5564\n",
      "Epoch [209/300], Step [360/384], Loss: 65.5789\n",
      "Epoch [209/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [40/384], Loss: 46.4993\n",
      "Epoch [210/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [140/384], Loss: 94.9191\n",
      "Epoch [210/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [200/384], Loss: 47.8695\n",
      "Epoch [210/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [240/384], Loss: 174.9922\n",
      "Epoch [210/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [210/300], Step [340/384], Loss: 166.8248\n",
      "Epoch [210/300], Step [360/384], Loss: 80.9329\n",
      "Epoch [210/300], Step [380/384], Loss: 103.7986\n",
      "Epoch [211/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [40/384], Loss: 6.5729\n",
      "Epoch [211/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [100/384], Loss: 14.8981\n",
      "Epoch [211/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [160/384], Loss: 43.8721\n",
      "Epoch [211/300], Step [180/384], Loss: 91.7775\n",
      "Epoch [211/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [220/384], Loss: 10.2294\n",
      "Epoch [211/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [211/300], Step [360/384], Loss: 12.3595\n",
      "Epoch [211/300], Step [380/384], Loss: 23.6805\n",
      "Epoch [212/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [212/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [212/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [212/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [212/300], Step [100/384], Loss: 85.8250\n",
      "Epoch [212/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [212/300], Step [140/384], Loss: 53.8012\n",
      "Epoch [212/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [212/300], Step [180/384], Loss: 26.2031\n",
      "Epoch [212/300], Step [200/384], Loss: 36.0426\n",
      "Epoch [212/300], Step [220/384], Loss: 0.2587\n",
      "Epoch [212/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [212/300], Step [260/384], Loss: 87.0908\n",
      "Epoch [212/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [212/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [212/300], Step [320/384], Loss: 85.1612\n",
      "Epoch [212/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [212/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [212/300], Step [380/384], Loss: 7.0053\n",
      "Epoch [213/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [200/384], Loss: 31.0549\n",
      "Epoch [213/300], Step [220/384], Loss: 88.9018\n",
      "Epoch [213/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [213/300], Step [360/384], Loss: 60.2097\n",
      "Epoch [213/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [20/384], Loss: 42.7527\n",
      "Epoch [214/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [100/384], Loss: 54.2765\n",
      "Epoch [214/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [180/384], Loss: 3.9400\n",
      "Epoch [214/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [260/384], Loss: 36.6497\n",
      "Epoch [214/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [214/300], Step [320/384], Loss: 60.4394\n",
      "Epoch [214/300], Step [340/384], Loss: 53.1957\n",
      "Epoch [214/300], Step [360/384], Loss: 52.3536\n",
      "Epoch [214/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [215/300], Step [20/384], Loss: 7.0090\n",
      "Epoch [215/300], Step [40/384], Loss: 95.1631\n",
      "Epoch [215/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [215/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [215/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [215/300], Step [120/384], Loss: 13.3858\n",
      "Epoch [215/300], Step [140/384], Loss: 294.0655\n",
      "Epoch [215/300], Step [160/384], Loss: 0.6231\n",
      "Epoch [215/300], Step [180/384], Loss: 23.0038\n",
      "Epoch [215/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [215/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [215/300], Step [240/384], Loss: 102.3077\n",
      "Epoch [215/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [215/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [215/300], Step [300/384], Loss: 130.5449\n",
      "Epoch [215/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [215/300], Step [340/384], Loss: 47.7113\n",
      "Epoch [215/300], Step [360/384], Loss: 50.1296\n",
      "Epoch [215/300], Step [380/384], Loss: 187.0915\n",
      "Epoch [216/300], Step [20/384], Loss: 100.0838\n",
      "Epoch [216/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [216/300], Step [60/384], Loss: 74.0312\n",
      "Epoch [216/300], Step [80/384], Loss: 18.4271\n",
      "Epoch [216/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [216/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [216/300], Step [140/384], Loss: 19.9291\n",
      "Epoch [216/300], Step [160/384], Loss: 33.6093\n",
      "Epoch [216/300], Step [180/384], Loss: 0.0009\n",
      "Epoch [216/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [216/300], Step [220/384], Loss: 9.7747\n",
      "Epoch [216/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [216/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [216/300], Step [280/384], Loss: 46.6763\n",
      "Epoch [216/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [216/300], Step [320/384], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [216/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [216/300], Step [360/384], Loss: 40.9042\n",
      "Epoch [216/300], Step [380/384], Loss: 27.1488\n",
      "Epoch [217/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [217/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [217/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [217/300], Step [80/384], Loss: 18.6371\n",
      "Epoch [217/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [217/300], Step [120/384], Loss: 16.1868\n",
      "Epoch [217/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [217/300], Step [160/384], Loss: 175.3274\n",
      "Epoch [217/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [217/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [217/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [217/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [217/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [217/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [217/300], Step [300/384], Loss: 193.6335\n",
      "Epoch [217/300], Step [320/384], Loss: 45.9305\n",
      "Epoch [217/300], Step [340/384], Loss: 147.1295\n",
      "Epoch [217/300], Step [360/384], Loss: 59.2187\n",
      "Epoch [217/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [20/384], Loss: 10.5952\n",
      "Epoch [218/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [60/384], Loss: 56.1798\n",
      "Epoch [218/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [100/384], Loss: 185.5063\n",
      "Epoch [218/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [160/384], Loss: 77.0655\n",
      "Epoch [218/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [200/384], Loss: 23.1578\n",
      "Epoch [218/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [218/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [219/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [219/300], Step [40/384], Loss: 63.1959\n",
      "Epoch [219/300], Step [60/384], Loss: 79.8691\n",
      "Epoch [219/300], Step [80/384], Loss: 14.0645\n",
      "Epoch [219/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [219/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [219/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [219/300], Step [160/384], Loss: 29.1479\n",
      "Epoch [219/300], Step [180/384], Loss: 137.0475\n",
      "Epoch [219/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [219/300], Step [220/384], Loss: 84.9678\n",
      "Epoch [219/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [219/300], Step [260/384], Loss: 12.9097\n",
      "Epoch [219/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [219/300], Step [300/384], Loss: 60.7972\n",
      "Epoch [219/300], Step [320/384], Loss: 64.1924\n",
      "Epoch [219/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [219/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [219/300], Step [380/384], Loss: 17.3809\n",
      "Epoch [220/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [220/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [220/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [220/300], Step [80/384], Loss: 60.2166\n",
      "Epoch [220/300], Step [100/384], Loss: 111.6759\n",
      "Epoch [220/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [220/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [220/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [220/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [220/300], Step [200/384], Loss: 31.3980\n",
      "Epoch [220/300], Step [220/384], Loss: 1.6800\n",
      "Epoch [220/300], Step [240/384], Loss: 70.3657\n",
      "Epoch [220/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [220/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [220/300], Step [300/384], Loss: 92.5265\n",
      "Epoch [220/300], Step [320/384], Loss: 108.7757\n",
      "Epoch [220/300], Step [340/384], Loss: 96.3097\n",
      "Epoch [220/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [220/300], Step [380/384], Loss: 6.2633\n",
      "Epoch [221/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [221/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [221/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [221/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [221/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [221/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [221/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [221/300], Step [160/384], Loss: 183.8438\n",
      "Epoch [221/300], Step [180/384], Loss: 9.8747\n",
      "Epoch [221/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [221/300], Step [220/384], Loss: 240.9822\n",
      "Epoch [221/300], Step [240/384], Loss: 2.4273\n",
      "Epoch [221/300], Step [260/384], Loss: 61.7557\n",
      "Epoch [221/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [221/300], Step [300/384], Loss: 77.6212\n",
      "Epoch [221/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [221/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [221/300], Step [360/384], Loss: 3.9632\n",
      "Epoch [221/300], Step [380/384], Loss: 276.0280\n",
      "Epoch [222/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [222/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [222/300], Step [60/384], Loss: 3.0268\n",
      "Epoch [222/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [222/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [222/300], Step [120/384], Loss: 14.0506\n",
      "Epoch [222/300], Step [140/384], Loss: 19.6029\n",
      "Epoch [222/300], Step [160/384], Loss: 32.3054\n",
      "Epoch [222/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [222/300], Step [200/384], Loss: 43.9539\n",
      "Epoch [222/300], Step [220/384], Loss: 16.1536\n",
      "Epoch [222/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [222/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [222/300], Step [280/384], Loss: 115.9777\n",
      "Epoch [222/300], Step [300/384], Loss: 8.3621\n",
      "Epoch [222/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [222/300], Step [340/384], Loss: 90.9204\n",
      "Epoch [222/300], Step [360/384], Loss: 87.2382\n",
      "Epoch [222/300], Step [380/384], Loss: 6.0348\n",
      "Epoch [223/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [223/300], Step [40/384], Loss: 214.9855\n",
      "Epoch [223/300], Step [60/384], Loss: 6.6196\n",
      "Epoch [223/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [223/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [223/300], Step [120/384], Loss: 85.0542\n",
      "Epoch [223/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [223/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [223/300], Step [180/384], Loss: 35.4866\n",
      "Epoch [223/300], Step [200/384], Loss: 145.7398\n",
      "Epoch [223/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [223/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [223/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [223/300], Step [280/384], Loss: 162.7617\n",
      "Epoch [223/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [223/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [223/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [223/300], Step [360/384], Loss: 201.6209\n",
      "Epoch [223/300], Step [380/384], Loss: 54.2174\n",
      "Epoch [224/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [224/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [224/300], Step [60/384], Loss: 31.9468\n",
      "Epoch [224/300], Step [80/384], Loss: 90.8912\n",
      "Epoch [224/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [224/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [224/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [224/300], Step [160/384], Loss: 29.9150\n",
      "Epoch [224/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [224/300], Step [200/384], Loss: 212.4821\n",
      "Epoch [224/300], Step [220/384], Loss: 19.1090\n",
      "Epoch [224/300], Step [240/384], Loss: 147.8921\n",
      "Epoch [224/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [224/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [224/300], Step [300/384], Loss: 16.3921\n",
      "Epoch [224/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [224/300], Step [340/384], Loss: 83.5974\n",
      "Epoch [224/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [224/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [225/300], Step [20/384], Loss: 20.2049\n",
      "Epoch [225/300], Step [40/384], Loss: 100.2035\n",
      "Epoch [225/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [225/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [225/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [225/300], Step [120/384], Loss: 41.1533\n",
      "Epoch [225/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [225/300], Step [160/384], Loss: 130.8959\n",
      "Epoch [225/300], Step [180/384], Loss: 105.0794\n",
      "Epoch [225/300], Step [200/384], Loss: 15.2746\n",
      "Epoch [225/300], Step [220/384], Loss: 63.1731\n",
      "Epoch [225/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [225/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [225/300], Step [280/384], Loss: 166.8695\n",
      "Epoch [225/300], Step [300/384], Loss: 32.9297\n",
      "Epoch [225/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [225/300], Step [340/384], Loss: 26.6046\n",
      "Epoch [225/300], Step [360/384], Loss: 10.1845\n",
      "Epoch [225/300], Step [380/384], Loss: 53.4997\n",
      "Epoch [226/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [226/300], Step [40/384], Loss: 41.2101\n",
      "Epoch [226/300], Step [60/384], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [226/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [226/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [226/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [226/300], Step [140/384], Loss: 148.5855\n",
      "Epoch [226/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [226/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [226/300], Step [200/384], Loss: 125.0759\n",
      "Epoch [226/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [226/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [226/300], Step [260/384], Loss: 62.6455\n",
      "Epoch [226/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [226/300], Step [300/384], Loss: 13.0485\n",
      "Epoch [226/300], Step [320/384], Loss: 49.1995\n",
      "Epoch [226/300], Step [340/384], Loss: 121.8418\n",
      "Epoch [226/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [226/300], Step [380/384], Loss: 14.8703\n",
      "Epoch [227/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [60/384], Loss: 39.5175\n",
      "Epoch [227/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [100/384], Loss: 28.4164\n",
      "Epoch [227/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [140/384], Loss: 33.8069\n",
      "Epoch [227/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [220/384], Loss: 0.0017\n",
      "Epoch [227/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [227/300], Step [360/384], Loss: 67.8869\n",
      "Epoch [227/300], Step [380/384], Loss: 27.4170\n",
      "Epoch [228/300], Step [20/384], Loss: 51.2353\n",
      "Epoch [228/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [120/384], Loss: 94.2433\n",
      "Epoch [228/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [280/384], Loss: 24.2799\n",
      "Epoch [228/300], Step [300/384], Loss: 81.0017\n",
      "Epoch [228/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [228/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [40/384], Loss: 140.3411\n",
      "Epoch [229/300], Step [60/384], Loss: 85.3299\n",
      "Epoch [229/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [240/384], Loss: 69.3603\n",
      "Epoch [229/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [229/300], Step [360/384], Loss: 73.6426\n",
      "Epoch [229/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [230/300], Step [20/384], Loss: 117.6490\n",
      "Epoch [230/300], Step [40/384], Loss: 10.3040\n",
      "Epoch [230/300], Step [60/384], Loss: 50.1348\n",
      "Epoch [230/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [230/300], Step [100/384], Loss: 46.7755\n",
      "Epoch [230/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [230/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [230/300], Step [160/384], Loss: 0.0012\n",
      "Epoch [230/300], Step [180/384], Loss: 29.0334\n",
      "Epoch [230/300], Step [200/384], Loss: 34.9143\n",
      "Epoch [230/300], Step [220/384], Loss: 68.6964\n",
      "Epoch [230/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [230/300], Step [260/384], Loss: 42.4580\n",
      "Epoch [230/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [230/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [230/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [230/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [230/300], Step [360/384], Loss: 76.7983\n",
      "Epoch [230/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [231/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [231/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [231/300], Step [60/384], Loss: 170.6795\n",
      "Epoch [231/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [231/300], Step [100/384], Loss: 69.9641\n",
      "Epoch [231/300], Step [120/384], Loss: 19.7393\n",
      "Epoch [231/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [231/300], Step [160/384], Loss: 9.2017\n",
      "Epoch [231/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [231/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [231/300], Step [220/384], Loss: 22.1806\n",
      "Epoch [231/300], Step [240/384], Loss: 198.7946\n",
      "Epoch [231/300], Step [260/384], Loss: 42.5129\n",
      "Epoch [231/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [231/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [231/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [231/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [231/300], Step [360/384], Loss: 32.2571\n",
      "Epoch [231/300], Step [380/384], Loss: 8.4205\n",
      "Epoch [232/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [232/300], Step [40/384], Loss: 153.3779\n",
      "Epoch [232/300], Step [60/384], Loss: 14.6913\n",
      "Epoch [232/300], Step [80/384], Loss: 10.4599\n",
      "Epoch [232/300], Step [100/384], Loss: 72.7118\n",
      "Epoch [232/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [232/300], Step [140/384], Loss: 254.0184\n",
      "Epoch [232/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [232/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [232/300], Step [200/384], Loss: 14.2356\n",
      "Epoch [232/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [232/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [232/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [232/300], Step [280/384], Loss: 51.4536\n",
      "Epoch [232/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [232/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [232/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [232/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [232/300], Step [380/384], Loss: 0.0335\n",
      "Epoch [233/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [60/384], Loss: 83.9390\n",
      "Epoch [233/300], Step [80/384], Loss: 0.0005\n",
      "Epoch [233/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [220/384], Loss: 70.4959\n",
      "Epoch [233/300], Step [240/384], Loss: 43.5826\n",
      "Epoch [233/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [280/384], Loss: 109.2065\n",
      "Epoch [233/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [233/300], Step [360/384], Loss: 169.0310\n",
      "Epoch [233/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [234/300], Step [20/384], Loss: 45.4166\n",
      "Epoch [234/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [234/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [234/300], Step [80/384], Loss: 0.0010\n",
      "Epoch [234/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [234/300], Step [120/384], Loss: 187.6063\n",
      "Epoch [234/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [234/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [234/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [234/300], Step [200/384], Loss: 22.1087\n",
      "Epoch [234/300], Step [220/384], Loss: 0.0148\n",
      "Epoch [234/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [234/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [234/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [234/300], Step [300/384], Loss: 367.2193\n",
      "Epoch [234/300], Step [320/384], Loss: 19.2402\n",
      "Epoch [234/300], Step [340/384], Loss: 96.4993\n",
      "Epoch [234/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [234/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [235/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [235/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [235/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [235/300], Step [80/384], Loss: 70.6851\n",
      "Epoch [235/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [235/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [235/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [235/300], Step [160/384], Loss: 30.3994\n",
      "Epoch [235/300], Step [180/384], Loss: 179.6457\n",
      "Epoch [235/300], Step [200/384], Loss: 81.5590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [235/300], Step [220/384], Loss: 115.4939\n",
      "Epoch [235/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [235/300], Step [260/384], Loss: 16.6610\n",
      "Epoch [235/300], Step [280/384], Loss: 7.0410\n",
      "Epoch [235/300], Step [300/384], Loss: 1.9835\n",
      "Epoch [235/300], Step [320/384], Loss: 20.6799\n",
      "Epoch [235/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [235/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [235/300], Step [380/384], Loss: 77.6814\n",
      "Epoch [236/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [40/384], Loss: 1.9507\n",
      "Epoch [236/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [100/384], Loss: 30.8440\n",
      "Epoch [236/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [260/384], Loss: 14.6932\n",
      "Epoch [236/300], Step [280/384], Loss: 51.5278\n",
      "Epoch [236/300], Step [300/384], Loss: 74.8035\n",
      "Epoch [236/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [236/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [40/384], Loss: 46.5543\n",
      "Epoch [237/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [140/384], Loss: 79.7721\n",
      "Epoch [237/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [260/384], Loss: 10.7929\n",
      "Epoch [237/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [340/384], Loss: 114.9171\n",
      "Epoch [237/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [237/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [238/300], Step [20/384], Loss: 0.4211\n",
      "Epoch [238/300], Step [40/384], Loss: 51.6178\n",
      "Epoch [238/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [238/300], Step [80/384], Loss: 26.5674\n",
      "Epoch [238/300], Step [100/384], Loss: 167.4248\n",
      "Epoch [238/300], Step [120/384], Loss: 63.6161\n",
      "Epoch [238/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [238/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [238/300], Step [180/384], Loss: 79.5417\n",
      "Epoch [238/300], Step [200/384], Loss: 41.8663\n",
      "Epoch [238/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [238/300], Step [240/384], Loss: 1.5159\n",
      "Epoch [238/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [238/300], Step [280/384], Loss: 158.7495\n",
      "Epoch [238/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [238/300], Step [320/384], Loss: 45.6904\n",
      "Epoch [238/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [238/300], Step [360/384], Loss: 193.4023\n",
      "Epoch [238/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [100/384], Loss: 28.7814\n",
      "Epoch [239/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [220/384], Loss: 51.8945\n",
      "Epoch [239/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [260/384], Loss: 3.9275\n",
      "Epoch [239/300], Step [280/384], Loss: 88.3675\n",
      "Epoch [239/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [340/384], Loss: 61.0589\n",
      "Epoch [239/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [239/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [20/384], Loss: 70.1249\n",
      "Epoch [240/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [80/384], Loss: 14.3022\n",
      "Epoch [240/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [320/384], Loss: 19.2332\n",
      "Epoch [240/300], Step [340/384], Loss: 111.5833\n",
      "Epoch [240/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [240/300], Step [380/384], Loss: 116.5021\n",
      "Epoch [241/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [80/384], Loss: 2.0595\n",
      "Epoch [241/300], Step [100/384], Loss: 81.7012\n",
      "Epoch [241/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [180/384], Loss: 12.0537\n",
      "Epoch [241/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [300/384], Loss: 45.0960\n",
      "Epoch [241/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [241/300], Step [340/384], Loss: 11.0417\n",
      "Epoch [241/300], Step [360/384], Loss: 133.6435\n",
      "Epoch [241/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [60/384], Loss: 108.5701\n",
      "Epoch [242/300], Step [80/384], Loss: 89.4757\n",
      "Epoch [242/300], Step [100/384], Loss: 0.0088\n",
      "Epoch [242/300], Step [120/384], Loss: 86.4156\n",
      "Epoch [242/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [180/384], Loss: 126.5634\n",
      "Epoch [242/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [242/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [20/384], Loss: 88.7194\n",
      "Epoch [243/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [60/384], Loss: 54.3345\n",
      "Epoch [243/300], Step [80/384], Loss: 90.4842\n",
      "Epoch [243/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [120/384], Loss: 218.6779\n",
      "Epoch [243/300], Step [140/384], Loss: 88.1604\n",
      "Epoch [243/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [243/300], Step [360/384], Loss: 55.0380\n",
      "Epoch [243/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [80/384], Loss: 32.6870\n",
      "Epoch [244/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [140/384], Loss: 8.9019\n",
      "Epoch [244/300], Step [160/384], Loss: 22.0198\n",
      "Epoch [244/300], Step [180/384], Loss: 139.9708\n",
      "Epoch [244/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [260/384], Loss: 36.9180\n",
      "Epoch [244/300], Step [280/384], Loss: 20.0575\n",
      "Epoch [244/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [340/384], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [244/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [244/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [245/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [245/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [245/300], Step [60/384], Loss: 46.8779\n",
      "Epoch [245/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [245/300], Step [100/384], Loss: 63.9887\n",
      "Epoch [245/300], Step [120/384], Loss: 68.1857\n",
      "Epoch [245/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [245/300], Step [160/384], Loss: 64.1160\n",
      "Epoch [245/300], Step [180/384], Loss: 84.0503\n",
      "Epoch [245/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [245/300], Step [220/384], Loss: 26.0340\n",
      "Epoch [245/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [245/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [245/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [245/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [245/300], Step [320/384], Loss: 50.2728\n",
      "Epoch [245/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [245/300], Step [360/384], Loss: 39.9249\n",
      "Epoch [245/300], Step [380/384], Loss: 21.9484\n",
      "Epoch [246/300], Step [20/384], Loss: 54.2935\n",
      "Epoch [246/300], Step [40/384], Loss: 41.9463\n",
      "Epoch [246/300], Step [60/384], Loss: 40.0187\n",
      "Epoch [246/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [246/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [246/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [246/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [246/300], Step [160/384], Loss: 162.1552\n",
      "Epoch [246/300], Step [180/384], Loss: 15.5498\n",
      "Epoch [246/300], Step [200/384], Loss: 40.4957\n",
      "Epoch [246/300], Step [220/384], Loss: 87.1517\n",
      "Epoch [246/300], Step [240/384], Loss: 2.3812\n",
      "Epoch [246/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [246/300], Step [280/384], Loss: 4.6368\n",
      "Epoch [246/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [246/300], Step [320/384], Loss: 50.3259\n",
      "Epoch [246/300], Step [340/384], Loss: 73.4574\n",
      "Epoch [246/300], Step [360/384], Loss: 153.0007\n",
      "Epoch [246/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [247/300], Step [20/384], Loss: 255.3375\n",
      "Epoch [247/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [247/300], Step [60/384], Loss: 127.3275\n",
      "Epoch [247/300], Step [80/384], Loss: 7.9448\n",
      "Epoch [247/300], Step [100/384], Loss: 131.8011\n",
      "Epoch [247/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [247/300], Step [140/384], Loss: 73.9606\n",
      "Epoch [247/300], Step [160/384], Loss: 78.9276\n",
      "Epoch [247/300], Step [180/384], Loss: 431.1266\n",
      "Epoch [247/300], Step [200/384], Loss: 27.0231\n",
      "Epoch [247/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [247/300], Step [240/384], Loss: 77.3296\n",
      "Epoch [247/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [247/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [247/300], Step [300/384], Loss: 42.2340\n",
      "Epoch [247/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [247/300], Step [340/384], Loss: 53.6270\n",
      "Epoch [247/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [247/300], Step [380/384], Loss: 2.2117\n",
      "Epoch [248/300], Step [20/384], Loss: 65.0558\n",
      "Epoch [248/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [248/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [248/300], Step [80/384], Loss: 59.1805\n",
      "Epoch [248/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [248/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [248/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [248/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [248/300], Step [180/384], Loss: 22.9651\n",
      "Epoch [248/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [248/300], Step [220/384], Loss: 21.5419\n",
      "Epoch [248/300], Step [240/384], Loss: 47.8582\n",
      "Epoch [248/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [248/300], Step [280/384], Loss: 358.5600\n",
      "Epoch [248/300], Step [300/384], Loss: 41.0973\n",
      "Epoch [248/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [248/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [248/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [248/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [40/384], Loss: 73.9561\n",
      "Epoch [249/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [80/384], Loss: 5.0574\n",
      "Epoch [249/300], Step [100/384], Loss: 12.2852\n",
      "Epoch [249/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [260/384], Loss: 39.5810\n",
      "Epoch [249/300], Step [280/384], Loss: 14.2507\n",
      "Epoch [249/300], Step [300/384], Loss: 0.0178\n",
      "Epoch [249/300], Step [320/384], Loss: 89.5519\n",
      "Epoch [249/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [249/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [250/300], Step [20/384], Loss: 0.0033\n",
      "Epoch [250/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [250/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [250/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [250/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [250/300], Step [120/384], Loss: 24.1650\n",
      "Epoch [250/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [250/300], Step [160/384], Loss: 97.7811\n",
      "Epoch [250/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [250/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [250/300], Step [220/384], Loss: 62.9160\n",
      "Epoch [250/300], Step [240/384], Loss: 13.6038\n",
      "Epoch [250/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [250/300], Step [280/384], Loss: 38.1483\n",
      "Epoch [250/300], Step [300/384], Loss: 68.4570\n",
      "Epoch [250/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [250/300], Step [340/384], Loss: 27.8180\n",
      "Epoch [250/300], Step [360/384], Loss: 138.6422\n",
      "Epoch [250/300], Step [380/384], Loss: 84.1433\n",
      "Epoch [251/300], Step [20/384], Loss: 33.7140\n",
      "Epoch [251/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [60/384], Loss: 106.0625\n",
      "Epoch [251/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [140/384], Loss: 8.3289\n",
      "Epoch [251/300], Step [160/384], Loss: 31.3547\n",
      "Epoch [251/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [260/384], Loss: 80.5735\n",
      "Epoch [251/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [300/384], Loss: 0.6828\n",
      "Epoch [251/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [251/300], Step [380/384], Loss: 61.1711\n",
      "Epoch [252/300], Step [20/384], Loss: 55.8574\n",
      "Epoch [252/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [252/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [252/300], Step [80/384], Loss: 32.5948\n",
      "Epoch [252/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [252/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [252/300], Step [140/384], Loss: 0.0072\n",
      "Epoch [252/300], Step [160/384], Loss: 26.7922\n",
      "Epoch [252/300], Step [180/384], Loss: 82.1434\n",
      "Epoch [252/300], Step [200/384], Loss: 59.9339\n",
      "Epoch [252/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [252/300], Step [240/384], Loss: 18.1696\n",
      "Epoch [252/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [252/300], Step [280/384], Loss: 82.9664\n",
      "Epoch [252/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [252/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [252/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [252/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [252/300], Step [380/384], Loss: 4.8595\n",
      "Epoch [253/300], Step [20/384], Loss: 94.0465\n",
      "Epoch [253/300], Step [40/384], Loss: 61.2078\n",
      "Epoch [253/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [80/384], Loss: 34.6915\n",
      "Epoch [253/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [200/384], Loss: 44.1229\n",
      "Epoch [253/300], Step [220/384], Loss: 108.9709\n",
      "Epoch [253/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [280/384], Loss: 28.2581\n",
      "Epoch [253/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [253/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [100/384], Loss: 37.6319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [254/300], Step [120/384], Loss: 33.0976\n",
      "Epoch [254/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [180/384], Loss: 88.9466\n",
      "Epoch [254/300], Step [200/384], Loss: 133.5982\n",
      "Epoch [254/300], Step [220/384], Loss: 55.0255\n",
      "Epoch [254/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [300/384], Loss: 258.6114\n",
      "Epoch [254/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [254/300], Step [360/384], Loss: 27.2569\n",
      "Epoch [254/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [255/300], Step [20/384], Loss: 72.4472\n",
      "Epoch [255/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [255/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [255/300], Step [80/384], Loss: 34.7027\n",
      "Epoch [255/300], Step [100/384], Loss: 82.3471\n",
      "Epoch [255/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [255/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [255/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [255/300], Step [180/384], Loss: 114.2612\n",
      "Epoch [255/300], Step [200/384], Loss: 42.5316\n",
      "Epoch [255/300], Step [220/384], Loss: 11.8053\n",
      "Epoch [255/300], Step [240/384], Loss: 205.6674\n",
      "Epoch [255/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [255/300], Step [280/384], Loss: 147.8683\n",
      "Epoch [255/300], Step [300/384], Loss: 5.9961\n",
      "Epoch [255/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [255/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [255/300], Step [360/384], Loss: 39.1509\n",
      "Epoch [255/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [256/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [256/300], Step [40/384], Loss: 106.8118\n",
      "Epoch [256/300], Step [60/384], Loss: 111.8559\n",
      "Epoch [256/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [256/300], Step [100/384], Loss: 127.4570\n",
      "Epoch [256/300], Step [120/384], Loss: 5.2044\n",
      "Epoch [256/300], Step [140/384], Loss: 0.5685\n",
      "Epoch [256/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [256/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [256/300], Step [200/384], Loss: 113.4824\n",
      "Epoch [256/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [256/300], Step [240/384], Loss: 40.6351\n",
      "Epoch [256/300], Step [260/384], Loss: 46.3215\n",
      "Epoch [256/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [256/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [256/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [256/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [256/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [256/300], Step [380/384], Loss: 122.5369\n",
      "Epoch [257/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [257/300], Step [40/384], Loss: 13.1507\n",
      "Epoch [257/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [257/300], Step [80/384], Loss: 268.7841\n",
      "Epoch [257/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [257/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [257/300], Step [140/384], Loss: 16.4703\n",
      "Epoch [257/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [257/300], Step [180/384], Loss: 131.7760\n",
      "Epoch [257/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [257/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [257/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [257/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [257/300], Step [280/384], Loss: 170.1481\n",
      "Epoch [257/300], Step [300/384], Loss: 22.1340\n",
      "Epoch [257/300], Step [320/384], Loss: 60.0998\n",
      "Epoch [257/300], Step [340/384], Loss: 76.2727\n",
      "Epoch [257/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [257/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [100/384], Loss: 0.0048\n",
      "Epoch [258/300], Step [120/384], Loss: 42.7149\n",
      "Epoch [258/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [220/384], Loss: 37.1093\n",
      "Epoch [258/300], Step [240/384], Loss: 54.3055\n",
      "Epoch [258/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [258/300], Step [360/384], Loss: 60.0388\n",
      "Epoch [258/300], Step [380/384], Loss: 54.3547\n",
      "Epoch [259/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [80/384], Loss: 26.0553\n",
      "Epoch [259/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [140/384], Loss: 67.5285\n",
      "Epoch [259/300], Step [160/384], Loss: 29.0594\n",
      "Epoch [259/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [240/384], Loss: 106.9022\n",
      "Epoch [259/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [259/300], Step [380/384], Loss: 43.5251\n",
      "Epoch [260/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [260/300], Step [40/384], Loss: 76.4121\n",
      "Epoch [260/300], Step [60/384], Loss: 27.7657\n",
      "Epoch [260/300], Step [80/384], Loss: 62.8455\n",
      "Epoch [260/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [260/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [260/300], Step [140/384], Loss: 27.0362\n",
      "Epoch [260/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [260/300], Step [180/384], Loss: 99.1746\n",
      "Epoch [260/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [260/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [260/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [260/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [260/300], Step [280/384], Loss: 171.1010\n",
      "Epoch [260/300], Step [300/384], Loss: 218.4483\n",
      "Epoch [260/300], Step [320/384], Loss: 41.8976\n",
      "Epoch [260/300], Step [340/384], Loss: 47.0188\n",
      "Epoch [260/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [260/300], Step [380/384], Loss: 5.0415\n",
      "Epoch [261/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [100/384], Loss: 15.4007\n",
      "Epoch [261/300], Step [120/384], Loss: 83.9212\n",
      "Epoch [261/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [200/384], Loss: 128.4731\n",
      "Epoch [261/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [260/384], Loss: 6.5086\n",
      "Epoch [261/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [261/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [20/384], Loss: 2.9776\n",
      "Epoch [262/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [60/384], Loss: 67.1095\n",
      "Epoch [262/300], Step [80/384], Loss: 31.3910\n",
      "Epoch [262/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [120/384], Loss: 32.5674\n",
      "Epoch [262/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [160/384], Loss: 38.7082\n",
      "Epoch [262/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [240/384], Loss: 88.4768\n",
      "Epoch [262/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [262/300], Step [360/384], Loss: 74.3247\n",
      "Epoch [262/300], Step [380/384], Loss: 68.4992\n",
      "Epoch [263/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [60/384], Loss: 11.0299\n",
      "Epoch [263/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [100/384], Loss: 89.2161\n",
      "Epoch [263/300], Step [120/384], Loss: 0.0023\n",
      "Epoch [263/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [240/384], Loss: 101.7329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [263/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [300/384], Loss: 19.3329\n",
      "Epoch [263/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [263/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [20/384], Loss: 20.3769\n",
      "Epoch [264/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [120/384], Loss: 5.2632\n",
      "Epoch [264/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [180/384], Loss: 41.9492\n",
      "Epoch [264/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [220/384], Loss: 7.1369\n",
      "Epoch [264/300], Step [240/384], Loss: 11.7757\n",
      "Epoch [264/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [280/384], Loss: 38.9424\n",
      "Epoch [264/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [264/300], Step [380/384], Loss: 32.3631\n",
      "Epoch [265/300], Step [20/384], Loss: 42.2380\n",
      "Epoch [265/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [160/384], Loss: 15.0856\n",
      "Epoch [265/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [220/384], Loss: 207.2833\n",
      "Epoch [265/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [265/300], Step [380/384], Loss: 171.1775\n",
      "Epoch [266/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [266/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [266/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [266/300], Step [80/384], Loss: 94.8518\n",
      "Epoch [266/300], Step [100/384], Loss: 59.0183\n",
      "Epoch [266/300], Step [120/384], Loss: 77.7776\n",
      "Epoch [266/300], Step [140/384], Loss: 47.9432\n",
      "Epoch [266/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [266/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [266/300], Step [200/384], Loss: 89.1387\n",
      "Epoch [266/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [266/300], Step [240/384], Loss: 38.1400\n",
      "Epoch [266/300], Step [260/384], Loss: 43.9076\n",
      "Epoch [266/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [266/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [266/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [266/300], Step [340/384], Loss: 104.2985\n",
      "Epoch [266/300], Step [360/384], Loss: 9.3322\n",
      "Epoch [266/300], Step [380/384], Loss: 20.1442\n",
      "Epoch [267/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [267/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [267/300], Step [60/384], Loss: 5.2162\n",
      "Epoch [267/300], Step [80/384], Loss: 178.6407\n",
      "Epoch [267/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [267/300], Step [120/384], Loss: 33.5749\n",
      "Epoch [267/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [267/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [267/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [267/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [267/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [267/300], Step [240/384], Loss: 59.4263\n",
      "Epoch [267/300], Step [260/384], Loss: 58.3329\n",
      "Epoch [267/300], Step [280/384], Loss: 131.8516\n",
      "Epoch [267/300], Step [300/384], Loss: 89.9921\n",
      "Epoch [267/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [267/300], Step [340/384], Loss: 161.7238\n",
      "Epoch [267/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [267/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [268/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [268/300], Step [40/384], Loss: 27.7188\n",
      "Epoch [268/300], Step [60/384], Loss: 19.2679\n",
      "Epoch [268/300], Step [80/384], Loss: 9.2011\n",
      "Epoch [268/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [268/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [268/300], Step [140/384], Loss: 141.9817\n",
      "Epoch [268/300], Step [160/384], Loss: 94.1091\n",
      "Epoch [268/300], Step [180/384], Loss: 24.4521\n",
      "Epoch [268/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [268/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [268/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [268/300], Step [260/384], Loss: 47.9012\n",
      "Epoch [268/300], Step [280/384], Loss: 31.1282\n",
      "Epoch [268/300], Step [300/384], Loss: 46.7554\n",
      "Epoch [268/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [268/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [268/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [268/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [60/384], Loss: 23.1658\n",
      "Epoch [269/300], Step [80/384], Loss: 92.4819\n",
      "Epoch [269/300], Step [100/384], Loss: 32.3275\n",
      "Epoch [269/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [140/384], Loss: 33.1224\n",
      "Epoch [269/300], Step [160/384], Loss: 42.5858\n",
      "Epoch [269/300], Step [180/384], Loss: 23.1141\n",
      "Epoch [269/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [240/384], Loss: 114.0403\n",
      "Epoch [269/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [269/300], Step [380/384], Loss: 6.2340\n",
      "Epoch [270/300], Step [20/384], Loss: 63.1711\n",
      "Epoch [270/300], Step [40/384], Loss: 59.5585\n",
      "Epoch [270/300], Step [60/384], Loss: 0.2946\n",
      "Epoch [270/300], Step [80/384], Loss: 27.3939\n",
      "Epoch [270/300], Step [100/384], Loss: 83.9197\n",
      "Epoch [270/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [270/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [270/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [270/300], Step [180/384], Loss: 15.9616\n",
      "Epoch [270/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [270/300], Step [220/384], Loss: 0.0033\n",
      "Epoch [270/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [270/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [270/300], Step [280/384], Loss: 35.8239\n",
      "Epoch [270/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [270/300], Step [320/384], Loss: 24.0146\n",
      "Epoch [270/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [270/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [270/300], Step [380/384], Loss: 240.4818\n",
      "Epoch [271/300], Step [20/384], Loss: 36.9740\n",
      "Epoch [271/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [271/300], Step [60/384], Loss: 37.9185\n",
      "Epoch [271/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [271/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [271/300], Step [120/384], Loss: 82.5957\n",
      "Epoch [271/300], Step [140/384], Loss: 208.6537\n",
      "Epoch [271/300], Step [160/384], Loss: 22.1143\n",
      "Epoch [271/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [271/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [271/300], Step [220/384], Loss: 60.2117\n",
      "Epoch [271/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [271/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [271/300], Step [280/384], Loss: 37.6662\n",
      "Epoch [271/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [271/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [271/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [271/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [271/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [100/384], Loss: 52.3472\n",
      "Epoch [272/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [200/384], Loss: 47.6699\n",
      "Epoch [272/300], Step [220/384], Loss: 12.0395\n",
      "Epoch [272/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [340/384], Loss: 43.7836\n",
      "Epoch [272/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [272/300], Step [380/384], Loss: 50.3142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [273/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [273/300], Step [40/384], Loss: 0.0129\n",
      "Epoch [273/300], Step [60/384], Loss: 80.5161\n",
      "Epoch [273/300], Step [80/384], Loss: 33.9674\n",
      "Epoch [273/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [273/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [273/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [273/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [273/300], Step [180/384], Loss: 74.3732\n",
      "Epoch [273/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [273/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [273/300], Step [240/384], Loss: 16.8085\n",
      "Epoch [273/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [273/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [273/300], Step [300/384], Loss: 49.3534\n",
      "Epoch [273/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [273/300], Step [340/384], Loss: 63.7593\n",
      "Epoch [273/300], Step [360/384], Loss: 13.5928\n",
      "Epoch [273/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [274/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [274/300], Step [40/384], Loss: 45.6261\n",
      "Epoch [274/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [274/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [274/300], Step [100/384], Loss: 139.1039\n",
      "Epoch [274/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [274/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [274/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [274/300], Step [180/384], Loss: 34.4031\n",
      "Epoch [274/300], Step [200/384], Loss: 231.8695\n",
      "Epoch [274/300], Step [220/384], Loss: 0.0003\n",
      "Epoch [274/300], Step [240/384], Loss: 37.7814\n",
      "Epoch [274/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [274/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [274/300], Step [300/384], Loss: 0.0300\n",
      "Epoch [274/300], Step [320/384], Loss: 18.7115\n",
      "Epoch [274/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [274/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [274/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [275/300], Step [20/384], Loss: 23.3202\n",
      "Epoch [275/300], Step [40/384], Loss: 55.2640\n",
      "Epoch [275/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [275/300], Step [80/384], Loss: 82.7666\n",
      "Epoch [275/300], Step [100/384], Loss: 7.0767\n",
      "Epoch [275/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [275/300], Step [140/384], Loss: 104.7201\n",
      "Epoch [275/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [275/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [275/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [275/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [275/300], Step [240/384], Loss: 28.1269\n",
      "Epoch [275/300], Step [260/384], Loss: 8.3471\n",
      "Epoch [275/300], Step [280/384], Loss: 32.3690\n",
      "Epoch [275/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [275/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [275/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [275/300], Step [360/384], Loss: 118.1242\n",
      "Epoch [275/300], Step [380/384], Loss: 55.0865\n",
      "Epoch [276/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [276/300], Step [40/384], Loss: 0.3253\n",
      "Epoch [276/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [276/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [276/300], Step [100/384], Loss: 180.1876\n",
      "Epoch [276/300], Step [120/384], Loss: 62.9082\n",
      "Epoch [276/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [276/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [276/300], Step [180/384], Loss: 11.0083\n",
      "Epoch [276/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [276/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [276/300], Step [240/384], Loss: 40.2837\n",
      "Epoch [276/300], Step [260/384], Loss: 107.4022\n",
      "Epoch [276/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [276/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [276/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [276/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [276/300], Step [360/384], Loss: 34.4405\n",
      "Epoch [276/300], Step [380/384], Loss: 26.8931\n",
      "Epoch [277/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [277/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [277/300], Step [60/384], Loss: 4.8342\n",
      "Epoch [277/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [277/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [277/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [277/300], Step [140/384], Loss: 51.5035\n",
      "Epoch [277/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [277/300], Step [180/384], Loss: 48.7413\n",
      "Epoch [277/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [277/300], Step [220/384], Loss: 27.2424\n",
      "Epoch [277/300], Step [240/384], Loss: 6.9745\n",
      "Epoch [277/300], Step [260/384], Loss: 59.6449\n",
      "Epoch [277/300], Step [280/384], Loss: 102.8775\n",
      "Epoch [277/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [277/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [277/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [277/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [277/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [278/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [278/300], Step [40/384], Loss: 23.0410\n",
      "Epoch [278/300], Step [60/384], Loss: 58.4922\n",
      "Epoch [278/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [278/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [278/300], Step [120/384], Loss: 45.8273\n",
      "Epoch [278/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [278/300], Step [160/384], Loss: 30.7183\n",
      "Epoch [278/300], Step [180/384], Loss: 26.5316\n",
      "Epoch [278/300], Step [200/384], Loss: 153.0775\n",
      "Epoch [278/300], Step [220/384], Loss: 183.7869\n",
      "Epoch [278/300], Step [240/384], Loss: 76.0514\n",
      "Epoch [278/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [278/300], Step [280/384], Loss: 41.5558\n",
      "Epoch [278/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [278/300], Step [320/384], Loss: 58.3387\n",
      "Epoch [278/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [278/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [278/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [100/384], Loss: 113.0014\n",
      "Epoch [279/300], Step [120/384], Loss: 185.5464\n",
      "Epoch [279/300], Step [140/384], Loss: 55.2765\n",
      "Epoch [279/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [240/384], Loss: 127.1859\n",
      "Epoch [279/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [280/384], Loss: 299.3948\n",
      "Epoch [279/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [279/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [100/384], Loss: 10.2822\n",
      "Epoch [280/300], Step [120/384], Loss: 136.2934\n",
      "Epoch [280/300], Step [140/384], Loss: 0.8099\n",
      "Epoch [280/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [260/384], Loss: 73.0617\n",
      "Epoch [280/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [280/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [40/384], Loss: 22.2261\n",
      "Epoch [281/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [160/384], Loss: 0.0006\n",
      "Epoch [281/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [260/384], Loss: 35.4527\n",
      "Epoch [281/300], Step [280/384], Loss: 62.6442\n",
      "Epoch [281/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [281/300], Step [340/384], Loss: 155.9868\n",
      "Epoch [281/300], Step [360/384], Loss: 9.5032\n",
      "Epoch [281/300], Step [380/384], Loss: 35.6678\n",
      "Epoch [282/300], Step [20/384], Loss: 358.0121\n",
      "Epoch [282/300], Step [40/384], Loss: 44.8021\n",
      "Epoch [282/300], Step [60/384], Loss: 5.7394\n",
      "Epoch [282/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [282/300], Step [100/384], Loss: 90.0691\n",
      "Epoch [282/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [282/300], Step [140/384], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [282/300], Step [160/384], Loss: 93.8688\n",
      "Epoch [282/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [282/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [282/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [282/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [282/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [282/300], Step [280/384], Loss: 47.1006\n",
      "Epoch [282/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [282/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [282/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [282/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [282/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [283/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [283/300], Step [40/384], Loss: 134.5718\n",
      "Epoch [283/300], Step [60/384], Loss: 31.5957\n",
      "Epoch [283/300], Step [80/384], Loss: 47.8037\n",
      "Epoch [283/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [283/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [283/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [283/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [283/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [283/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [283/300], Step [220/384], Loss: 31.5048\n",
      "Epoch [283/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [283/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [283/300], Step [280/384], Loss: 105.2891\n",
      "Epoch [283/300], Step [300/384], Loss: 32.9624\n",
      "Epoch [283/300], Step [320/384], Loss: 65.8782\n",
      "Epoch [283/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [283/300], Step [360/384], Loss: 1.3048\n",
      "Epoch [283/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [20/384], Loss: 13.4299\n",
      "Epoch [284/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [80/384], Loss: 109.2609\n",
      "Epoch [284/300], Step [100/384], Loss: 6.9730\n",
      "Epoch [284/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [160/384], Loss: 166.2358\n",
      "Epoch [284/300], Step [180/384], Loss: 24.2707\n",
      "Epoch [284/300], Step [200/384], Loss: 29.6176\n",
      "Epoch [284/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [284/300], Step [380/384], Loss: 127.2257\n",
      "Epoch [285/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [285/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [285/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [285/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [285/300], Step [100/384], Loss: 193.8471\n",
      "Epoch [285/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [285/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [285/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [285/300], Step [180/384], Loss: 55.6398\n",
      "Epoch [285/300], Step [200/384], Loss: 13.8493\n",
      "Epoch [285/300], Step [220/384], Loss: 94.3523\n",
      "Epoch [285/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [285/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [285/300], Step [280/384], Loss: 67.5981\n",
      "Epoch [285/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [285/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [285/300], Step [340/384], Loss: 66.9236\n",
      "Epoch [285/300], Step [360/384], Loss: 48.3750\n",
      "Epoch [285/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [120/384], Loss: 85.9908\n",
      "Epoch [286/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [200/384], Loss: 41.0848\n",
      "Epoch [286/300], Step [220/384], Loss: 27.4672\n",
      "Epoch [286/300], Step [240/384], Loss: 37.9919\n",
      "Epoch [286/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [280/384], Loss: 52.7858\n",
      "Epoch [286/300], Step [300/384], Loss: 13.3011\n",
      "Epoch [286/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [286/300], Step [360/384], Loss: 54.6545\n",
      "Epoch [286/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [20/384], Loss: 136.1518\n",
      "Epoch [287/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [80/384], Loss: 188.8861\n",
      "Epoch [287/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [240/384], Loss: 23.7410\n",
      "Epoch [287/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [300/384], Loss: 6.3519\n",
      "Epoch [287/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [287/300], Step [340/384], Loss: 148.4376\n",
      "Epoch [287/300], Step [360/384], Loss: 0.1414\n",
      "Epoch [287/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [220/384], Loss: 85.5675\n",
      "Epoch [288/300], Step [240/384], Loss: 1.2047\n",
      "Epoch [288/300], Step [260/384], Loss: 73.3889\n",
      "Epoch [288/300], Step [280/384], Loss: 91.1335\n",
      "Epoch [288/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [320/384], Loss: 45.8490\n",
      "Epoch [288/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [288/300], Step [380/384], Loss: 34.3999\n",
      "Epoch [289/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [160/384], Loss: 76.1031\n",
      "Epoch [289/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [200/384], Loss: 36.2174\n",
      "Epoch [289/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [289/300], Step [360/384], Loss: 101.0570\n",
      "Epoch [289/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [290/300], Step [20/384], Loss: 5.0094\n",
      "Epoch [290/300], Step [40/384], Loss: 37.1544\n",
      "Epoch [290/300], Step [60/384], Loss: 43.9057\n",
      "Epoch [290/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [290/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [290/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [290/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [290/300], Step [160/384], Loss: 103.5307\n",
      "Epoch [290/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [290/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [290/300], Step [220/384], Loss: 3.8004\n",
      "Epoch [290/300], Step [240/384], Loss: 83.4071\n",
      "Epoch [290/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [290/300], Step [280/384], Loss: 120.5472\n",
      "Epoch [290/300], Step [300/384], Loss: 95.9044\n",
      "Epoch [290/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [290/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [290/300], Step [360/384], Loss: 12.7770\n",
      "Epoch [290/300], Step [380/384], Loss: 9.5616\n",
      "Epoch [291/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [291/300], Step [40/384], Loss: 124.9764\n",
      "Epoch [291/300], Step [60/384], Loss: 3.1092\n",
      "Epoch [291/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [291/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [291/300], Step [120/384], Loss: 85.7068\n",
      "Epoch [291/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [291/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [291/300], Step [180/384], Loss: 19.0413\n",
      "Epoch [291/300], Step [200/384], Loss: 188.1875\n",
      "Epoch [291/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [291/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [291/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [291/300], Step [280/384], Loss: 64.4530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [291/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [291/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [291/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [291/300], Step [360/384], Loss: 74.0528\n",
      "Epoch [291/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [40/384], Loss: 74.9503\n",
      "Epoch [292/300], Step [60/384], Loss: 126.1653\n",
      "Epoch [292/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [100/384], Loss: 85.9443\n",
      "Epoch [292/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [160/384], Loss: 58.9848\n",
      "Epoch [292/300], Step [180/384], Loss: 127.0925\n",
      "Epoch [292/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [260/384], Loss: 35.0548\n",
      "Epoch [292/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [292/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [60/384], Loss: 103.2340\n",
      "Epoch [293/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [340/384], Loss: 21.7182\n",
      "Epoch [293/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [293/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [60/384], Loss: 110.1307\n",
      "Epoch [294/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [100/384], Loss: 31.2099\n",
      "Epoch [294/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [160/384], Loss: 106.4115\n",
      "Epoch [294/300], Step [180/384], Loss: 48.2011\n",
      "Epoch [294/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [300/384], Loss: 35.1571\n",
      "Epoch [294/300], Step [320/384], Loss: 3.6799\n",
      "Epoch [294/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [294/300], Step [380/384], Loss: 123.8508\n",
      "Epoch [295/300], Step [20/384], Loss: 82.8270\n",
      "Epoch [295/300], Step [40/384], Loss: 5.2310\n",
      "Epoch [295/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [100/384], Loss: 16.7740\n",
      "Epoch [295/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [200/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [240/384], Loss: 58.6453\n",
      "Epoch [295/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [340/384], Loss: 95.1783\n",
      "Epoch [295/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [295/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [80/384], Loss: 54.3144\n",
      "Epoch [296/300], Step [100/384], Loss: 18.5201\n",
      "Epoch [296/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [200/384], Loss: 2.4667\n",
      "Epoch [296/300], Step [220/384], Loss: 19.3458\n",
      "Epoch [296/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [260/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [300/384], Loss: 8.2117\n",
      "Epoch [296/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [340/384], Loss: 81.4016\n",
      "Epoch [296/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [296/300], Step [380/384], Loss: 2.3676\n",
      "Epoch [297/300], Step [20/384], Loss: 87.3532\n",
      "Epoch [297/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [80/384], Loss: 14.0854\n",
      "Epoch [297/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [120/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [160/384], Loss: 75.1780\n",
      "Epoch [297/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [200/384], Loss: 224.2022\n",
      "Epoch [297/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [260/384], Loss: 8.9636\n",
      "Epoch [297/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [340/384], Loss: 16.0414\n",
      "Epoch [297/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [297/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [298/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [298/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [298/300], Step [60/384], Loss: 13.4189\n",
      "Epoch [298/300], Step [80/384], Loss: 31.1840\n",
      "Epoch [298/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [298/300], Step [120/384], Loss: 17.5542\n",
      "Epoch [298/300], Step [140/384], Loss: 15.2004\n",
      "Epoch [298/300], Step [160/384], Loss: 0.0278\n",
      "Epoch [298/300], Step [180/384], Loss: 28.5649\n",
      "Epoch [298/300], Step [200/384], Loss: 190.2986\n",
      "Epoch [298/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [298/300], Step [240/384], Loss: 67.0334\n",
      "Epoch [298/300], Step [260/384], Loss: 112.2268\n",
      "Epoch [298/300], Step [280/384], Loss: 11.7679\n",
      "Epoch [298/300], Step [300/384], Loss: 30.9709\n",
      "Epoch [298/300], Step [320/384], Loss: 2.2993\n",
      "Epoch [298/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [298/300], Step [360/384], Loss: 49.0378\n",
      "Epoch [298/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [299/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [299/300], Step [40/384], Loss: 43.8250\n",
      "Epoch [299/300], Step [60/384], Loss: 222.0854\n",
      "Epoch [299/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [299/300], Step [100/384], Loss: 41.5811\n",
      "Epoch [299/300], Step [120/384], Loss: 17.1269\n",
      "Epoch [299/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [299/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [299/300], Step [180/384], Loss: 1.2509\n",
      "Epoch [299/300], Step [200/384], Loss: 87.8938\n",
      "Epoch [299/300], Step [220/384], Loss: 28.4599\n",
      "Epoch [299/300], Step [240/384], Loss: 56.7060\n",
      "Epoch [299/300], Step [260/384], Loss: 23.0064\n",
      "Epoch [299/300], Step [280/384], Loss: 0.0000\n",
      "Epoch [299/300], Step [300/384], Loss: 21.2191\n",
      "Epoch [299/300], Step [320/384], Loss: 238.6537\n",
      "Epoch [299/300], Step [340/384], Loss: 32.8078\n",
      "Epoch [299/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [299/300], Step [380/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [20/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [40/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [60/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [80/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [100/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [120/384], Loss: 5.3629\n",
      "Epoch [300/300], Step [140/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [160/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [180/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [200/384], Loss: 0.2509\n",
      "Epoch [300/300], Step [220/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [240/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [260/384], Loss: 60.3029\n",
      "Epoch [300/300], Step [280/384], Loss: 32.1194\n",
      "Epoch [300/300], Step [300/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [320/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [340/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [360/384], Loss: 0.0000\n",
      "Epoch [300/300], Step [380/384], Loss: 49.7916\n"
     ]
    }
   ],
   "source": [
    "#define training params\n",
    "num_epochs = 300\n",
    "learning_rate = 0.008\n",
    "\n",
    "#define loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#collect loss for each batch\n",
    "loss_list = []\n",
    "\n",
    "#information for plotting\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (window, labels) in enumerate(train_loader):\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(window.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #plot information during training\n",
    "        if (i+1) % 20 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np_loss_list = []\n",
    "loss_total_epoch = 0\n",
    "\n",
    "#iterate through collected loss during training\n",
    "for iterator, loss in enumerate(loss_list):\n",
    "    \n",
    "    #remove the computational graph of the torch tensor and transform torch to numpy tensor\n",
    "    loss_total_epoch += loss.detach().numpy()\n",
    "    \n",
    "    #sum up the batch losses from each epoch\n",
    "    if iterator%len(train_loader) == 0:\n",
    "        np_loss_list.append(loss_total_epoch)\n",
    "        loss_total_epoch =0 \n",
    "\n",
    "#plot loss\n",
    "plt.plot(np_loss_list)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: tensor([2, 1, 2, 3])\n",
      "tensor([[ -641.0454,  3140.0796,  4272.8643, -6768.0332],\n",
      "        [  -27.6719,   416.3887,  -600.3907,   215.2950],\n",
      "        [ -242.3289,   594.4327,   572.5288,  -921.2480],\n",
      "        [  225.5517,  -771.4608,  -432.4328,   979.2803]])\n",
      "argmax out: 2\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 1\n",
      "argmax out: 3\n",
      "3correct\n",
      "labels: tensor([2, 0, 3, 0])\n",
      "tensor([[  -56.7267,   260.9501,  1742.9858, -1943.3401],\n",
      "        [  808.2579,  -498.2379,  -937.7645,   629.2237],\n",
      "        [   31.1063,   199.0655, -1251.3966,  1024.2822],\n",
      "        [ 2214.7263,  -775.0059, -1752.7979,   313.7882]])\n",
      "argmax out: 2\n",
      "0correct\n",
      "argmax out: 0\n",
      "1correct\n",
      "argmax out: 3\n",
      "2correct\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([3, 1, 3, 3])\n",
      "tensor([[ -79.8841,  156.1156, -357.9826,  286.9725],\n",
      "        [ -22.8737,  610.7881,  261.2741, -843.6342],\n",
      "        [ -56.3988,   42.8587, -284.0754,  302.9667],\n",
      "        [ -89.2518,  394.7990, -692.2767,  388.6483]])\n",
      "argmax out: 3\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 3\n",
      "2correct\n",
      "argmax out: 1\n",
      "labels: tensor([3, 1, 3, 2])\n",
      "tensor([[  -13.1928,  -531.4680,  -235.7976,   782.6654],\n",
      "        [ -213.9321,   816.8583,  -870.5584,   271.7524],\n",
      "        [ 3422.7664, -1353.9440, -7516.0098,  5450.7861],\n",
      "        [ -677.0747,  -234.6320,   828.4746,    87.4457]])\n",
      "argmax out: 3\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 3\n",
      "2correct\n",
      "argmax out: 2\n",
      "3correct\n",
      "labels: tensor([2, 2, 1, 1])\n",
      "tensor([[  -75.8403,   426.7288,  1393.7040, -1741.2845],\n",
      "        [ -372.7648,   581.7793,   524.4509,  -729.0821],\n",
      "        [ -191.8050,   372.9740,   195.9312,  -372.5550],\n",
      "        [  228.3573,  1558.7490, -2608.9497,   827.1646]])\n",
      "argmax out: 2\n",
      "0correct\n",
      "argmax out: 1\n",
      "argmax out: 1\n",
      "2correct\n",
      "argmax out: 1\n",
      "3correct\n",
      "labels: tensor([3, 3, 1, 2])\n",
      "tensor([[ 9.3430e+02,  1.3685e+01, -2.7201e+03,  1.7761e+03],\n",
      "        [ 1.5518e+03, -4.2573e+02, -3.3318e+03,  2.2103e+03],\n",
      "        [-1.2458e+02,  1.4295e+02,  1.2738e+02, -1.4242e+02],\n",
      "        [ 1.2109e+02,  1.8055e+02, -3.0100e+02,  2.3293e+00]])\n",
      "argmax out: 3\n",
      "0correct\n",
      "argmax out: 3\n",
      "1correct\n",
      "argmax out: 1\n",
      "2correct\n",
      "argmax out: 1\n",
      "labels: tensor([3, 2, 2, 2])\n",
      "tensor([[  439.2272,  -367.6122,  -492.4081,   424.4431],\n",
      "        [  -55.8967,   -42.5567,   214.3230,  -113.8191],\n",
      "        [  454.7741,  -450.1393,   519.0207,  -522.2956],\n",
      "        [-1293.0267,  1282.6992,  2616.8296, -2604.4885]])\n",
      "argmax out: 0\n",
      "argmax out: 2\n",
      "1correct\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 2\n",
      "3correct\n",
      "labels: tensor([3, 0, 2, 3])\n",
      "tensor([[ -103.3332,   283.2886,  -623.9375,   449.8781],\n",
      "        [ 1162.0746,  -156.3380, -1421.6112,   417.1770],\n",
      "        [ -813.2170,   387.0536,   233.4422,   196.9866],\n",
      "        [ -655.4171,   481.6635,  -480.2354,   658.1722]])\n",
      "argmax out: 3\n",
      "0correct\n",
      "argmax out: 0\n",
      "1correct\n",
      "argmax out: 1\n",
      "argmax out: 3\n",
      "3correct\n",
      "labels: tensor([1, 2, 3, 1])\n",
      "tensor([[ -520.5089,  2372.5471,  -432.7708, -1416.9603],\n",
      "        [-1211.7544,   309.2926,  1471.1816,  -568.0734],\n",
      "        [  212.9779,  -298.7585, -1152.3312,  1241.4984],\n",
      "        [  -54.0261,  1088.4944,   273.0783, -1304.3979]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 2\n",
      "1correct\n",
      "argmax out: 3\n",
      "2correct\n",
      "argmax out: 1\n",
      "3correct\n",
      "labels: tensor([3, 2, 1, 1])\n",
      "tensor([[   761.0083,   -311.4057,  -2000.3110,   1555.3895],\n",
      "        [ -1445.9874,   5099.4854,   6757.6772, -10409.0283],\n",
      "        [    30.1540,    670.7794,   -354.4910,   -341.8042],\n",
      "        [  -366.7993,    401.8436,   -183.1296,    152.6636]])\n",
      "argmax out: 3\n",
      "0correct\n",
      "argmax out: 2\n",
      "1correct\n",
      "argmax out: 1\n",
      "2correct\n",
      "argmax out: 1\n",
      "3correct\n",
      "labels: tensor([2, 2, 0, 0])\n",
      "tensor([[ 1744.4584,   159.0727,  3953.9722, -5857.6704],\n",
      "        [-1333.0886,   870.0764,  1904.6467, -1439.3481],\n",
      "        [ 1221.5978,   -24.6835, -1116.7142,   -79.3574],\n",
      "        [  681.4118,   119.6595,  -543.6727,  -253.1476]])\n",
      "argmax out: 2\n",
      "0correct\n",
      "argmax out: 2\n",
      "1correct\n",
      "argmax out: 0\n",
      "2correct\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([3, 0, 3, 1])\n",
      "tensor([[   227.7091,   -617.7166,   -433.4456,    827.3303],\n",
      "        [    32.2528,    172.2469,   -217.0257,     16.6018],\n",
      "        [  5382.8867,  -1872.0568, -10520.6416,   7011.4546],\n",
      "        [  -286.2244,    892.8990,    396.8006,   -999.8846]])\n",
      "argmax out: 3\n",
      "0correct\n",
      "argmax out: 1\n",
      "argmax out: 3\n",
      "2correct\n",
      "argmax out: 1\n",
      "3correct\n",
      "labels: tensor([0, 2, 2, 1])\n",
      "tensor([[  804.3620,  -422.0966,  -608.1886,   231.9199],\n",
      "        [ -373.8703,   660.0081,   463.8990,  -745.3947],\n",
      "        [ 1305.2758,  -406.5444,  2616.8479, -3514.8982],\n",
      "        [  308.5067,  1198.9623,    -6.8879, -1498.9467]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 1\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 1\n",
      "3correct\n",
      "labels: tensor([0, 2, 1, 2])\n",
      "tensor([[ 1277.2512, -1021.0429,  -587.4404,   334.1924],\n",
      "        [ -709.0399,    43.2319,   602.3912,    67.7421],\n",
      "        [  213.0054,  1855.9948,   187.1169, -2254.1331],\n",
      "        [  -76.7753,   971.6559,  1684.4877, -2577.5964]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 2\n",
      "1correct\n",
      "argmax out: 1\n",
      "2correct\n",
      "argmax out: 2\n",
      "3correct\n",
      "labels: tensor([3, 1, 1, 0])\n",
      "tensor([[   70.0194,  -381.4276,  -286.9389,   602.9246],\n",
      "        [-1288.4102,  2394.3916,   561.4197, -1666.8970],\n",
      "        [   69.9589,   723.7778,  -383.5887,  -405.3298],\n",
      "        [  417.3222,    65.3838,  -452.2570,   -26.3710]])\n",
      "argmax out: 3\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 1\n",
      "2correct\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([1, 0, 3, 0])\n",
      "tensor([[  176.3114,   745.8032,    74.9622,  -995.2027],\n",
      "        [  479.4566,   106.5869,  -860.5322,   279.4743],\n",
      "        [  156.6046,   451.6686, -1629.2811,  1026.1901],\n",
      "        [  366.1890,  -133.0874,  -243.8146,    13.2219]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 0\n",
      "1correct\n",
      "argmax out: 3\n",
      "2correct\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([0, 0, 2, 0])\n",
      "tensor([[ 1295.4662,     8.3851, -1274.9263,   -25.1895],\n",
      "        [  861.8077,  -493.8379,  -985.5976,   620.8989],\n",
      "        [ -538.7911,   511.3577,  1099.0281, -1066.1801],\n",
      "        [ 1155.8278,   147.1409,  -909.6940,  -392.4709]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 0\n",
      "1correct\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([1, 2, 2, 3])\n",
      "tensor([[ -147.2438,   524.1454,    33.6111,  -406.3373],\n",
      "        [ -878.9217,  3027.2583,  4061.9250, -6206.9746],\n",
      "        [ -343.1537,   862.5558,  1236.3138, -1752.6827],\n",
      "        [ -208.9757,   104.1973,  -280.7789,   390.3449]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 2\n",
      "1correct\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 3\n",
      "3correct\n",
      "labels: tensor([2, 0, 3, 3])\n",
      "tensor([[ -472.5970,   473.3223,   934.8446,  -931.0319],\n",
      "        [ 1160.2150,   593.8767, -1364.4041,  -385.0941],\n",
      "        [  415.5604,  -676.4484, -1201.1477,  1462.7963],\n",
      "        [ -563.1580,   216.3274,  -291.2644,   641.8959]])\n",
      "argmax out: 2\n",
      "0correct\n",
      "argmax out: 0\n",
      "1correct\n",
      "argmax out: 3\n",
      "2correct\n",
      "argmax out: 3\n",
      "3correct\n",
      "labels: tensor([0, 2, 2, 2])\n",
      "tensor([[  782.5374,  -279.1082,   109.5896,  -608.4735],\n",
      "        [ -454.7534,   704.1154,   976.7762, -1222.7466],\n",
      "        [-2127.7161,  1521.9690,  3513.6006, -2907.8633],\n",
      "        [ -371.0864,    71.1067,   288.7823,    15.6674]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 2\n",
      "1correct\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 2\n",
      "3correct\n",
      "labels: tensor([0, 0, 0, 3])\n",
      "tensor([[ 596.3264,  -14.1691, -964.9514,  386.7847],\n",
      "        [ 647.5281, -186.0930, -800.8806,  343.4749],\n",
      "        [ 365.8603,   64.0558, -416.2398,   -9.6161],\n",
      "        [ 304.4394, -404.9664, -941.6204, 1045.0314]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 0\n",
      "1correct\n",
      "argmax out: 0\n",
      "2correct\n",
      "argmax out: 3\n",
      "3correct\n",
      "labels: tensor([2, 2, 1, 3])\n",
      "tensor([[ -546.2805,    39.4595,   375.5336,   135.7448],\n",
      "        [ -624.5666,  1038.9952,  1440.0315, -1852.7936],\n",
      "        [ -912.9865,  2313.1089,  1333.7946, -2731.7356],\n",
      "        [  374.4540, -1794.1434, -1388.8741,  2810.0762]])\n",
      "argmax out: 2\n",
      "0correct\n",
      "argmax out: 2\n",
      "1correct\n",
      "argmax out: 1\n",
      "2correct\n",
      "argmax out: 3\n",
      "3correct\n",
      "labels: tensor([3, 1, 0, 0])\n",
      "tensor([[  116.7678, -1131.0366,  -772.3142,  1788.3942],\n",
      "        [-1929.5846,  1280.8033,  1065.8309,  -412.9584],\n",
      "        [ 1147.5944,  -869.3176,  -918.5048,   641.0045],\n",
      "        [ 1123.9578,   526.9625,  -908.0584,  -740.5618]])\n",
      "argmax out: 3\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 0\n",
      "2correct\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([1, 0, 0, 3])\n",
      "tensor([[  -55.9262,  3041.6968,  2673.2451, -5655.9111],\n",
      "        [  616.0049,  -109.3295,  -329.2554,  -175.7550],\n",
      "        [  746.6139,    25.1051,  -428.5041,  -339.3098],\n",
      "        [  239.0094,  -263.2485,  -692.1642,   720.8416]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 0\n",
      "1correct\n",
      "argmax out: 0\n",
      "2correct\n",
      "argmax out: 3\n",
      "3correct\n",
      "labels: tensor([0, 0, 2, 1])\n",
      "tensor([[ 1424.3099,  -205.9330,  -631.2999,  -583.4652],\n",
      "        [  519.6343,  -225.1568,  -375.9345,    84.7582],\n",
      "        [  -52.9855,   235.5889,   124.4287,  -304.1035],\n",
      "        [-1698.1196,  3551.0315,   705.8596, -2560.3303]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 0\n",
      "1correct\n",
      "argmax out: 1\n",
      "argmax out: 1\n",
      "3correct\n",
      "labels: tensor([0, 1, 1, 0])\n",
      "tensor([[  411.7022,   -90.8773,  -638.4009,   321.8928],\n",
      "        [ -134.5709,   493.8500,    98.7204,  -453.9590],\n",
      "        [   12.1833,  3640.9851,  2700.9951, -6350.4028],\n",
      "        [  -71.4974,   -50.8373,  -291.8768,   416.9543]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 1\n",
      "2correct\n",
      "argmax out: 3\n",
      "labels: tensor([0, 2, 2, 0])\n",
      "tensor([[  453.0741,   773.2092,  -557.3190,  -665.3090],\n",
      "        [  354.0233,  -480.7034,  1257.7539, -1129.2684],\n",
      "        [-2377.8809,  3083.5876,  3425.7275, -4132.4219],\n",
      "        [ 1151.5631,  -179.4223,  -881.1204,   -85.3444]])\n",
      "argmax out: 1\n",
      "argmax out: 2\n",
      "1correct\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([1, 2, 2, 0])\n",
      "tensor([[  268.0259,   767.9327,  -118.0417,  -914.2934],\n",
      "        [   18.8164,    59.5158,  1057.0366, -1131.8522],\n",
      "        [ -816.1472,   877.7086,  1330.2211, -1390.3379],\n",
      "        [  379.2675,   241.9530,  -147.7521,  -468.7498]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 2\n",
      "1correct\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([0, 3, 3, 2])\n",
      "tensor([[   382.8543,    165.1819,   -587.6409,     43.3179],\n",
      "        [   -17.8934,   -368.8042,   -242.7921,    633.7720],\n",
      "        [  5089.9365,  -2802.2800, -10446.0117,   8160.4297],\n",
      "        [  -664.7922,    277.3724,    130.8036,    260.7388]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 3\n",
      "1correct\n",
      "argmax out: 3\n",
      "2correct\n",
      "argmax out: 1\n",
      "labels: tensor([1, 3, 2, 2])\n",
      "tensor([[-108.3007,  568.6693,  150.4595, -606.4529],\n",
      "        [  31.9754, -142.9286, -246.3047,  361.1740],\n",
      "        [ -88.1854, -354.1763,  264.5123,  181.9320],\n",
      "        [-469.4505,  486.5303,  788.8300, -801.5664]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 3\n",
      "1correct\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 2\n",
      "3correct\n",
      "labels: tensor([2, 1, 0, 1])\n",
      "tensor([[ -519.7632,   817.0598,  1748.6675, -2045.7784],\n",
      "        [  266.0545,  2108.5366, -1072.1896, -1298.6202],\n",
      "        [  845.5250,   223.9615,  -942.9664,  -122.7455],\n",
      "        [   75.8909,   243.2865,  -442.9264,   128.7145]])\n",
      "argmax out: 2\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 0\n",
      "2correct\n",
      "argmax out: 1\n",
      "3correct\n",
      "labels: tensor([1, 3, 2, 0])\n",
      "tensor([[  116.4730,  1435.7449,   325.6774, -1874.6852],\n",
      "        [ -476.9833,   107.9880,  -551.0302,   923.7126],\n",
      "        [ -417.4804,   263.6571,   596.2635,  -438.6465],\n",
      "        [ 1913.9250,   397.4286, -2711.7788,   403.8701]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 3\n",
      "1correct\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([2, 3, 0, 3])\n",
      "tensor([[ -101.7331,   146.2140,   127.3193,  -167.2731],\n",
      "        [   30.4001, -1099.0355,  -446.8180,  1516.2971],\n",
      "        [  111.4162,   278.9807,  -488.4162,    99.8594],\n",
      "        [  691.7742,  -426.8078, -1764.0876,  1503.9044]])\n",
      "argmax out: 1\n",
      "argmax out: 3\n",
      "1correct\n",
      "argmax out: 1\n",
      "argmax out: 3\n",
      "3correct\n",
      "labels: tensor([2, 0, 1, 1])\n",
      "tensor([[-1.0534e+03,  5.0109e+02,  1.4955e+03, -9.4014e+02],\n",
      "        [ 2.3706e+03,  9.1221e+02, -4.2056e+03,  9.2771e+02],\n",
      "        [ 3.3131e+00,  7.4730e+02, -9.5582e+02,  2.0936e+02],\n",
      "        [-3.8986e+02,  8.9947e+02,  2.8233e-01, -5.0514e+02]])\n",
      "argmax out: 2\n",
      "0correct\n",
      "argmax out: 0\n",
      "1correct\n",
      "argmax out: 1\n",
      "2correct\n",
      "argmax out: 1\n",
      "3correct\n",
      "labels: tensor([1, 0, 3, 1])\n",
      "tensor([[   30.9456,   357.5876,  -204.7615,  -180.5050],\n",
      "        [  566.9766,   135.4048,  -675.3540,   -22.7894],\n",
      "        [ 4248.3452, -2107.0632, -9613.3555,  7474.6689],\n",
      "        [ -153.0758,   842.9315,  -126.9795,  -559.6896]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 0\n",
      "1correct\n",
      "argmax out: 3\n",
      "2correct\n",
      "argmax out: 1\n",
      "3correct\n",
      "labels: tensor([1, 3, 1, 3])\n",
      "tensor([[-1001.4160,  2028.6538,  1174.5530, -2199.3464],\n",
      "        [   -6.0785,   111.6683, -1536.1882,  1433.4358],\n",
      "        [ -244.4343,  1649.0151,  1137.0405, -2540.0012],\n",
      "        [   60.8419, -1507.6055, -1263.0154,  2713.2351]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 3\n",
      "1correct\n",
      "argmax out: 1\n",
      "2correct\n",
      "argmax out: 3\n",
      "3correct\n",
      "labels: tensor([0, 1, 0, 3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  912.8445,   104.2696,  -832.2355,  -180.4438],\n",
      "        [ -362.2013,   724.0480,   125.4180,  -484.6555],\n",
      "        [ 1068.5063,   700.8339, -1258.4369,  -509.4034],\n",
      "        [ -241.6171,   567.4251,  -921.8184,   598.4452]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 0\n",
      "2correct\n",
      "argmax out: 3\n",
      "3correct\n",
      "labels: tensor([1, 3, 2, 1])\n",
      "tensor([[ -530.2075,  1706.9431,  -159.6423, -1012.4838],\n",
      "        [    7.5745,  -423.1631, -1273.3169,  1691.7072],\n",
      "        [ -951.8186,  1903.3309,  2315.4768, -3265.4880],\n",
      "        [ -114.6841,  2876.4006,   812.2487, -3571.6304]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 3\n",
      "1correct\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 1\n",
      "3correct\n",
      "labels: tensor([2, 0, 1, 0])\n",
      "tensor([[ -348.6103,  2956.0791,  3645.3533, -6249.7939],\n",
      "        [  426.0378,   184.6531,  -727.0202,   120.1495],\n",
      "        [  -54.2570,   349.2798,  -253.1583,   -37.3743],\n",
      "        [  461.4566,    97.3050,  -707.6160,   153.0686]])\n",
      "argmax out: 2\n",
      "0correct\n",
      "argmax out: 0\n",
      "1correct\n",
      "argmax out: 1\n",
      "2correct\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([0, 2, 0, 2])\n",
      "tensor([[ 1848.5903,  -119.1555,  -891.5110,  -836.6628],\n",
      "        [ -125.8100,  1169.0393,  2337.5830, -3378.8271],\n",
      "        [ 1284.2002,  -362.9059,   840.0512, -1758.4421],\n",
      "        [-1925.2147,   989.2200,  1145.8591,  -206.1286]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 2\n",
      "1correct\n",
      "argmax out: 0\n",
      "2correct\n",
      "argmax out: 2\n",
      "3correct\n",
      "labels: tensor([1, 0, 0, 3])\n",
      "tensor([[ -930.0139,   984.0994, -1135.5958,  1085.7749],\n",
      "        [ 1830.5315,  -970.4246,  -990.8203,   134.7365],\n",
      "        [ 1230.2036,   336.1403,  -850.7660,  -710.1155],\n",
      "        [  314.3331,  -240.3087,  -920.8593,   848.1259]])\n",
      "argmax out: 3\n",
      "argmax out: 0\n",
      "1correct\n",
      "argmax out: 0\n",
      "2correct\n",
      "argmax out: 3\n",
      "3correct\n",
      "labels: tensor([2, 0, 3, 2])\n",
      "tensor([[ -177.3306,   207.9490,    43.4779,   -70.1833],\n",
      "        [ 1230.8918,  -429.5616, -1075.0468,   276.0403],\n",
      "        [  481.6443,  -746.2265, -1105.3868,  1372.0525],\n",
      "        [ -215.5925,   797.8363,   902.0718, -1478.9656]])\n",
      "argmax out: 1\n",
      "argmax out: 0\n",
      "1correct\n",
      "argmax out: 3\n",
      "2correct\n",
      "argmax out: 2\n",
      "3correct\n",
      "labels: tensor([2, 2, 0, 2])\n",
      "tensor([[-1678.0909,  1098.0946,  2545.8584, -1964.7336],\n",
      "        [ -421.5403,   332.7675,   135.4131,   -43.0364],\n",
      "        [  758.5881,   429.9780,  -919.1045,  -268.2096],\n",
      "        [ -399.6464,   209.7774,   440.8747,  -247.4082]])\n",
      "argmax out: 2\n",
      "0correct\n",
      "argmax out: 1\n",
      "argmax out: 0\n",
      "2correct\n",
      "argmax out: 2\n",
      "3correct\n",
      "labels: tensor([1, 2, 1, 1])\n",
      "tensor([[   90.4280,  1279.3077, -1072.1920,  -293.5199],\n",
      "        [  -30.9731,  -135.6057,  1931.7458, -1761.4167],\n",
      "        [   40.9068,  1477.4691,   550.0012, -2069.8645],\n",
      "        [  419.0528,   449.5364,  -963.0285,    96.1947]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 2\n",
      "1correct\n",
      "argmax out: 1\n",
      "2correct\n",
      "argmax out: 1\n",
      "3correct\n",
      "labels: tensor([2, 2, 0, 2])\n",
      "tensor([[ -320.5121,   371.0544,   468.4191,  -516.4208],\n",
      "        [ -322.9349,   411.8859,   157.5785,  -244.1239],\n",
      "        [ 1524.6487, -1741.6826,   409.5805,  -191.0294],\n",
      "        [ -248.6585,   493.0322,   432.7708,  -672.7604]])\n",
      "argmax out: 2\n",
      "0correct\n",
      "argmax out: 1\n",
      "argmax out: 0\n",
      "2correct\n",
      "argmax out: 1\n",
      "labels: tensor([1, 1, 0, 2])\n",
      "tensor([[-102.4493,  599.4053,   11.5950, -506.0269],\n",
      "        [  -4.6018,  377.6305,   52.3121, -419.9091],\n",
      "        [ 513.7258,  221.6536, -829.4609,   98.5066],\n",
      "        [   1.4052, -104.0045,  464.6983, -356.9397]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 0\n",
      "2correct\n",
      "argmax out: 2\n",
      "3correct\n",
      "labels: tensor([3, 1, 1, 0])\n",
      "tensor([[   92.8952,  -199.7936,  -350.1571,   460.4400],\n",
      "        [  -92.7241,   491.9637,   -12.4213,  -383.1597],\n",
      "        [-1071.1370,  4595.4004,  4077.5203, -7599.9155],\n",
      "        [  184.5726,  -166.8489,   -90.8066,    75.6955]])\n",
      "argmax out: 3\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 1\n",
      "2correct\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([0, 1, 0, 3])\n",
      "tensor([[  631.6099,    88.7013,  -582.7205,  -134.1745],\n",
      "        [ -280.1834,   358.5061,   -69.0492,    -6.4298],\n",
      "        [ 1972.9530,   124.6330, -1912.8453,  -183.8997],\n",
      "        [  375.3144, -1771.5896, -1705.8755,  3105.1194]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 0\n",
      "2correct\n",
      "argmax out: 3\n",
      "3correct\n",
      "labels: tensor([3, 3, 2, 1])\n",
      "tensor([[ -925.7472,   554.1436, -1145.3887,  1521.2126],\n",
      "        [  687.2592,  -719.4516,  -955.4081,   992.1998],\n",
      "        [ -386.0598,   493.2376,   707.9734,  -811.4974],\n",
      "        [ -456.7612,  1024.2096,   696.4050, -1260.4320]])\n",
      "argmax out: 3\n",
      "0correct\n",
      "argmax out: 3\n",
      "1correct\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 1\n",
      "3correct\n",
      "labels: tensor([2, 1, 2, 2])\n",
      "tensor([[-1625.1108,   251.4878,  1296.9934,    77.0686],\n",
      "        [   69.0071,  1140.1840,   187.5307, -1393.1470],\n",
      "        [ -208.7008,   144.5841,  1851.7286, -1785.7906],\n",
      "        [ -481.3159,   465.1253,   713.2589,  -691.8099]])\n",
      "argmax out: 2\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 2\n",
      "3correct\n",
      "labels: tensor([3, 3, 2, 0])\n",
      "tensor([[ -152.4122,  -455.3508, -1535.9041,  2146.8508],\n",
      "        [  470.2517,  -646.7517,  -502.1392,   684.9448],\n",
      "        [  189.0225,  -347.1850,   504.6324,  -343.9904],\n",
      "        [ -157.6032,   247.3894,  -215.0172,   129.2221]])\n",
      "argmax out: 3\n",
      "0correct\n",
      "argmax out: 3\n",
      "1correct\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 1\n",
      "labels: tensor([3, 1, 0, 1])\n",
      "tensor([[ -224.8246,   387.6541, -1129.0569,   965.4863],\n",
      "        [ -127.0911,  2106.6873,  -737.3952, -1235.6674],\n",
      "        [ 1308.6136,   -96.0798, -1835.0725,   630.2020],\n",
      "        [ -197.8225,  1577.2437,  -255.0819, -1119.7335]])\n",
      "argmax out: 3\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 0\n",
      "2correct\n",
      "argmax out: 1\n",
      "3correct\n",
      "labels: tensor([2, 1, 1, 0])\n",
      "tensor([[ -154.6095,   160.9838,   218.9615,  -222.4932],\n",
      "        [  220.3921,   764.4531,  -412.7052,  -566.9596],\n",
      "        [  187.4158,   217.8318, -1367.4929,   967.1757],\n",
      "        [ 1420.3793,  -829.2468,  -921.4426,   334.4313]])\n",
      "argmax out: 2\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 3\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([0, 1, 2, 3])\n",
      "tensor([[  283.4133,   228.3209,  -633.0213,   123.1881],\n",
      "        [-1234.2646,   418.8254,   334.7103,   484.3563],\n",
      "        [-1977.0518,   473.7059,  1156.2396,   351.7626],\n",
      "        [  704.5587,   297.2589, -2654.6313,  1656.9343]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 3\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 3\n",
      "3correct\n",
      "labels: tensor([0, 2, 0, 2])\n",
      "tensor([[ 1593.3501,    11.3377, -2138.8879,   542.4059],\n",
      "        [ -523.7845,   302.7947,   960.8756,  -735.2696],\n",
      "        [  435.9704,   254.9375,  -800.5485,   111.3117],\n",
      "        [  482.9093,   417.3242,  1340.8809, -2236.3254]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 2\n",
      "1correct\n",
      "argmax out: 0\n",
      "2correct\n",
      "argmax out: 2\n",
      "3correct\n",
      "labels: tensor([1, 3, 1, 0])\n",
      "tensor([[  149.2005,  1017.2429,  -523.0593,  -639.4359],\n",
      "        [ 1022.9839,  -254.6100, -2334.2605,  1565.7292],\n",
      "        [  170.4709,  2438.9961,  -796.3748, -1809.6399],\n",
      "        [  735.0159,  -137.4612,  -820.5677,   225.8507]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 3\n",
      "1correct\n",
      "argmax out: 1\n",
      "2correct\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([1, 0, 2, 1])\n",
      "tensor([[   85.9475,  1694.7947,  -370.6846, -1407.0521],\n",
      "        [ 1642.7172,  -184.9152, -2267.2935,   814.1846],\n",
      "        [ -152.8855,   237.9774,   265.1476,  -346.6131],\n",
      "        [   49.7404,   543.0939,  -484.7324,  -103.1961]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 0\n",
      "1correct\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 1\n",
      "3correct\n",
      "labels: tensor([0, 2, 1, 0])\n",
      "tensor([[  974.4234,   143.1702,  -678.3705,  -434.0251],\n",
      "        [ -545.5532,  1120.1541,  1732.5565, -2306.2776],\n",
      "        [ -434.7658,  1743.8450,   486.7573, -1790.5142],\n",
      "        [   97.3977,   134.7667,  -168.5003,   -59.4922]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 2\n",
      "1correct\n",
      "argmax out: 1\n",
      "2correct\n",
      "argmax out: 1\n",
      "labels: tensor([0, 1, 0, 0])\n",
      "tensor([[ 1163.9086,  -363.7044,  -487.5155,  -310.7831],\n",
      "        [  411.7903,  1920.2240,   441.8134, -2771.2424],\n",
      "        [  672.8564,    98.1855,  -718.4786,   -48.2263],\n",
      "        [  950.3995,  -293.1357,  -945.8895,   291.2574]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 0\n",
      "2correct\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([0, 0, 0, 2])\n",
      "tensor([[ 1.9599e+02,  7.7679e+01, -2.9220e+02,  2.2844e+01],\n",
      "        [-3.0415e+02,  9.6605e+02, -1.1102e+03,  4.5277e+02],\n",
      "        [ 2.0976e+02,  1.7584e+00, -2.3719e+02,  2.9858e+01],\n",
      "        [-7.5796e+02,  3.2982e+03,  4.4564e+03, -6.9929e+03]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 1\n",
      "argmax out: 0\n",
      "2correct\n",
      "argmax out: 2\n",
      "3correct\n",
      "labels: tensor([0, 0, 2, 0])\n",
      "tensor([[  730.5441,  -307.2309,  -661.7239,   242.9363],\n",
      "        [   56.9744,   238.5873,  -199.7215,   -91.6435],\n",
      "        [ -381.7685,  1232.4369,  1813.1730, -2663.6541],\n",
      "        [  947.7014,  -209.5848, -1301.7224,   564.7739]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 1\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([1, 0, 0, 3])\n",
      "tensor([[ -427.8323,   258.0034,  -244.3001,   418.0794],\n",
      "        [ 5001.8149, -1130.8254, -7371.7798,  3504.6819],\n",
      "        [  988.7396,  -449.7451,  -380.0809,  -155.6368],\n",
      "        [  257.6481,  -445.0938,  -732.9508,   924.3962]])\n",
      "argmax out: 3\n",
      "argmax out: 0\n",
      "1correct\n",
      "argmax out: 0\n",
      "2correct\n",
      "argmax out: 3\n",
      "3correct\n",
      "labels: tensor([3, 3, 2, 2])\n",
      "tensor([[ -529.3356,   164.5107,  -613.7772,   982.2499],\n",
      "        [ -244.6025,   382.7489,  -437.5241,   303.3111],\n",
      "        [ -986.6262,  3336.5654,  4779.7280, -7126.0566],\n",
      "        [ -232.1552,   647.0085,   467.6948,  -877.7863]])\n",
      "argmax out: 3\n",
      "0correct\n",
      "argmax out: 1\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 1\n",
      "labels: tensor([0, 0, 1, 2])\n",
      "tensor([[ 1570.8934,  -422.3289, -1285.7958,   140.1550],\n",
      "        [  725.7279,    25.8397,  -194.6115,  -554.3905],\n",
      "        [ -395.8108,  1450.6948,  -495.9253,  -555.2396],\n",
      "        [  -84.5126,  1213.0542,  2550.8550, -3677.1550]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 0\n",
      "1correct\n",
      "argmax out: 1\n",
      "2correct\n",
      "argmax out: 2\n",
      "3correct\n",
      "labels: tensor([2, 0, 2, 3])\n",
      "tensor([[ -276.0486,   267.8545,   371.4583,  -359.5432],\n",
      "        [  491.5971,   505.7985,  -897.0516,   -96.1317],\n",
      "        [ -811.3208,   787.7605,  1294.9791, -1269.3704],\n",
      "        [ 3218.9600, -1717.7485, -7310.6865,  5813.5273]])\n",
      "argmax out: 2\n",
      "0correct\n",
      "argmax out: 1\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 3\n",
      "3correct\n",
      "labels: tensor([0, 2, 1, 3])\n",
      "tensor([[  402.9680,    64.5412,  -362.1427,  -100.6408],\n",
      "        [ -600.0579,   351.2791,  1179.6455,  -925.8649],\n",
      "        [  414.3600,   938.6194,   -89.8999, -1263.3632],\n",
      "        [ -288.2546,   226.4248,  -323.0061,   388.9014]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 2\n",
      "1correct\n",
      "argmax out: 1\n",
      "2correct\n",
      "argmax out: 3\n",
      "3correct\n",
      "labels: tensor([3, 1, 2, 0])\n",
      "tensor([[ -252.4026,    48.0007,  -319.0403,   528.1404],\n",
      "        [  318.7874,  1061.7827,  -156.7916, -1219.2740],\n",
      "        [  127.8829,   956.3743,  -711.4070,  -368.2281],\n",
      "        [ 2951.7100,  1426.0287, -4300.7441,   -73.4682]])\n",
      "argmax out: 3\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 1\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([2, 1, 2, 1])\n",
      "tensor([[ 183.9552, -256.4228,  418.8223, -344.1589],\n",
      "        [ -45.1202,  473.9339, -235.0796, -187.2614],\n",
      "        [-487.6737,  493.5919,  780.7599, -784.8458],\n",
      "        [ 248.1911, 1681.2832, -979.0088, -946.1426]])\n",
      "argmax out: 2\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 1\n",
      "3correct\n",
      "labels: tensor([1, 3, 3, 1])\n",
      "tensor([[  149.7103,  1283.6119,    54.0976, -1487.2791],\n",
      "        [   23.5314,  -101.2195, -1299.5508,  1380.4413],\n",
      "        [  608.0385,  -179.2653, -1011.4982,   586.6974],\n",
      "        [  194.7846,   240.7615,  -415.3209,   -17.7367]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 3\n",
      "1correct\n",
      "argmax out: 0\n",
      "argmax out: 1\n",
      "3correct\n",
      "labels: tensor([1, 1, 3, 2])\n",
      "tensor([[ -687.4451,   539.6176,   202.9957,   -51.5339],\n",
      "        [ -274.3391,   482.9063,   -32.8856,  -169.4317],\n",
      "        [ -591.2864,   471.9771,  -153.3032,   276.0056],\n",
      "        [-1180.8391,   887.4470,  1765.1438, -1468.9095]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 1\n",
      "argmax out: 2\n",
      "3correct\n",
      "labels: tensor([1, 3, 2, 1])\n",
      "tensor([[  -55.4247,  2453.1235,  1610.8330, -4004.2397],\n",
      "        [ -388.4946,   192.9514,   -57.8258,   257.0762],\n",
      "        [ -157.4359,   343.8699,   213.2969,  -395.6297],\n",
      "        [ -307.5114,  2442.8223,  2270.5007, -4401.7246]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 3\n",
      "1correct\n",
      "argmax out: 1\n",
      "argmax out: 1\n",
      "3correct\n",
      "labels: tensor([3, 2, 3, 3])\n",
      "tensor([[  -47.1503,  -114.5137,  -609.7756,   777.1210],\n",
      "        [ -672.3111,   216.4405,   273.3574,   187.0695],\n",
      "        [ 1539.4615, -1130.6431, -3645.2229,  3243.6016],\n",
      "        [  597.7056, -1216.2289, -1472.7415,  2094.5647]])\n",
      "argmax out: 3\n",
      "0correct\n",
      "argmax out: 2\n",
      "1correct\n",
      "argmax out: 3\n",
      "2correct\n",
      "argmax out: 3\n",
      "3correct\n",
      "labels: tensor([0, 1, 1, 2])\n",
      "tensor([[  333.0410,   381.4656, -1230.6730,   516.7706],\n",
      "        [ -142.1665,   148.1544,     7.6988,    -8.4308],\n",
      "        [  299.3856,  2353.7190,   -55.3175, -2596.1182],\n",
      "        [ -135.9854,   230.9446,   203.3209,  -293.1946]])\n",
      "argmax out: 3\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 1\n",
      "2correct\n",
      "argmax out: 1\n",
      "labels: tensor([1, 1, 3, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  264.7055,  1176.3257,  -266.1067, -1170.2365],\n",
      "        [-1706.8901,  2608.9199,  1649.9819, -2552.2817],\n",
      "        [-1725.6960,  1071.9338,  -114.2169,   770.3801],\n",
      "        [  578.0967,  -105.3959,    28.6137,  -497.2942]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 1\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([2, 2, 1, 0])\n",
      "tensor([[ -782.0535,    97.4313,   901.6715,  -212.9827],\n",
      "        [-1001.9368,    47.5734,  1398.6458,  -442.3016],\n",
      "        [  336.3195,  1261.7964,  -705.1199,  -888.2096],\n",
      "        [  808.7646,  -239.8881,  -790.3754,   223.9138]])\n",
      "argmax out: 2\n",
      "0correct\n",
      "argmax out: 2\n",
      "1correct\n",
      "argmax out: 1\n",
      "2correct\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([1, 1, 2, 2])\n",
      "tensor([[-1693.7977,  1183.0365,   213.0139,   298.7075],\n",
      "        [  287.1426,  1697.8501, -2086.0039,   105.5448],\n",
      "        [ -509.1367,   456.7550,  1123.2936, -1069.7174],\n",
      "        [ -340.9164,   439.9195,   494.3476,  -588.8518]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 2\n",
      "3correct\n",
      "labels: tensor([1, 0, 2, 0])\n",
      "tensor([[ -209.0231,   483.2254,  -185.4194,   -86.0191],\n",
      "        [  710.6371,  -146.1665,    81.9360,  -641.4477],\n",
      "        [ -306.1845,   180.3559,   231.8513,  -100.5106],\n",
      "        [ 1679.2623,   279.1865, -1907.5151,   -49.0771]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 0\n",
      "1correct\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([1, 1, 0, 3])\n",
      "tensor([[  -21.1407,  2391.9019,  -158.1652, -2211.4521],\n",
      "        [ -175.7178,   937.2255,    33.1289,  -789.9479],\n",
      "        [ 2085.6353,   505.1971, -2201.7258,  -387.4815],\n",
      "        [   16.1339,    73.4321, -2099.0632,  2012.6111]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 0\n",
      "2correct\n",
      "argmax out: 3\n",
      "3correct\n",
      "labels: tensor([1, 3, 2, 2])\n",
      "tensor([[ -140.6484,  1376.1890,  -864.9789,  -365.4438],\n",
      "        [ -250.9783,   370.0575,  -118.0660,     3.3253],\n",
      "        [ -441.7667,    92.7478,   551.4690,  -196.5940],\n",
      "        [ -412.4787,   418.5314,  1161.1429, -1162.7186]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 1\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 2\n",
      "3correct\n",
      "labels: tensor([1, 1, 1, 0])\n",
      "tensor([[   84.8426,  1271.9749,    31.1910, -1384.0983],\n",
      "        [ -244.3559,  2908.7427,  1248.8170, -3911.8132],\n",
      "        [  318.0239,  1059.3829,   178.8082, -1552.1989],\n",
      "        [ 1912.0179, -1174.4106,  -183.9149,  -551.3847]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 1\n",
      "2correct\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([0, 2, 2, 1])\n",
      "tensor([[ 576.9863,  -49.1213, -803.4240,  278.5191],\n",
      "        [-581.5450,  193.1289,  560.6058, -167.9982],\n",
      "        [-656.3723,  602.1891, 1053.0767, -993.7841],\n",
      "        [-248.3101,  274.1845, -111.0137,   90.1018]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 2\n",
      "1correct\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 1\n",
      "3correct\n",
      "labels: tensor([3, 3, 0, 0])\n",
      "tensor([[  205.8195,  -831.4014,  -493.7361,  1121.8458],\n",
      "        [  662.3828,  -285.5595,  -852.2250,   479.8049],\n",
      "        [ 2518.1396,   682.7305, -2184.1733, -1016.8445],\n",
      "        [  510.4825,    25.0445,  -907.4390,   375.2506]])\n",
      "argmax out: 3\n",
      "0correct\n",
      "argmax out: 0\n",
      "argmax out: 0\n",
      "2correct\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([2, 0, 0, 0])\n",
      "tensor([[ -150.4194,  1192.4493,  2057.3552, -3097.3618],\n",
      "        [ 1485.7898,   115.1528, -1150.0747,  -447.5000],\n",
      "        [  155.7493,   318.8427,  -167.4674,  -302.5915],\n",
      "        [ 1825.3197,   213.9361, -1488.5620,  -547.3018]])\n",
      "argmax out: 2\n",
      "0correct\n",
      "argmax out: 0\n",
      "1correct\n",
      "argmax out: 1\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([3, 3, 1, 0])\n",
      "tensor([[  104.9720,   267.6369,  -784.7429,   415.0419],\n",
      "        [  737.8228,  -157.8547, -1783.4525,  1208.2954],\n",
      "        [   85.3238,   354.6821,  -413.1652,   -21.7199],\n",
      "        [  351.1375,  -553.1588,  -386.5773,   592.8533]])\n",
      "argmax out: 3\n",
      "0correct\n",
      "argmax out: 3\n",
      "1correct\n",
      "argmax out: 1\n",
      "2correct\n",
      "argmax out: 3\n",
      "labels: tensor([3, 0, 3, 0])\n",
      "tensor([[  144.8306,  -596.6929,  -222.5194,   679.4246],\n",
      "        [   89.1095,   354.5386,  -356.6145,   -82.2328],\n",
      "        [  528.4125,  -395.5791, -1100.8458,   970.7025],\n",
      "        [ 1506.6512,   184.9082, -1497.1724,  -190.6241]])\n",
      "argmax out: 3\n",
      "0correct\n",
      "argmax out: 1\n",
      "argmax out: 3\n",
      "2correct\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([3, 2, 1, 2])\n",
      "tensor([[ 1037.0417, -2945.4333, -1135.4208,  3047.8340],\n",
      "        [ -457.6268,   729.5631,  1327.4602, -1595.1501],\n",
      "        [  199.4797,  1802.1758,  -688.5954, -1309.1036],\n",
      "        [-1835.1631,   697.2601,  1184.6802,   -42.4577]])\n",
      "argmax out: 3\n",
      "0correct\n",
      "argmax out: 2\n",
      "1correct\n",
      "argmax out: 1\n",
      "2correct\n",
      "argmax out: 2\n",
      "3correct\n",
      "labels: tensor([0, 3, 0, 0])\n",
      "tensor([[ 1193.3555,     3.6428,  -601.5093,  -591.2261],\n",
      "        [   21.8247,   230.6217,  -673.0734,   421.6810],\n",
      "        [ 1091.8191,  -333.5913,  -699.0760,   -57.2394],\n",
      "        [ 1846.0018,   702.4401, -2375.3457,  -172.4402]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 3\n",
      "1correct\n",
      "argmax out: 0\n",
      "2correct\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([3, 1, 2, 2])\n",
      "tensor([[ -220.3308,   145.2162,  -296.7365,   377.9324],\n",
      "        [  -70.8620,   759.7081,  -535.9125,  -147.4034],\n",
      "        [ -308.6996,   402.5715,   193.1319,  -285.0713],\n",
      "        [ -724.7236,   794.5815,  1074.9312, -1142.9082]])\n",
      "argmax out: 3\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 1\n",
      "argmax out: 2\n",
      "3correct\n",
      "labels: tensor([1, 3, 2, 3])\n",
      "tensor([[  575.6067,  1195.1104, -1027.3832,  -738.6567],\n",
      "        [  530.4875, -1210.2455, -2156.7615,  2836.5950],\n",
      "        [-1315.9089,   137.6222,  1388.7986,  -207.4052],\n",
      "        [  508.8274,  -541.6190, -1421.1764,  1456.7646]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 3\n",
      "1correct\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 3\n",
      "3correct\n",
      "labels: tensor([0, 1, 2, 3])\n",
      "tensor([[  589.0274,    44.8341, -1090.9270,   458.9380],\n",
      "        [ -341.3722,   830.9046,  -420.4109,   -66.7725],\n",
      "        [-1891.9758,   609.8748,  1599.3633,  -316.6432],\n",
      "        [  248.1730,  -822.5100,  -728.9659,  1305.3251]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 3\n",
      "3correct\n",
      "labels: tensor([3, 2, 0, 2])\n",
      "tensor([[ -119.7764,   -26.4479,  -251.7016,   402.1743],\n",
      "        [ -280.7392,  1107.3408,  1314.4889, -2140.4949],\n",
      "        [ 1170.0942,   -78.1412,   663.7659, -1752.7448],\n",
      "        [ -441.9789,   406.5129,   582.5638,  -541.9995]])\n",
      "argmax out: 3\n",
      "0correct\n",
      "argmax out: 2\n",
      "1correct\n",
      "argmax out: 0\n",
      "2correct\n",
      "argmax out: 2\n",
      "3correct\n",
      "labels: tensor([3, 0, 0, 2])\n",
      "tensor([[  210.8957,  -651.7358,  -265.4528,   710.7274],\n",
      "        [ -477.7647,    59.0574,   257.5005,   165.1022],\n",
      "        [ 1346.4154,   761.0225, -1782.9639,  -319.6841],\n",
      "        [ 1128.3108,  -696.2266,  2710.2053, -3142.6355]])\n",
      "argmax out: 3\n",
      "0correct\n",
      "argmax out: 2\n",
      "argmax out: 0\n",
      "2correct\n",
      "argmax out: 2\n",
      "3correct\n",
      "labels: tensor([1, 0, 3, 2])\n",
      "tensor([[   84.2514,   264.0170,  -157.5450,  -187.1146],\n",
      "        [  440.7523,   235.3533,  -313.1756,  -359.0740],\n",
      "        [ 4862.4785, -1834.7454, -9144.7764,  6118.4092],\n",
      "        [ -236.6138,   410.6393,   587.7197,  -756.9739]])\n",
      "argmax out: 1\n",
      "0correct\n",
      "argmax out: 0\n",
      "1correct\n",
      "argmax out: 3\n",
      "2correct\n",
      "argmax out: 2\n",
      "3correct\n",
      "labels: tensor([0, 2, 2, 0])\n",
      "tensor([[ 1936.8578,    19.3894, -1855.6781,   -99.3044],\n",
      "        [  -12.1679,  -213.9249,  1850.3550, -1623.1461],\n",
      "        [  -74.4424,  -412.7239,   947.9371,  -458.7964],\n",
      "        [  243.2663,   124.7281,  -320.2291,   -42.2211]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 2\n",
      "1correct\n",
      "argmax out: 2\n",
      "2correct\n",
      "argmax out: 0\n",
      "3correct\n",
      "labels: tensor([0, 3, 0, 1])\n",
      "tensor([[ 2.2660e+03,  9.3396e+02, -4.0065e+03,  8.1103e+02],\n",
      "        [-4.7532e+02,  3.0253e+02, -7.0743e+02,  8.8401e+02],\n",
      "        [ 1.7107e+02,  1.7362e+02, -3.3855e+02, -1.7069e+00],\n",
      "        [-2.0677e+02,  3.2940e+02,  8.3043e+00, -1.2681e+02]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 3\n",
      "1correct\n",
      "argmax out: 1\n",
      "argmax out: 1\n",
      "3correct\n",
      "labels: tensor([0, 1, 0, 2])\n",
      "tensor([[ 1158.3115,  -114.3903,  -819.4058,  -223.7364],\n",
      "        [ -164.2794,  2021.0693,  1235.8087, -3090.8018],\n",
      "        [  786.9994,  -101.1521,  -780.2526,    98.3410],\n",
      "        [-1242.0957,   211.9682,  1367.8569,  -335.5433]])\n",
      "argmax out: 0\n",
      "0correct\n",
      "argmax out: 1\n",
      "1correct\n",
      "argmax out: 0\n",
      "2correct\n",
      "argmax out: 2\n",
      "3correct\n",
      "Accuracy of the network: 88.54166666666667 %\n",
      "Accuracy of BSD_11: 85.98130841121495 %\n",
      "Accuracy of BSD_21: 95.78947368421052 %\n",
      "Accuracy of BSD_31: 83.49514563106796 %\n",
      "Accuracy of BSD_P1: 89.87341772151899 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    classes = ['BSD_11', 'BSD_21', 'BSD_31', 'BSD_P1']\n",
    "    \n",
    "    #collect information about labels, predictions\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(4)]\n",
    "    n_class_samples = [0 for i in range(4)]\n",
    "    n_class_samples_out = [0 for i in range(4)]\n",
    "    \n",
    "    #iterate through bateches in test_loader\n",
    "    for window, labels in test_loader:\n",
    "        #make predictions for each batch\n",
    "        print(f\"labels: {labels}\")\n",
    "        outputs = model(window.float())\n",
    "        print(outputs)\n",
    "        #for each element in batch check if prediction is correct and collect total and correct predictions and labels\n",
    "        for i in range(batch_size):\n",
    "            if len(labels)==4:\n",
    "                label = labels[i]\n",
    "                output = torch.argmax(outputs[i])\n",
    "                print(f\"argmax out: {output}\")\n",
    "                if label == output:\n",
    "                    n_correct+=1\n",
    "                    n_class_correct[label]+=1\n",
    "                    print(f'{i}correct')\n",
    "                \n",
    "                n_samples+=1\n",
    "                n_class_samples[label]+=1\n",
    "                n_class_samples_out[output]+=1\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    #calculate total accuracy\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "    \n",
    "    #calculate class accuracy\n",
    "    for i in range(4):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[96, 117, 88, 83]\n",
      "[107, 95, 103, 79]\n",
      "[93, 92, 87, 75]\n"
     ]
    }
   ],
   "source": [
    "print(n_class_samples_out)\n",
    "print(n_class_samples)\n",
    "print(n_class_correct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
