{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c3cdad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbb85ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#TEST\\nfrequencies = [1,4,1.4,3.6]\\namplitudes = [6,2,5,4]\\nfreq_noise = 0.3\\nampl_noise = 2\\nwindow_size = 1000\\nn_samples, X_data_source, y_data_source, X_data_target, y_data_target = create_data_window(1000, frequencies, amplitudes, freq_noise, ampl_noise, window_size)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_curve(freq, ampl, freq_noise, ampl_noise, window_size):\n",
    "    freq = random.gauss(freq, freq_noise) #add noise to frequency\n",
    "    ampl = random.gauss(ampl, ampl_noise) #add noise to amplitude\n",
    "    time = np.linspace(0, 10*np.pi, window_size)\n",
    "    x = ampl*np.cos(freq*time)\n",
    "    noise = np.random.normal(random.uniform(-1,0), random.uniform(0,1), window_size)\n",
    "    x+=noise\n",
    "    x = np.expand_dims(x, axis = 0) #expand to get 2d array (features, window length)\n",
    "    x = np.expand_dims(x, axis = 0) #expand to get 3d array to store 2d elements\n",
    "    #print(f\"freq: {freq}, ampl:{ampl}\")\n",
    "    #fig1 = plt.figure()\n",
    "    #plt.plot(time, x[0,0,:])\n",
    "    #plt.show()\n",
    "    #fig1.savefig(f'CLASS:{CLASS}, iteration: {it}')\n",
    "    \n",
    "    return x\n",
    "\n",
    "def create_data_window(n, frequencies, amplitudes, freq_noise, ampl_noise, window_size):\n",
    "   \n",
    "    X_data_class_1_domain_1 = np.empty((0,1,window_size))\n",
    "    X_data_class_2_domain_1 = np.empty((0,1,window_size))\n",
    "    X_data_class_1_domain_2 = np.empty((0,1,window_size))\n",
    "    X_data_class_2_domain_2 = np.empty((0,1,window_size))\n",
    "    \n",
    "    for i in range(n):\n",
    "        X_data_class_1_domain_1 = np.append(X_data_class_1_domain_1, create_curve(frequencies[0], amplitudes[0], freq_noise, ampl_noise, window_size), axis = 0) \n",
    "        X_data_class_2_domain_1 = np.append(X_data_class_2_domain_1, create_curve(frequencies[1], amplitudes[1], freq_noise, ampl_noise, window_size), axis = 0)\n",
    "        X_data_class_1_domain_2 = np.append(X_data_class_1_domain_2, create_curve(frequencies[2], amplitudes[2], freq_noise, ampl_noise, window_size), axis = 0)\n",
    "        X_data_class_2_domain_2 = np.append(X_data_class_2_domain_2, create_curve(frequencies[3], amplitudes[3], freq_noise, ampl_noise, window_size), axis = 0)\n",
    "        \n",
    "        #print(i)\n",
    "        #print((X_data_class_1_domain_1))\n",
    "        \n",
    "    n_samples = np.shape(X_data_class_2_domain_1)[0]*2  \n",
    "    \n",
    "    y_data_class_1_domain_1 = np.asarray([0]*np.shape(X_data_class_1_domain_1)[0])\n",
    "    y_data_class_2_domain_1 = np.asarray([1]*np.shape(X_data_class_2_domain_1)[0])\n",
    "    y_data_class_1_domain_2 = np.asarray([0]*np.shape(X_data_class_1_domain_2)[0])\n",
    "    y_data_class_2_domain_2 = np.asarray([1]*np.shape(X_data_class_2_domain_2)[0])\n",
    "    \n",
    "    X_data_source = np.concatenate((X_data_class_1_domain_1, X_data_class_2_domain_1), axis = 0)\n",
    "    y_data_source = np.concatenate((y_data_class_1_domain_1, y_data_class_2_domain_1), axis = 0)\n",
    "    X_data_target = np.concatenate((X_data_class_1_domain_2, X_data_class_2_domain_2), axis = 0)\n",
    "    y_data_target = np.concatenate((y_data_class_1_domain_2, y_data_class_2_domain_2), axis = 0)\n",
    "    \n",
    "    \n",
    "    X_data_source = torch.from_numpy(X_data_source)\n",
    "    y_data_source = torch.from_numpy(y_data_source)\n",
    "    X_data_target = torch.from_numpy(X_data_target)\n",
    "    y_data_target = torch.from_numpy(y_data_target)\n",
    "    \n",
    "    return n_samples, X_data_source, y_data_source, X_data_target, y_data_target\n",
    "\"\"\"\n",
    "#TEST\n",
    "frequencies = [1,4,1.4,3.6]\n",
    "amplitudes = [6,2,5,4]\n",
    "freq_noise = 0.3\n",
    "ampl_noise = 2\n",
    "window_size = 1000\n",
    "n_samples, X_data_source, y_data_source, X_data_target, y_data_target = create_data_window(1000, frequencies, amplitudes, freq_noise, ampl_noise, window_size)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f3733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Dummy_Source_Window(Dataset):\n",
    "\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        n = 8000\n",
    "        frequencies = [1,4,1.9,3.1]#[1,4,1.6,3.4]\n",
    "        amplitudes = [6,2,5,4]\n",
    "        freq_noise = 0.5\n",
    "        ampl_noise = 2\n",
    "        window_size = 1000\n",
    "        self.n_samples, self.x_data, self.y_data, _, _ = create_data_window(n, frequencies, amplitudes, freq_noise, ampl_noise, window_size)\n",
    "        \n",
    "        \n",
    "                  \n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "class Dataset_Dummy_Target_Window(Dataset):\n",
    "\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        n = 8000\n",
    "        frequencies = [1,4,1.9,3.1]\n",
    "        amplitudes = [6,2,5,4]\n",
    "        freq_noise = 0.5\n",
    "        ampl_noise = 2\n",
    "        window_size = 1000\n",
    "        self.n_samples, _, _, self.x_data, self.y_data = create_data_window(n, frequencies, amplitudes, freq_noise, ampl_noise, window_size)\n",
    "        \n",
    "        \n",
    "                  \n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e521e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader\n",
    "\n",
    "dataset_source = Dataset_Dummy_Source_Window()\n",
    "\n",
    "# define train/val dimensions\n",
    "train_size_source = int(0.8 * len(dataset_source))\n",
    "validation_size_source = len(dataset_source) - train_size_source\n",
    "\n",
    "training_dataset_source, validation_dataset_source = torch.utils.data.random_split(dataset_source, [train_size_source, validation_size_source])\n",
    "batch_size = 64\n",
    "train_loader_source = DataLoader(dataset=training_dataset_source,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "validation_loader_source = DataLoader(dataset=validation_dataset_source,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_target = Dataset_Dummy_Target_Window()\n",
    "\n",
    "# define train/val dimensions\n",
    "train_size_target = int(0.8 * len(dataset_target))\n",
    "validation_size_target = len(dataset_target) - train_size_target\n",
    "\n",
    "training_dataset_target, validation_dataset_target = torch.utils.data.random_split(dataset_target, [train_size_target, validation_size_target])\n",
    "batch_size = 64\n",
    "train_loader_target = DataLoader(dataset=training_dataset_target,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "validation_loader_target = DataLoader(dataset=validation_dataset_target,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "source_loader = {}\n",
    "source_loader[\"train\"] = train_loader_source\n",
    "source_loader[\"val\"] = validation_loader_source\n",
    "\n",
    "target_loader = {}\n",
    "target_loader[\"train\"] = train_loader_target\n",
    "target_loader[\"val\"] = validation_loader_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63a30f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d(1, 64, kernel_size=(100,), stride=(1,))\n",
      "Conv1d(64, 32, kernel_size=(10,), stride=(1,), padding=(1,))\n",
      "Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(1,))\n",
      "BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Linear(in_features=28544, out_features=100, bias=True)\n",
      "Linear(in_features=100, out_features=3, bias=True)\n",
      "Linear(in_features=3, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "input_size = 1\n",
    "input_fc_size = 32*892 #25*40 \n",
    "hidden_fc_size_1 = 100\n",
    "hidden_fc_size_2 = 3\n",
    "output_size = 2\n",
    "\n",
    "#CNN Layers\n",
    "cnn1 = nn.Conv1d(input_size, 64, kernel_size=100, stride=1)\n",
    "cnn2 = nn.Conv1d(64,32,kernel_size=10, stride = 1, padding=1)\n",
    "batch1 = nn.BatchNorm1d(32)\n",
    "cnn3 = nn.Conv1d(32,32,kernel_size=5, stride = 1, padding=1)\n",
    "batch2 = nn.BatchNorm1d(32)\n",
    "\n",
    "cnn_layers = [cnn1, cnn2, batch1, cnn3, batch2]\n",
    "\n",
    "#FC Layers\n",
    "classifier_layer_1 = nn.Linear(input_fc_size, hidden_fc_size_1)\n",
    "classifier_layer_2 = nn.Linear(hidden_fc_size_1, hidden_fc_size_2)\n",
    "classifier_layer_3 = nn.Linear(hidden_fc_size_2, output_size)\n",
    "\n",
    "fc_layers = [classifier_layer_1, classifier_layer_2, classifier_layer_3]\n",
    "\n",
    "\n",
    "print(cnn1)\n",
    "\n",
    "print(cnn2)\n",
    "\n",
    "print(cnn3)\n",
    "\n",
    "print(batch1)\n",
    "\n",
    "print(batch2)\n",
    "\n",
    "print(classifier_layer_1)\n",
    "\n",
    "print(classifier_layer_2)\n",
    "\n",
    "print(classifier_layer_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d92abe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from mmd_loss import MMD_loss\n",
    "class MMD_loss(nn.Module):\n",
    "    def __init__(self, fix_sigma = None, kernel_mul = 2.0, kernel_num = 5):\n",
    "        super(MMD_loss, self).__init__()\n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.fix_sigma = fix_sigma\n",
    "        return\n",
    "    def gaussian_kernel(self, source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "        n_samples = int(source.size()[0])+int(target.size()[0])\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "        \n",
    "        total0 = total.unsqueeze(0).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        total1 = total.unsqueeze(1).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        L2_distance = ((total0-total1)**2).sum(2)\n",
    "        if torch.is_tensor(fix_sigma):\n",
    "            bandwidth_list = fix_sigma\n",
    "        else:\n",
    "                bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
    "                bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "                bandwidth_list = [bandwidth * (kernel_mul**i) for i in range(kernel_num)]\n",
    "        kernel_val = [torch.exp(-L2_distance / bandwidth_temp) for bandwidth_temp in bandwidth_list]\n",
    "        return sum(kernel_val)\n",
    "\n",
    "    def forward(self, source, target):\n",
    "\n",
    "        batch_size = int(source.size()[0])\n",
    "        kernels = self.gaussian_kernel(source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n",
    "        XX = kernels[:batch_size, :batch_size]\n",
    "        YY = kernels[batch_size:, batch_size:]\n",
    "        XY = kernels[:batch_size, batch_size:]\n",
    "        YX = kernels[batch_size:, :batch_size]\n",
    "        loss = torch.mean(XX + YY - XY -YX)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bcb9d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(cnn_layers, fc_layers, data, labels_source, labels_target, criterion, MMD_loss_calculator, GAMMA):\n",
    "        \n",
    "        \n",
    "        batch_size = len(labels_source)\n",
    "        \n",
    "        mmd_loss = 0\n",
    "        \n",
    "        #CNN applied on data\n",
    "        cnn_data = data.float()\n",
    "        for cnn_layer in cnn_layers:\n",
    "            cnn_data = cnn_layer(cnn_data)\n",
    "            mmd_loss += MMD_loss_calculator.forward(cnn_data[:batch_size, :], cnn_data[batch_size:, :])\n",
    "            \n",
    "        #Flatten CNN Output for FC\n",
    "        flatten = torch.reshape(batch_out2,(cnn_data.shape[0],cnn_data.shape[1]*cnn_data.shape[2]))\n",
    "        \n",
    "        #FC applied on data\n",
    "        fc_data = flatten\n",
    "        for fc_layer in fc_layers:\n",
    "            fc_data = fc_layer(fc_data)\n",
    "            mmd_loss += MMD_loss_calculator.forward(fc_data[:batch_size, :], fc_data[batch_size:, :])\n",
    "        \n",
    "        source_pred = fc_data[:batch_size, :]\n",
    "        target_pred = fc_data[batch_size:, :]\n",
    "        #CE LOSS\n",
    "        source_ce_loss = criterion(source_pred, labels_source)\n",
    "        target_ce_loss = criterion(target_pred, labels_target)\n",
    "        \n",
    "        #TOTAL LOSS\n",
    "        loss = source_ce_loss + GAMMA * mmd_loss\n",
    "        \n",
    "        #collect information about labels, predictions\n",
    "        n_correct_source = 0\n",
    "        n_correct_target = 0\n",
    "        n_samples_source = 0\n",
    "        n_samples_target = 0\n",
    "        \n",
    "        # plot list\n",
    "        class_0_source = np.empty((0,3))\n",
    "        class_1_source = np.empty((0,3))\n",
    "        class_0_target = np.empty((0,3))\n",
    "        class_1_target = np.empty((0,3))\n",
    "        \n",
    "        #for each element in batch check if prediction is correct and collect total and correct predictions and labels\n",
    "        for i in range(len(labels_source)):\n",
    "            label_source = labels_source[i]\n",
    "            output_source = torch.argmax(source_pred[i])\n",
    "            if label_source == output_source:\n",
    "                n_correct_source+=1\n",
    "            n_samples_source+=1\n",
    "            \n",
    "            if label_source == 0:\n",
    "                class_0_source = np.append(class_0_source, np.expand_dims(source_out[i,:].detach().numpy(), axis = 0), axis = 0)\n",
    "            elif label_source == 1:\n",
    "                class_1_source = np.append(class_1_source, np.expand_dims(source_out[i,:].detach().numpy(), axis = 0), axis = 0)\n",
    "                \n",
    "        acc_total_source = 100.0 * n_correct_source / n_samples_source\n",
    "            \n",
    "        for i in range(len(labels_target)):\n",
    "            label_target = labels_target[i]\n",
    "            output_target = torch.argmax(target_pred[i])\n",
    "            if label_target == output_target:\n",
    "                n_correct_target+=1\n",
    "            n_samples_target+=1\n",
    "                \n",
    "            if label_target == 0:\n",
    "                class_0_target = np.append(class_0_target, np.expand_dims(target_out[i,:].detach().numpy(), axis = 0), axis = 0)\n",
    "            elif label_target == 1:\n",
    "                class_1_target = np.append(class_1_target, np.expand_dims(target_out[i,:].detach().numpy(), axis = 0), axis = 0)\n",
    "\n",
    "            \n",
    "\n",
    "        acc_total_target = 100.0 * n_correct_target / n_samples_target\n",
    "\n",
    "        \n",
    "        return loss, mmd_loss, source_ce_loss, target_ce_loss, acc_total_source, acc_total_target, class_0_source, class_1_source, class_0_target, class_1_target\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "906ad42e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rg/g_9b4q1j0h94scy_c8tdgd980000gn/T/ipykernel_4467/506735443.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mMMD_loss_calculator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMMD_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfix_sigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSIGMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "writer_graph = SummaryWriter('runs/Dataloader2/graph')\n",
    "writer_source_train = SummaryWriter('runs/Dataloader2/source_train')\n",
    "writer_target_train = SummaryWriter('runs/Dataloader2/target_train')\n",
    "writer_source_val = SummaryWriter('runs/Dataloader2/source_val')\n",
    "writer_target_val = SummaryWriter('runs/Dataloader2/target_val')\n",
    "\n",
    "writer_source = {}\n",
    "writer_source[\"train\"] = writer_source_train\n",
    "writer_source[\"val\"] = writer_source_val\n",
    "\n",
    "writer_target = {}\n",
    "writer_target[\"train\"] = writer_target_train\n",
    "writer_target[\"val\"] = writer_target_val\n",
    "\n",
    "#define training params\n",
    "num_epochs = 10\n",
    "learning_rate = 0.1#0.008\n",
    "GAMMA = 2.5  # 1000 more weight to transferability\n",
    "SIGMA = torch.tensor([1,2,4,8,16],dtype=torch.float64)\n",
    "\n",
    "#define loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "MMD_loss_calculator = MMD_loss(fix_sigma = SIGMA)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "#collect loss for each batch\n",
    "loss_collected = 0\n",
    "target_ce_loss_collected = 0\n",
    "mmd_loss_collected = 0\n",
    "\n",
    "mmd_loss = {}\n",
    "mmd_loss[\"train\"] = []\n",
    "mmd_loss[\"val\"] = []\n",
    "\n",
    "ce_loss_source = {}\n",
    "ce_loss_source[\"train\"] = []\n",
    "ce_loss_source[\"val\"] = []\n",
    "\n",
    "ce_loss_target = {}\n",
    "ce_loss_target[\"train\"] = []\n",
    "ce_loss_target[\"val\"] = []\n",
    "\n",
    "accuracy_source = {}\n",
    "accuracy_source[\"train\"] = []\n",
    "accuracy_source[\"val\"] = []\n",
    "\n",
    "accuracy_target = {}\n",
    "accuracy_target[\"train\"] = []\n",
    "accuracy_target[\"val\"] = []\n",
    "\n",
    "\n",
    "\n",
    "phases = [\"val\", \"train\"]\n",
    "# Train and Validate the model\n",
    "for epoch in range(num_epochs):\n",
    "    #plot mmd\n",
    "    class_0_source_collect = np.empty((0,3))\n",
    "    class_1_source_collect = np.empty((0,3))\n",
    "    class_0_target_collect = np.empty((0,3))\n",
    "    class_1_target_collect = np.empty((0,3))\n",
    "    \n",
    "    loss_collected = 0\n",
    "    acc_collected = 0\n",
    "    \n",
    "\n",
    "    \n",
    "    for phase in phases:\n",
    "        iter_loader_source = iter(source_loader[phase])\n",
    "        iter_loader_target = iter(target_loader[phase])\n",
    "        \n",
    "        #get number of batches of train loader\n",
    "        assert len(source_loader[phase]) == len(target_loader[phase])\n",
    "        len_loader = len(source_loader[phase])\n",
    "        \n",
    "        \n",
    "        source_ce_loss_collected = 0\n",
    "        target_ce_loss_collected = 0\n",
    "        mmd_loss_collected = 0\n",
    "        acc_total_source_collected = 0\n",
    "        acc_total_target_collected = 0\n",
    "        \n",
    "        for i in range(len_loader):\n",
    "\n",
    "            ########Forward pass########\n",
    "            data_source, labels_source = iter_loader_source.next() #batch_size number of windows and labels from source domain\n",
    "            data_target, labels_target = iter_loader_target.next() #batch_size number of windows from target domain\n",
    "            data = torch.cat((data_source, data_target), dim=0) #concat the windows to 2*batch_size number of windows\n",
    "            \n",
    "            #get number of samples per batch\n",
    "            assert len(labels_source) == len(labels_target)\n",
    "            batch_size = len(labels_source) #take length of shorter dataoader which is the one from source domain (reason:train, val split)\n",
    "                        \n",
    "            if phase == \"val\":\n",
    "                \n",
    "                map(lambda x: x.train(False),cnn_layers) \n",
    "                map(lambda x: x.train(False),fc_layers) \n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    _, mmd_loss, source_ce_loss, target_ce_loss, acc_total_source, acc_total_target, class_0_source, class_1_source, class_0_target, class_1_target = forward(cnn_layers, fc_layers, data, labels_source, labels_target, criterion, MMD_loss_calculator, GAMMA)\n",
    "                    \n",
    "                    mmd_loss_collected += mmd_loss\n",
    "                    source_ce_loss_collected += source_ce_loss\n",
    "                    target_ce_loss_collected += target_ce_loss\n",
    "                    acc_total_source_collected += acc_total_source\n",
    "                    acc_total_target_collected += acc_total_target\n",
    "                    \n",
    "                    # collect plot values\n",
    "                    class_0_source_collect = np.append(class_0_source_collect, class_0_source, axis = 0)\n",
    "                    class_1_source_collect = np.append(class_1_source_collect, class_1_source, axis = 0)\n",
    "                    class_0_target_collect = np.append(class_0_target_collect, class_0_target, axis = 0)\n",
    "                    class_1_target_collect = np.append(class_1_target_collect, class_1_target, axis = 0)\n",
    "                    \n",
    "                    \n",
    "            \n",
    "            elif phase == \"train\":\n",
    "                \n",
    "                map(lambda x: x.train(True),cnn_layers) \n",
    "                map(lambda x: x.train(True),fc_layers) \n",
    "                \n",
    "                                \n",
    "                ########Forward pass########\n",
    "                loss, mmd_loss, source_ce_loss, target_ce_loss, acc_total_source, acc_total_target, _, _, _, _ = forward(cnn_layers, fc_layers, data, labels_source, labels_target, criterion, MMD_loss_calculator, GAMMA)\n",
    "                \n",
    "                mmd_loss_collected += mmd_loss.detatch()\n",
    "                source_ce_loss_collected += source_ce_loss.detatch()\n",
    "                target_ce_loss_collected += target_ce_loss.detatch()\n",
    "                acc_total_source_collected += acc_total_source\n",
    "                acc_total_target_collected += acc_total_target\n",
    "\n",
    "                ########Backward pass########\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                \n",
    "        #plot\n",
    "        if phase == \"val\" and (epoch ==0 or epoch ==2 or epoch == 4 or epoch ==6):\n",
    "\n",
    "            fig = plt.figure()\n",
    "            plt.gcf().set_size_inches((20, 20)) \n",
    "            ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "            m = [1,2,3,4]\n",
    "            data = [class_0_source_collect, class_1_source_collect, class_0_target_collect, class_1_target_collect]\n",
    "            for i in range(4):\n",
    "                ax.scatter(data[i][:,0], data[i][:,1], data[i][:,2], marker=m[i])\n",
    "            \n",
    "            plt.show()\n",
    "            fig.savefig(f\"no_mmd_epoch{epoch}\")       \n",
    "                \n",
    "                \n",
    "    \n",
    "        running_ce_loss_source = source_ce_loss_collected / len_loader\n",
    "        running_ce_loss_target = target_ce_loss_collected / len_loader\n",
    "        \n",
    "        running_acc_source = acc_total_source_collected / len_loader\n",
    "        running_acc_target = acc_total_target_collected / len_loader\n",
    "        \n",
    "        running_mmd_loss = mmd_loss_collected/ len_loader\n",
    "        \n",
    "        \n",
    "\n",
    "        ce_loss_source[phase].apppend(running_ce_loss_source)\n",
    "        ce_loss_target[phase].apppend(running_ce_loss_target)\n",
    "        writer_source[phase].add_scalar(f'CE-Loss', running_ce_loss_source, epoch)\n",
    "        writer_target[phase].add_scalar(f'CE-Loss', running_ce_loss_target, epoch)\n",
    "        \n",
    "        accuracy_source[phase].apppend(running_acc_source)\n",
    "        accuracy_target[phase].apppend(running_acc_target)\n",
    "        writer_source[phase].add_scalar(f'Accuracy', running_acc_source, epoch)\n",
    "        writer_target[phase].add_scalar(f'Accuracy', running_acc_target, epoch)\n",
    "        \n",
    "        mmd_loss[phase].append(running_mmd_loss)\n",
    "        writer_source[phase].add_scalar(f'MMD-Loss', running_mmd_loss, epoch)\n",
    "            \n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} successfull\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca37bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.title('CE-Loss')\n",
    "plt.plot(ce_loss_source['train'], 'bo-', label = 'Source Train', linewidth=1,markersize=0.1)\n",
    "plt.plot(ce_loss_source['val'], 'go-', label = 'Source Val', linewidth=1,markersize=0.1)\n",
    "plt.plot(ce_loss_target['train'], 'ro-', label = 'Target Train', linewidth=1,markersize=0.1)\n",
    "plt.plot(ce_loss_target['val'], 'yo-', label = 'Target Val', linewidth=1,markersize=0.1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig1.savefig('CE_Loss')\n",
    "\n",
    "fig2 = plt.figure()\n",
    "plt.title('Accuracy')\n",
    "plt.plot(accuracy_source['train'], 'bo-', label = 'Source Train', linewidth=1,markersize=0.1)\n",
    "plt.plot(accuracy_source['val'], 'go-', label = 'Source Val', linewidth=1,markersize=0.1)\n",
    "plt.plot(accuracy_target['train'], 'ro-', label = 'Target Train', linewidth=1,markersize=0.1)\n",
    "plt.plot(accuracy_target['val'], 'yo-', label = 'Target Val', linewidth=1,markersize=0.1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig2.savefig('Accuracy')\n",
    "\n",
    "fig3 = plt.figure()\n",
    "plt.title('MMD-Loss')\n",
    "plt.plot(mmd_loss['train'], 'bo-', label = 'MMD Train', linewidth=1,markersize=0.1)\n",
    "plt.plot(mmd_loss['val'], 'go-', label = 'MMD Val', linewidth=1,markersize=0.1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig3.savefig('MMD_Loss')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
